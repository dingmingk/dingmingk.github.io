<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>他的国</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="他的国">
<meta property="og:url" content="https://dingmingk.github.io/page/2/index.html">
<meta property="og:site_name" content="他的国">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="他的国">
  
    <link rel="alternative" href="/atom.xml" title="他的国" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <script src="/style.js"></script>
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">dingmingk</a></h1>
		</hgroup>

		
		<p class="header-subtitle">做一个安静的美男子</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="3" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
		        
					<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">dingmingk</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">dingmingk</h1>
			</hgroup>
			
			<p class="header-subtitle">做一个安静的美男子</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
			        
						<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-k8s_monitor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/31/k8s_monitor/">Kubernetes 集群性能监控</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在 Kubernetes 系统中，使用 cAdvisor 对 Node 所在主机资源和在该 Node 上运行的容器进行监控和性能数据采样。由于 cAdvisor 集成在 Kubelet 中，即运行在每个 Node 上，所以一个 cAdvisor 仅能对一台 Node 进行监控。在大规模容器集群中，我们需要对所有 Node 和全部容器进行性能监控，Kubernetes 使用一套工具来实现集群性能数据的采集、存储和展示：Heapster、InfluxDB 和 Grafana。</p>
<blockquote>
<p>Heapster：是对集群中各 Node、Pod 的资源使用数据进行采集的系统，通过访问每个 Node 上 Kubelet 的 API，再通过 Kubelet 调用 cAdvisor 的 API 来采集该节点上所有容器的性能数据。之后 Heapster 进行数据聚合，并将结果保存到后端存储系统中。Heapster 支持多种后端存储系统，包括 memory、InfluxDB、BigQuery、谷歌云平台提供的 Google Cloud Monitoring 和 Google Cloud Logging等。</p>
<p>InfluxDB：是分布式时序数据库（每条记录都带有时间戳属性），主要用于实时数据采集、事件跟踪记录、存储时间图标、原始数据等。InfluxDB 提供 REST API 用于数据的存储和查询。</p>
<p>Grafana：通过 Dashboard 将 InfluxDB 中的时序数据展现成图标或曲线等形式，便于运维人员查看集群的运行状态。</p>
</blockquote>
<h2 id="配置-Kubernetes-集群的-ServiceAccount-和-Secret"><a href="#配置-Kubernetes-集群的-ServiceAccount-和-Secret" class="headerlink" title="配置 Kubernetes 集群的 ServiceAccount 和 Secret"></a>配置 Kubernetes 集群的 ServiceAccount 和 Secret</h2><p>Heapster 当前版本需要使用 HTTPS 的安全方式与 Kubernetes Master 进行连接，所以需要先进行 ServiceAccount 和 Secret 的创建。如果不使用 Secret，则 Heapster 启动时将会报错，然后 Heapster 容器会被 ReplicationController 反复销毁、创建，无法正常工作。</p>
<p>关于 ServiceAccount 和 Secret 的原理详见<a href="http://blog.dingmingk.com/blog/kube_security.html" target="_blank" rel="external">http://blog.dingmingk.com/blog/kube_security.html</a>。</p>
<p>在进行一下操作时，我们假设在 Kubernetes 集群中没有创建过 Secret（如果之前创建过，则可以先删除 etcd 中与 Secret 相关的键值）。</p>
<p>首先，使用 OpenSSL 工具在 Master 服务器上创建一些证书和私钥相关的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># openssl genrsa -out ca.key 2048</div><div class="line"># openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=yourcompany.com&quot; -days 5000 -out ca.crt</div><div class="line"># openssl genrsa -out server.key 2048</div><div class="line"># openssl req -new -key server.key -subj &quot;/CN=kubernetes-master&quot; -out server.csr</div><div class="line"># openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 5000</div></pre></td></tr></table></figure>
<p>注意，在生成 server.csr 时 -subj 参数中 /CN 指定的名字需为 Master 的主机名。另外，在生成 ca.crt 时 -subj 参数中 /CN 的名字最好与主机名不同，设为相同可能导致对普通 Master 的 HTTPS 访问认证失败。</p>
<p>执行完成后会生成 6 个文件：ca.crt、ca.key、ca.srl、server.crt、server.csr、server.key。将这些文件复制到 /var/run/kubernetes/ 目录中，然后设置kube-apiserver 的启动参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">--client_ca_file=/var/run/kubernetes/ca.crt</div><div class="line">--tls-private-key-file=/var/run/kubernetes/server.key</div><div class="line">--tls-cert-file=/var/run/kubernetes/server.crt</div></pre></td></tr></table></figure>
<p>之后重启 kube-apiserver 服务。</p>
<p>接下来，给 kube-controller-manager 服务添加以下启动参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">--service_account_private_key_file=/var/run/kubernetes/server.key</div><div class="line">--root-ca-file=/var/run/kubernetes/ca.crt</div></pre></td></tr></table></figure>
<p>然后重启 kube-controller-manager 服务。</p>
<p>在 kube-apiserver 服务成功启动后，系统会自动为每个命名空间创建一个 ServiceAccount 和一个 Secret（包含一个 ca.crt 和一个 token）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl get serviceaccounts --all-namespaces</div><div class="line">$ kubectl get secrets --all-namespaces</div><div class="line">$ kubectl describe secret xxx</div></pre></td></tr></table></figure>
<p>之后 ReplicationController 在创建 Pod 时，会生成类型为 Secret 的 Volume 存储卷，并将该 Volume 挂载到 Pod 内的如下目录中：/var/run/secrets/kubernetes.io/serviceaccount。然后，容器内的应用程序就可以使用该 Secret 与 Master 建立 HTTPS 连接了。Pod 的 Volumes 设置和挂载操作由 ReplicationController 和 Kubelet 自动完成，可以通过查看 Pod 的详细信息了解到。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get pods kube-dns-v8-xxxxx --namespace=kube-system -o yaml</div></pre></td></tr></table></figure>
<p>进入容器，查看/var/run/secrets/kubernetes.io/serviceaccount 目录，可以看到两个文件 ca.crt 和 token，这两个文件就是与 Master 通信时所需的证书和秘钥信息。</p>
<h2 id="部署-Heapster、InfluxDB、Grafana"><a href="#部署-Heapster、InfluxDB、Grafana" class="headerlink" title="部署 Heapster、InfluxDB、Grafana"></a>部署 Heapster、InfluxDB、Grafana</h2><p>在 ServiceAccount 和 Secrets 创建完成后，我们就可以创建 Heapster、InfluxDB 和 Grafana 等 ReplicationController 和 Service 了。</p>
<p><strong>heapsster-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">    kubernetes.io/name: Heapster</div><div class="line">  name: heapster</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">    - port: 80</div><div class="line">      targetPort: 8082</div><div class="line">  selector:</div><div class="line">    k8s-app: heapster</div></pre></td></tr></table></figure>
<p><strong>InfluxDB-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels: null</div><div class="line">  name: monitoring-InfluxDB</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">ports:</div><div class="line">  - name: http</div><div class="line">    port: 8083</div><div class="line">    targetPort: 8083</div><div class="line">    nodePort: 30083</div><div class="line">  - name: api</div><div class="line">    port: 8086</div><div class="line">    targetPort: 8086</div><div class="line">    nodePort: 30086</div><div class="line">selector:</div><div class="line">  name: influxGrafana</div></pre></td></tr></table></figure>
<p><strong>Grafana-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    kubernetes.io/name: monitoring-Grafana</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">  name: monitoring-Grafana</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">  ports:</div><div class="line">    - port: 80</div><div class="line">      targetPort: 8080</div><div class="line">      nodePort: 30080</div><div class="line">  selector:</div><div class="line">    name: influxGrafana</div></pre></td></tr></table></figure>
<p><strong>heapster-controller.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    k8s-app: heapster</div><div class="line">    name: heapster</div><div class="line">    version: v6</div><div class="line">  name: heapster</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: heapster</div><div class="line">    k8s-app: heapster</div><div class="line">    version: v6</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: heapster</div><div class="line">        version: v6</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: heapster</div><div class="line">          image: gcr.io/google_containers/heapster:v0.17.0</div><div class="line">          command:</div><div class="line">            - /heapster</div><div class="line">            - --source=kubernetes:http://192.168.1.128:8080?inClusterConfig=false&amp;kubeletHttps=true&amp;useServiceAccount=true&amp;auth=</div><div class="line">            - --sink=InfluxDB:http://monitoring-InfluxDB:8086</div></pre></td></tr></table></figure>
<p>Heapster 需要设置的参数如下：</p>
<ul>
<li>–source：为配置监控来源。在本例中使用 kubernetes:表示从 Kubernetes Master 获取各 Node 的信息。在 URL 后面的参数部分，修改 kubeletHttps、inClusterConfig、useServiceAccount 的值，并设置 auth 的值为空。URL中可配置的参数如下：<ul>
<li>IP 地址和端口号：为 Kubernetes Master 的地址。</li>
<li>kubeletPort：默认为 10255（Kubelet 服务的只读端口号）。</li>
<li>kubeletHttps：是否通过 HTTPS 方式连接 Kubelet，默认为 false。</li>
<li>apiVersion： API 版本号，默认为 Kubernetes 系统的版本号，当前为 v1.</li>
<li>inClusterConfig：是否使用 Heapster 命名空间中的 ServiceAccount，默认为 true。</li>
<li>insecure：是否信任 Kubernetes 证书，默认为 false。</li>
<li>auth：客户端认证授权文件，当 ServiceAccount 不可用时对其进行设置。</li>
<li>useServiceAccount：是否使用 ServiceAccount，默认为 false。</li>
</ul>
</li>
<li>–sink：为配置后端的存储系统，在本例中使用 InfluxDB 系统。</li>
</ul>
<p><strong>InfluxDB-Grafana-controller.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    name: influxGrafana</div><div class="line">  name: infludb-Grafana</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: influxGrafana</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: influxGrafana</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: InfluxDB</div><div class="line">          image: gcr.io/google_containers/heapster_InfluxDB:v0.3</div><div class="line">          ports:</div><div class="line">            - containerPort: 8083</div><div class="line">              hostPort: 8083</div><div class="line">            - containerPort: 8086</div><div class="line">              hostPort: 8086</div><div class="line">        - name: Grafana</div><div class="line">          image: gcr.io/google_containers/heapster_Grafana:v0.7</div><div class="line">          ports:</div><div class="line">            - containerPort: 8080</div><div class="line">              hostPort: 8080</div><div class="line">          env:</div><div class="line">            - name: INFLUXDB_HOST</div><div class="line">              value: monitoring-InfluxDB</div></pre></td></tr></table></figure>
<p>最后，创建所有 Service 和 RC：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f heapster-service.yaml</div><div class="line">$ kubectl create -f InfluxDB-server.yaml</div><div class="line">$ kubectl create -f Grafana-service.yaml</div><div class="line">$ kubectl create -f InfluxDB-Grafana-controller.yaml</div><div class="line">$ kubectl create -f heapster-controller.yaml</div></pre></td></tr></table></figure>
<p>通过 kubectl get pods –namespace=kube-system 确认各 Pod 都成功启动了。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/31/k8s_monitor/" class="archive-article-date">
  	<time datetime="2016-03-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_security" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/31/k8s_security/">Kubernetes 安全机制</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kubernetes 通过一系列机制来实现集群的安全控制，其中包括 API Server 的认证授权、准入控制机制及保护敏感信息的 Secret 机制等。集群的安全性必须考虑如下几个目标：</p>
<ol>
<li>保证容器与其所在的宿主机的隔离；</li>
<li>限制容器给基础设施及其他容器带来消极影响的能力；</li>
<li>最小权限原则————合理限制所有组件的权限，确保组件只执行它被授权的行为，通过限制单个组件的能力来限制它所能到达的权限范围；</li>
<li>明确组件间边界的划分；</li>
<li>划分普通用户和管理员的角色；</li>
<li>在必要的时候允许将管理员权限赋给普通用户；</li>
<li>允许拥有“Secret”数据（Keys、Certs、Passwords）的应用在集群中运行。</li>
</ol>
<p>下面分别从 Authentication、Authorization、Admission Control、Secret 和 Service Account 六个方面来说明集群的安全机制。</p>
<h2 id="Authentication-认证"><a href="#Authentication-认证" class="headerlink" title="Authentication 认证"></a>Authentication 认证</h2><p>Kubernetes 对 API 调用使用 CA（Client Authentication）、Token 和 HTTP Base 方式实现用户认证。</p>
<p>使用 CA 认证的应用需包含一个 CA 认证机构给服务器端下发的根证书、服务端证书和私钥文件。因此 API Server 的三个参数“–client-ca-file”“–tls-cert-file”和“–tls-private-key-file”分别指向根证书文件、服务端证书文件和私钥文件。API Server 客户端应用的三个启动参数（例如 Kubectl 的三个参数 “certificate-authority”“client-certificate”和“client-key”），或客户端应用的 kubeconfig 配置文件中的配置项“certificate-authority”“client-certificate”和“client-key”分别指向根证书文件、客户端证书文件和私钥文件。</p>
<p>Kubernetes 的 CA 认证方式通过添加 API Server 的启动参数“–client-ca-file=SOMEFILE”实现，其中“SOMEFILE”为认证授权文件，该文件包含一个或多个证书颁发机构（CA Certificates Authorities）。</p>
<p>Token 认证方式通过添加 API Server 的启动参数“–token_auth_file=SOMEFILE”实现，其中“SOMEFILE”指的是 Token 文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">例如 Token 文件内容为：</div><div class="line">lkjqweroiuuou,Thomas,8x7d1kklzseertyywx</div><div class="line">用 CURL 去访问该 API Server：</div><div class="line">curl $APISERVER/api --header &quot;Authorization: Bearer lkjqweroiuuou&quot; --insecure</div><div class="line">&#123;</div><div class="line">  &quot;version&quot; : [</div><div class="line">    &quot;v1&quot;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>基本认证方式是通过添加 API Server 的启动参数“–basic_auth_file=SOMEFILE”实现的，其中“SOMEFILE”指的是用于存储用户和密码信息的基本认证文件。</p>
<p>当使用基本认证方式从 HTTP 客户端访问 API Server 时，HTTP 请求头中的 Authorization 域必须包含“Basic BASE64ENCODEDUSER:PASSWORD”的值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tmp=`base64 &quot;dingmingk:passwd&quot;`</div><div class="line">curl $APISERVER/api --header &quot;Authorization: Basic $tmp&quot; --insecure</div><div class="line">&#123;</div><div class="line">  &quot;version&quot;:[</div><div class="line">    &quot;v1&quot;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Authorization-授权"><a href="#Authorization-授权" class="headerlink" title="Authorization 授权"></a>Authorization 授权</h2><p>在 Kubernetes 中，授权（Authorization）是认证（Authenticaiton）后的一个独立步骤，作用于 API Server 主要端口的所有 HTTP 访问。授权流程不作用于只读端口，在计划中只读端口在不久之后将被删除。授权流程通过访问策略比较请求上下文的属性（例如用户名、资源和 Namespace）。在通过 API 访问资源之前，必须通过访问策略进行校验。访问策略通过 API Server 的启动参数 –authorization_mode 配置，该参数包含如下三个值：</p>
<ul>
<li>–authorization_mode=AlwaysDeny</li>
<li>–authorization_mode=AlwaysAllow</li>
<li>–authorization_mode=ABAC</li>
</ul>
<p>其中，“AlwaysDeny”表示拒绝所有的请求，该配置一般用于测试；“AlwaysAllow”表示接受所有请求，如果集群不需要授权流程，则可以采用该策略；“ABAC”表示使用用户配置的授权策略去管理访问 API Server 的请求，ABAC（Attribute-Based Access Control）为基于属性的访问控制。</p>
<p>在 Kubernetes 中，一个 HTTP 请求包含如下 4 个能被授权进程识别的属性：</p>
<ul>
<li>用户名（代表一个已经被认证的用户的字符型用户名）；</li>
<li>是否是只读请求（REST 的 GET 操作是只读的）；</li>
<li>被访问的是哪一类资源，例如访问 Pod 资源/api/v1/namespaces/defaults/pods；</li>
<li>被访问对象所属的 Namespace，如果这被访问的资源不支持 Namespace，则是空字符串。</li>
</ul>
<p>如果选用 ABAC 模式，那么需要通过设置 API Server 的“–authorization_policy_file=SOME_FILENAME”参数来指定授权策略文件，其中“SOME_FILENAME”为授权策略文件。授权策略文件的每一行都是一个 JSON 对象，该 JSON 对象是一个 Map，这个 Map 内不包含 List 和 Map。每行都是一个“策略对象”。策略对象包含下面 4 个属性：</p>
<ul>
<li>user（用户名），为字符串类型，该字符串类型的用户名来源于 Token 文件或基本认证文件中的用户名字段的值；</li>
<li>readonly（只读标识），为布尔类型，当它的值为 true 时，表明该策略允许 GET 请求通过；</li>
<li>resource（资源），为字符串类型，来自于 URL 的资源，例如“Pods”；</li>
<li>namespace（命名空间），为字符串类型，表明该策略允许访问某个 Namespace 的资源。</li>
</ul>
<p>没被设置的属性，将被等同于根据值的类型设置成零值（例如为字符串类型属性设置一个空字符串；为布尔值属性设置 false；为数值类型属性设置 0）。</p>
<p>授权策略文件中的策略对象的一个未设置属性，表示匹配 HTTP 请求中该属性的任何值。对请求的 4 个属性值和授权策略文件中的所有策略对象逐个匹配，如果至少有一个策略对象被匹配上，则该请求将被授权通过。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">允许用户 alice 做任何事情：&#123;&quot;user&quot;:&quot;alice&quot;&#125;。</div><div class="line">用户 Kubelet 指定读取资源 Pods：&#123;&quot;user&quot;:&quot;kubelet&quot;,&quot;resource&quot;:&quot;pods&quot;,&quot;readonly&quot;:true&#125;。</div><div class="line">用户 Kubelet 能读和写资源 events：&#123;&quot;user&quot;:&quot;kubelet&quot;,&quot;resource&quot;:&quot;events&quot;&#125;</div><div class="line">用户 bob 只能读取 Namespace &quot;myNamespace&quot; 中的资源 Pods：&#123;&quot;user&quot;:&quot;bob&quot;,&quot;resource&quot;:&quot;pods&quot;,&quot;readonly&quot;:true,&quot;ns&quot;:&quot;myNamespace&quot;&#125;</div></pre></td></tr></table></figure>
<h2 id="Admission-Control-准入控制"><a href="#Admission-Control-准入控制" class="headerlink" title="Admission Control 准入控制"></a>Admission Control 准入控制</h2><p>Admission Control 是用于拦截所有经过认证和鉴权后的访问 API Server 请求的可插入代码（或插件）。这些可插入代码运行于 API Server 进程中，在被调用前必须被编译成二进制文件。在请求被 API Server 接收前，每个 Admission Control 插件按配置顺序执行。如果其中的任意一个插件拒绝该请求，就意味着这个请求被 API Server 拒绝，同时 API Server 反馈一个错误信息给请求发起方。</p>
<p>在某些情况下，Admission Control 插件会使用系统配置的默认值取改变进入集群对象的内容。此外，Admission Control 插件可能会改变请求处理所使用的配额，比如增加请求处理的资源配额。</p>
<p>通过配置 API Server 的启动参数“admission_control”，在该参数中加入需要的 Admission Control 插件列表，各插件的名称之间用逗号隔开。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--admission_control=NamespaceAutoProvision,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota</div></pre></td></tr></table></figure>
<p>Admission Control 的插件列表如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AlwaysAdmit</td>
<td style="text-align:left">允许所有请求通过</td>
</tr>
<tr>
<td style="text-align:center">AlwaysDeny</td>
<td style="text-align:left">拒绝所有请求，一般用于测试</td>
</tr>
<tr>
<td style="text-align:center">DenyExecOnPrivileged</td>
<td style="text-align:left">拦截所有带有 SecurityContext 属性的 Pod 的请求，拒绝在一个特权容器中执行命令</td>
</tr>
<tr>
<td style="text-align:center">ServiceAccount</td>
<td style="text-align:left">配合 Service Account Controller 使用，为设定了 Service Account 的 Pod 自动管理 Secret，使得 Pod 能够使用相应的 Secret 下载 Image 和访问 API Server</td>
</tr>
<tr>
<td style="text-align:center">SecurityContextDeny</td>
<td style="text-align:left">不允许带有 SecurityContext 属性的 Pod 存在，SecurityContext 属性用于创建特权容器</td>
</tr>
<tr>
<td style="text-align:center">ResourceQuota</td>
<td style="text-align:left">在 Namespace 中做资源配额限制</td>
</tr>
<tr>
<td style="text-align:center">LimitRanger</td>
<td style="text-align:left">限制 Namespace 中的 Pod 和 Container 的 CPU 和 内存配额</td>
</tr>
<tr>
<td style="text-align:center">NamespaceExists</td>
<td style="text-align:left">读区请求中的 Namespace 属性，如果该 Namespace 不存在，则拒绝该请求</td>
</tr>
<tr>
<td style="text-align:center">NamespaceAutoProvision(deprecated)</td>
<td style="text-align:left">读取请求中的 Namespace 属性，如果该 Namespace 不存在，则尝试创建该 Namespace</td>
</tr>
<tr>
<td style="text-align:center">NamespaceLifecycle</td>
<td style="text-align:left">该插件限制访问处于中止状态的 Namespace，禁止在该 Namespace 中创建新的内容。当 NamespaceLifecycle 和 NamespaceExists 能够合并成一个插件后，NamespaceAutoProvision 就会变成 deprecated</td>
</tr>
</tbody>
</table>
<h2 id="Secret-私密凭据"><a href="#Secret-私密凭据" class="headerlink" title="Secret 私密凭据"></a>Secret 私密凭据</h2><p>Secret 的主要作用是保管私密数据，比如密码、OAuth Tokens、SSH Keys 等信息。将这些私密信息放在 Secret 对象中比直接放在 Pod 或 Docker Image 中更安全，也更便于使用。</p>
<p>Kubernetes 在 Pod 创建时，如果该 Pod 指定了 Service Account，那么为该 Pod 自动添加包含凭证信息的 Secrets，用于访问 API Server 和下载 Image。该功能可以通过 Admission Control 添加或失效，然而如果需要以安全的方式去访问 API Server，则建议开启该功能。</p>
<p>下面的例子用于创建一个 Secret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ kubectl namespace myspace</div><div class="line">$ cat &lt;&lt;EOF &gt; secrets.yaml</div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysecret</div><div class="line">type: Opaque</div><div class="line">data:</div><div class="line">  password: dmFsdWUtMgOK</div><div class="line">  username: dmFsdWUtMQOK</div><div class="line">$ kubectl create -f secrets.yaml</div></pre></td></tr></table></figure>
<p>在上面的例子中，data域的各子域的值必须为 base64 编码值，其中 password 域和 username 域 base64 编码前的值分别为“value-1” 和“value-2”。</p>
<p>一旦 Secret 被创建，则可以通过下面的三种方式使用它：</p>
<ol>
<li>在创建 Pod 时，通过为 Pod 指定 Service Account 来自动使用该 Secret；</li>
<li>通过挂载该 Secret 到 Pod 来使用它；</li>
<li>在创建 Pod 时，指定 Pod 的 spc.ImagePullSecrets 来引用它。</li>
</ol>
<p>将一个 Secret 通过挂载的方式添加到 Pod 的 Volume 中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: myPod</div><div class="line">  namespace: myns</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    name: mycontainer</div><div class="line">    image: redis</div><div class="line">    volumeMounts:</div><div class="line">    - name: foo</div><div class="line">      mountPath: /etc/foo</div><div class="line">      readOnly: true</div><div class="line">  volumes:</div><div class="line">  - name: foo</div><div class="line">    secret:</div><div class="line">      secretNmae: mysecret</div></pre></td></tr></table></figure>
<p>手动使用 imagePullSecret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">$ docker login localhost:1180</div><div class="line"></div><div class="line">用 base64 编码 dockercfg 的内容</div><div class="line">$ cat ~/.dockercfg | base64</div><div class="line"></div><div class="line">将上一步命令的输出结果作为 Secret 的“data.dockercfg” 域的内容，由此来创建一个 Secret</div><div class="line">$ cat &gt; image-pull-secret.yaml &lt;&lt;EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: myregistrykey</div><div class="line">data:</div><div class="line">  .dockercfg: xxxxxxxxxxxxxxxxxxxxx</div><div class="line">type: kubernetes.io/dockercfg</div><div class="line">EOF</div><div class="line">$ kubectl create -f image-pull-secret.yaml</div><div class="line"></div><div class="line">在创建 Pod 时，引用该 Secret</div><div class="line">$ cat &gt; pods.yaml &lt;&lt; EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: mypod2</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: foo</div><div class="line">      image: janedoe/awesomeapp:v1</div><div class="line">  imagePullSecrets:</div><div class="line">    - name: myregistrykey</div><div class="line">EOF</div><div class="line"></div><div class="line">$ kubectl create -f pods.yaml</div></pre></td></tr></table></figure>
<p>Pod 创建时会验证所挂载的 Secret 是否真的指向一个 Secret 对象，因此 Secret 必须在任何引用它的 Pod 之前被创建。Secret 对象属于 Namespace，它们只能被同一个 NameSpace 中的 Pod 所引用。</p>
<p>Secret 包含三种类型：Opaque、ServiceAccount 和 Dockercfg。前面举例介绍了如何创建 Opaque 和 Dockercfg 类型的 Secret。下面的例子为创建一个 Service Account Secret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysecret</div><div class="line">  annotations:</div><div class="line">    kubernetes.io/service-account.name: myserviceaccount</div><div class="line">type: kubernetes.io/service-account-token</div></pre></td></tr></table></figure>
<h2 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h2><p>Service Account 是多个 Secret 的集合。它包含两类 Secret：一类为普通 Secret，用于访问 API Server，也被称为 Service Account Secret；另一类为 imagePullSecret，用于下载容器镜像。如果镜像库运行在 Insecure 模式下，则该 Service Account 可以不包含 imagePullSecret。在下面的例子中创建了一个名为 build-robot 的 Service Account，并查询该 Service Account 的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$ cat &gt; serviceaccount.yaml &lt;&lt; EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: ServiceAccount</div><div class="line">metadata:</div><div class="line">  name: myserviceaccount</div><div class="line">secrets:</div><div class="line">  - name: mysecret</div><div class="line">    kind: Secret</div><div class="line">    apiVersion: v1</div><div class="line">  - name: mysecret1</div><div class="line">    kind: Secret</div><div class="line">    apiVersion: v1</div><div class="line">imagePullSecrets:</div><div class="line">  - name: mysecret2</div><div class="line">EOF</div><div class="line">$ kubectl create -f serviceaccount.yaml</div><div class="line">$ kubectl get serviceaccounts build-robot -o yaml</div></pre></td></tr></table></figure>
<p>在 Pod 的创建过程中指定“spec.serviceAccountName”的值为相应的 Service Account 的名称：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: mypod</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: mycontainer</div><div class="line">      image: nginx</div><div class="line">  serviceAccountName: myserviceaccount</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/31/k8s_security/" class="archive-article-date">
  	<time datetime="2016-03-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_dns" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/30/k8s_dns/">Kubernetes DNS 服务配置案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在 Kubernetes 系统中，Pod 在访问其他 Pod 的 Service 时，可以通过两种服务发现方式完成，即环境变量和 DNS 方式。但是使用环境变量是有限制条件的，即 Service 必须在 Pod 之前被创建出来，然后系统才能在新建的 Pod 中自动设置与 Service 相关的环境变量。DNS 则没有这个限制，其通过全局的 DNS 服务器来完成服务的注册与发现。</p>
<p>Kubernetes 提供的 DNS 由以下三个组件组成：</p>
<ol>
<li>etcd: DNS 存储</li>
<li>kube2sky: 将 Kubernetes Master 中的 Service（服务）注册到 etcd。</li>
<li>skyDNS: 提供 DNS 域名解析服务。</li>
</ol>
<p>这三个组件以 Pod 的方式启动和运行，所以在一个 Kubernetes 集群中，它们都可能被调度到任意一个 Node 节点上去。为了能够使它们之间网络互通，需要将各 Pod 之间的网络打通。</p>
<h2 id="skydns-配置文件"><a href="#skydns-配置文件" class="headerlink" title="skydns 配置文件"></a>skydns 配置文件</h2><p>首先创建 DNS 服务的 ReplicationController 配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: kube-dns-v8</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    k8s-app: kube-dns</div><div class="line">    version: v8</div><div class="line">    kubernetes.io/cluster-service: &quot; true &quot;</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    k8s-app: kube-dns</div><div class="line">    version: v8</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: kube-dns</div><div class="line">        version: v8</div><div class="line">        kubernetes.io/cluster-service: &quot; true &quot;</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: etcd</div><div class="line">        image: gcr.io/google_containers/etcd:2.0.9</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        command:</div><div class="line">        - /usr/local/bin/etcd</div><div class="line">        - -data-dir</div><div class="line">        - /var/etcd/data</div><div class="line">        - -listen-client-urls</div><div class="line">        - http://127.0.0.1:2379,http://127.0.0.1:4001</div><div class="line">        - -advertise-client-urls</div><div class="line">        - http://127.0.0.1:2379,http://127.0.0.1:4001</div><div class="line">        - -initial-cluster-token</div><div class="line">        - skydns-etcd</div><div class="line">        volumeMounts:</div><div class="line">        - name: etcd-storage</div><div class="line">          mountPath: /var/etcd/data</div><div class="line">      - name: kube2sky</div><div class="line">        image: gcr.io/google_containers/kube2sky:1.11</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        args:</div><div class="line">        # command = &quot; /kube2sky &quot;</div><div class="line">        - --kube_master_url=http://192.168.1.128:8080</div><div class="line">        - -domain=cluster.local</div><div class="line">      - name: skydns</div><div class="line">        image: gcr.io/google_containers/skydns:2015-03-11-001</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        args:</div><div class="line">        # command = &quot; /skydns &quot;</div><div class="line">        - -machines=http://localhost:4001</div><div class="line">        - -addr=0.0.0.0:53</div><div class="line">        - -domain=cluster.local</div><div class="line">        ports:</div><div class="line">        - containerPort: 53</div><div class="line">          name: dns</div><div class="line">          protocol: UDP</div><div class="line">        - containerPort: 53</div><div class="line">          name: dns-tcp</div><div class="line">          protocol: TCP</div><div class="line">      volumes:</div><div class="line">        - name: etcd-storage</div><div class="line">          emptyDir: &#123;&#125;</div><div class="line">      dnsPolicy: Default</div></pre></td></tr></table></figure>
<p>需要修改的几个配置参数如下：</p>
<ul>
<li>kube2sky 容器需要访问 Kubernetes Master，需要配置 Master 所在物理主机的 IP 地址和端口号，本例中设置参数 –kube_master_url 的值为 <a href="http://192.168.1.128:8080。" target="_blank" rel="external">http://192.168.1.128:8080。</a></li>
<li>kube2sky 容器和 skydns 容器的启动参数 -domain,设置 Kubernetes 集群中 Service 所属的域名，本例中为 cluster.local。启动后，kube2sky 会监听 Kubernetes，当有新的 Service 创建时，就会生成相应的记录并保存到 etcd 中。kube2sky 为每个 Service 生成两条记录：<ul>
<li><service_name>.<namespace_name>.<domain>;</domain></namespace_name></service_name></li>
<li><service_name>.<namespace_name>.svc.<domain>。</domain></namespace_name></service_name></li>
</ul>
</li>
<li>skydns 的启动参数 -addr=0.0.0.0:53 表示使用本机 TCP 和 UDP 的 53 端口提供服务。</li>
</ul>
<p>创建 DNS 服务的 Service 配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: kube-dns</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    k8s-app: kube-system</div><div class="line">    labels:</div><div class="line">      k8s-app: kube-dns</div><div class="line">      kubernetes.io/cluster-services: &quot; true &quot;</div><div class="line">      kubernetes.io/name: &quot; KubeDNS &quot;</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    k8s-app: kube-dns</div><div class="line">  clusterIP: 20.1.0.100</div><div class="line">  ports:</div><div class="line">  - name: dns</div><div class="line">    port: 53</div><div class="line">    protocol: UDP</div><div class="line">  - name: dns-tcp</div><div class="line">    port: 53</div><div class="line">    protocol: TCP</div></pre></td></tr></table></figure>
<p>注意，skydns 服务使用的 clusterIP 需要我们指定一个固定的 IP 地址，每个 Node 的 Kubernetes 进程都将使用这个 IP 地址，不能通过 Kubernetes 自动分配。</p>
<p>另外，这个 IP 地址需要在 kube-apiserver 启动参数 –service-cluster-ip-range 指定的 IP 地址范围内。</p>
<h2 id="修改每个-Node-上的-Kubelet-启动参数"><a href="#修改每个-Node-上的-Kubelet-启动参数" class="headerlink" title="修改每个 Node 上的 Kubelet 启动参数"></a>修改每个 Node 上的 Kubelet 启动参数</h2><p>修改每台 Node 上 Kubelet 的启动参数：</p>
<ul>
<li>–cluster_dns=20.1.0.100，为 DNS 服务的 ClusterIP 地址；</li>
<li>–cluster_domain=cluster.local，为 DNS 服务中设置的域名。</li>
</ul>
<p>然后重启 Kubelet 服务。</p>
<h2 id="创建-skydns-Pod-和服务"><a href="#创建-skydns-Pod-和服务" class="headerlink" title="创建 skydns Pod 和服务"></a>创建 skydns Pod 和服务</h2><p>通过 kubectl create 完成 RC 和 Service 的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># kubectl create -f skydns-rc.yaml</div><div class="line"># kubectl create -f skydns-svc.yaml</div></pre></td></tr></table></figure>
<p>创建完成后，可以查看系统创建的 RC、Pod、Service 是否创建成功：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># kubectl get rc|pods|services --namespace=kube-system</div></pre></td></tr></table></figure>
<h2 id="DNS-服务的工作原理"><a href="#DNS-服务的工作原理" class="headerlink" title="DNS 服务的工作原理"></a>DNS 服务的工作原理</h2><p>让我们看看 DNS 服务背后的工作原理。</p>
<p>kube2sky 容器应用通过调用 Kubernetes Master 的 API 获得集群中所有 Service 的信息，并持续监控新 Service 的生成，然后写入 etcd 中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">查看 etcd 中存储的 Service 信息</div><div class="line"># kubectl exec kube-dns-v8-xxxxx -c etcd --namespace=kube-system etcdctl ls /skydns/local/cluster</div></pre></td></tr></table></figure>
<p>根据 Kubelet 启动参数的设置（–cluster_dns），Kubelet 会在每个新创建的 Pod 中设置 DNS 域名解析配置文件 /etc/resolv.conf 文件，在其中增加了一条 nameserver 配置和一条 search 配置。</p>
<p>最后，应用程序就能够像访问网站域名一样，仅仅通过服务的名字就能访问到服务了。</p>
<p>通过 DNS 设置，对于其他 Service 的查询将可以不再依赖系统为每个 Pod 创建的环境变量，而是直接使用 Service 的名字就能对其进行访问，使得应用程序中的代码更加简洁。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/30/k8s_dns/" class="archive-article-date">
  	<time datetime="2016-03-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-30</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/30/k8s_network/">Kubernetes 网络配置方案</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>为了实现各 Node 上 Pod 之间的互通，需要一些方案来打通网络，这是 Kubernetes 能够正常工作的前提。本篇将对常用的直接路由、Flannel 和 Open vSwitch 三种配置进行详细说明。</p>
<h2 id="直接路由"><a href="#直接路由" class="headerlink" title="直接路由"></a>直接路由</h2><p>通过在每个 Node 上添加到其他 Node 上 docker0 的静态路由规则，就可以将不同物理服务器上 Docker Daemon 创建的 docker0 网桥互联互通。</p>
<p>例如 Pod1 所在 docker0 网桥的 IP 子网是 10.1.10.0，Node 地址为 192.168.1.128；而 Pod2 所在 docker0 网桥的 IP 子网是 10.1.20.0，Node 地址为 192.168.1.129.</p>
<p>在 Node1 上增加一条到 Node2 上 docker0 的静态路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">route add -net 10.1.20.0 netmask 255.255.255.0 gw 192.168.1.129</div></pre></td></tr></table></figure>
<p>同样，在 Node2 上增加一条到 Node1 上 docker0 的静态路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">route add -net 10.1.10.0 netmask 255.255.255.0 gw 192.168.1.128</div></pre></td></tr></table></figure>
<p>我们的集群中机器数量通常很多。假设有 100 台服务器，那么就需要在每台服务器上手工添加到另外 99 台服务器 docker0 的路由规则。为了减少手工操作，可以使用 Quagga 软件来实现路由规则的动态添加。</p>
<p>除了在每台服务器上安装 Quagga 软件并启动，还可以使用互联网上的一个 Quagga 容器来运行，在这里使用 index.alauda.cn/georce/router 镜像启动 Quagga。在每台 Node 上下载该 Docker 镜像：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker pull index.alauda.cn/georce/router</div></pre></td></tr></table></figure>
<p>在运行 Quagga 路由器之前，需要确保每个 Node 上 docker0 网桥的子网地址不能重叠，也不能与物理机所在的网络重叠，这需要网络管理员仔细规划。</p>
<p>下面以 3 个 Node 为例，使用 ifconfig 命令修改 docker0 网桥的地址和子网（假设 Node 所在的物理网络不是 10.1.X.X地址段）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Node 1: # ifconfig docker0 10.1.10.1/24</div><div class="line">Node 2: # ifconfig docker0 10.1.20.1/24</div><div class="line">Node 3: # ifconfig docker0 10.1.30.1/24</div></pre></td></tr></table></figure>
<p>然后在每个 Node 上启动 Quagga 容器。需要说明的是，Quagga 需要以 –privileged 特权模式运行，并且指定 –net=host，表示直接使用物理机的网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router</div></pre></td></tr></table></figure>
<p>启动成功后，Quagga 会相互学习来完成到其他机器的 docker0 路由的添加。</p>
<h2 id="使用-flannel-叠加网络"><a href="#使用-flannel-叠加网络" class="headerlink" title="使用 flannel 叠加网络"></a>使用 flannel 叠加网络</h2><p>falnnel 采用叠加网络（Overlay Network）模型来完成网络的打通。</p>
<h4 id="安装-etcd"><a href="#安装-etcd" class="headerlink" title="安装 etcd"></a>安装 etcd</h4><p>由于 flannel 使用 etcd 作为数据库，所以需要预先安装好 etcd。</p>
<h4 id="安装-flannel"><a href="#安装-flannel" class="headerlink" title="安装 flannel"></a>安装 flannel</h4><p>需要在每台 Node 上都安装 flannel。flannel 软件的下载地址为 (<a href="https://github.com/coreos/flannel/releases)。将下载的压缩包解压，把二进制文件" target="_blank" rel="external">https://github.com/coreos/flannel/releases)。将下载的压缩包解压，把二进制文件</a> flanneld 和 mk-docker-opts.sh 复制到 /usr/bin（或其他 PATH 环境变量中的目录），即可完成对 flannel 的安装。</p>
<h4 id="配置-flannel"><a href="#配置-flannel" class="headerlink" title="配置 flannel"></a>配置 flannel</h4><p>此处以使用 systemd 系统为例对 flanneld 服务进行配置。编辑服务配置文件 /etc/systemd/system/flanneld.service：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Flanneld overlay address etcd agent</div><div class="line">After=network.target</div><div class="line">Before=docker.service</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">EnvironmentFile=/etc/sysconfig/flanneld</div><div class="line">EnvironmentFile=-/etc/sysconfig/docker-network</div><div class="line">ExecStart=/usr/bin/flanneld -etcd-endpoints=$&#123;FLANNEL_ETCD&#125; $FLANNEL_OPTIONS</div><div class="line"></div><div class="line">[Install]</div><div class="line">RequiredBy=docker.service</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<p>编辑配置文件/etc/sysconfig/flannel，设置 etcd 的 URL 地址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># Flanneld configuration options</div><div class="line"># etcd url location. Point this to the server where etcd runs</div><div class="line">FLANNEL_ETCD=&quot;http://192.168.1.128:2379&quot;</div><div class="line"></div><div class="line"># etcd config key. THis is the configuration key that flannel queries</div><div class="line"># For address range assignment</div><div class="line">FLANNEL_ETCD_KEY=&quot;/coreos.com/network&quot;</div><div class="line"></div><div class="line"># Any additional options that you want to pass</div><div class="line">#FLANNEL_OPTIONS=&quot; &quot;</div></pre></td></tr></table></figure>
<p>在启动 flannel 之前，需要在 etcd 中添加一条网络配置记录，这个配置将用于 flannel 分配给每个 Docker 的虚拟 IP 地址段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># etcdctl set /coreos.com/network/config &apos;&#123; &quot;Network&quot; : &quot; 10.1.0.0/16 &quot; &#125;&apos;</div></pre></td></tr></table></figure>
<h4 id="由于-flannel-将覆盖-docker0-网桥，所以如果-Docker-服务网已启动，则停止-Docker-服务。"><a href="#由于-flannel-将覆盖-docker0-网桥，所以如果-Docker-服务网已启动，则停止-Docker-服务。" class="headerlink" title="由于 flannel 将覆盖 docker0 网桥，所以如果 Docker 服务网已启动，则停止 Docker 服务。"></a>由于 flannel 将覆盖 docker0 网桥，所以如果 Docker 服务网已启动，则停止 Docker 服务。</h4><h4 id="启动-flanneld-服务"><a href="#启动-flanneld-服务" class="headerlink" title="启动 flanneld 服务"></a>启动 flanneld 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart flanneld</div></pre></td></tr></table></figure>
<h4 id="在每个-Node-节点执行以下命令来完成对-docker0-网桥的配置："><a href="#在每个-Node-节点执行以下命令来完成对-docker0-网桥的配置：" class="headerlink" title="在每个 Node 节点执行以下命令来完成对 docker0 网桥的配置："></a>在每个 Node 节点执行以下命令来完成对 docker0 网桥的配置：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mk-docker-opts.sh -i</div><div class="line"># source /run/flannel/subnet.env</div><div class="line"># ifconfig docker0 $&#123;FLANNEL_SUBNET&#125;</div></pre></td></tr></table></figure>
<h4 id="重新启动-Docker-服务"><a href="#重新启动-Docker-服务" class="headerlink" title="重新启动 Docker 服务"></a>重新启动 Docker 服务</h4><h2 id="使用-Open-vSwitch"><a href="#使用-Open-vSwitch" class="headerlink" title="使用 Open vSwitch"></a>使用 Open vSwitch</h2><p>以两个 Node 为例，首先确保节点 192.168.18.128 的 Docker0 采用 172.17.43.0／24 网段，而 192.168.18.131 的 Docker0 采用 172.17.42.0/24 网段，对应参数为 DockerDaemon 进程里的 bip 参数。</p>
<h4 id="安装-ovs"><a href="#安装-ovs" class="headerlink" title="安装 ovs"></a>安装 ovs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum install openvswitch-x.x.x-x.x86_64.rpm</div></pre></td></tr></table></figure>
<p>禁止 SELINUX 功能，配置后重启机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># vi /etc/selinux/config</div><div class="line">SELINUX=disabled</div></pre></td></tr></table></figure>
<p>查看 Open vSwitch 的服务状态，应该启动两个进程：ovsdb-server 与 ovs-vswitchd。</p>
<h4 id="创建网桥和-GRE-隧道"><a href="#创建网桥和-GRE-隧道" class="headerlink" title="创建网桥和 GRE 隧道"></a>创建网桥和 GRE 隧道</h4><p>接下来需要在每个 Node 上建立 ovs 的网桥 br0，然后在网桥上创建一个 GRE 隧道连接对端网桥，最后把 ovs 的网桥 br0 作为一个端口连接到 docker0 这个 Linux 网桥上。这样以来，两个节点上的 docker0 网段就能互通了。</p>
<p>创建网桥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ovs-vsctl add-br br0</div></pre></td></tr></table></figure>
<p>创建 GRE 隧道连接对端，remote_ip 为对端 eth0 的网卡地址</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ovs-vsctl add-port br0 grel -- set interface grel type=gre option:remote_ip=192.168.18.128</div></pre></td></tr></table></figure>
<p>添加 br0 到本地 docker0，使得容器流量通过 OVS 流经 tunnel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># brctl addif docker0 br0</div></pre></td></tr></table></figure>
<p>启动 br0 与 docker0 网桥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ip link set dev br0 up</div><div class="line"># ip link set dev docker0 up</div></pre></td></tr></table></figure>
<p>添加路由规则。由于 192.168.18.128 与 192.168.18.131 的 docker0 网段分别为 172.17.43.0/24 与 172.17.42.0／24，这两个网段的路由都需要经过本机的 docker0 网桥路由，其中一个 24 网段是通过 OVS 的 GRE 隧道到达对端的，因此需要在每个 Node 上添加通过 docker0 网桥转发的 172.17.0.0/16 段的路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ip route add 172.17.0.0/16 dev docker0</div></pre></td></tr></table></figure>
<p>清空 Docker 自带的 iptables 规则及 Linux 的规则 ，后者存在拒绝 icmp 报文通过防火墙的规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># iptables -t nat -F; iptables -F</div></pre></td></tr></table></figure>
<p>在 192.168.18.131 上完成上述步骤后，在 192.168.18.128 节点执行同样的操作。注意，GRE 隧道里的 IP 地址药改为对端节点（192.168.18.131）的 IP 地址。</p>
<h4 id="Node-上容器之间的互通测试"><a href="#Node-上容器之间的互通测试" class="headerlink" title="Node 上容器之间的互通测试"></a>Node 上容器之间的互通测试</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># tshark -i br0 -R ip proto great</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/30/k8s_network/" class="archive-article-date">
  	<time datetime="2016-03-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-30</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_quota" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/29/k8s_quota/">Kubernetes 资源配额管理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>资源的配额管理功能主要用于对集群中可用的资源进行分配和限制。为了开启配额管理，需要设置 kube-apiserver 的 –admission_control 参数，使之家在这两个准入控制器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kube-apiserver ... --admission_control=LimitRanger,ResourceQuota...</div></pre></td></tr></table></figure>
<h2 id="指定容器配额"><a href="#指定容器配额" class="headerlink" title="指定容器配额"></a>指定容器配额</h2><p>对指定容器实施配额管理非常简单，只要在 Pod 或 ReplicationController 的定义文件中设定 resources 属性即可为某个容器指定配额。目前容器支持 CPU 和 Memory 两类资源的配额限制。</p>
<p>在下面这个 RC 定义文件中增加了 redis-master 的资源配额声明:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master</div><div class="line">  labels:</div><div class="line">    name: redis-master</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubenetes/redis-master</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 0.5</div><div class="line">            memory: 128Mi</div></pre></td></tr></table></figure>
<p>以上配置表示，系统将对名为 master 的容器限制 CPU 为 0.5（也可以写为500m），可用内存限制为 128MiB字节。</p>
<blockquote>
<p>1 KB(kilobyte) = 1000 bytes = 8000 bits</p>
<p>1 KiB(kibibyte) = 2^10 bytes = 1024 bytes = 8192 bits</p>
</blockquote>
<h2 id="全局默认配额"><a href="#全局默认配额" class="headerlink" title="全局默认配额"></a>全局默认配额</h2><p>除了可以直接在容器（或 RC）的定义文件中给指定的容器增加资源配额参数，我们还可以通过创建 LimitRange 对象来定义一个全局默认配额模版。这个默认配额模版会加载到集群中的每个 Pod 及容器上，这样就不用为每个 Pod 和容器重复设置了。</p>
<p>我们定义一个名为 limit-range-1 的 LimitRange：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: LimitRange</div><div class="line">metadata:</div><div class="line">  name: limit-range-1</div><div class="line">spec:</div><div class="line">  limits:</div><div class="line">    - type: &quot; Pod &quot; </div><div class="line">      max:</div><div class="line">        cpu: &quot; 2 &quot;</div><div class="line">        memory: 1Gi</div><div class="line">      min:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 32Mi</div><div class="line">    - type: &quot; Container &quot;</div><div class="line">      max:</div><div class="line">        cpu: &quot; 2 &quot;</div><div class="line">        memory: 1Gi</div><div class="line">      min:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 32Mi</div><div class="line">      default:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 64Mi</div></pre></td></tr></table></figure>
<p>上述设置表明：</p>
<ul>
<li>任意 Pod 内所有容器的 CPU 使用限制在 0.25～2；</li>
<li>任意 Pod 内所有容器的内存使用限制在 32Mi～1Gi；</li>
<li>任意容器的 CPU 使用限制在 0.25～2，默认值为 0.25；</li>
<li>任意容器的内存使用限制在 32Mi～1Gi，默认值为 64Mi。</li>
</ul>
<h2 id="多租户配额管理"><a href="#多租户配额管理" class="headerlink" title="多租户配额管理"></a>多租户配额管理</h2><p>多租户在 Kubernetes 中以 Namespace 来体现，这里的多租户可以是多个租户、多个业务系统或者相互隔离的多种作业环境。一个集群中的资源总是有限的，当这个集群被多个租户的应用同时使用时，为了更好地使用这种有限的共有资源，我们需要将资源配额的管理单元提升到租户级别，只需要在不同租户对应的 Namespace 上加载对应的 ResourceQuota 配置即可达到目的。</p>
<p>假设我们集群拥有的总资源为：CPU 共有 128core；内存总量为 1024GiB；有两个租户，分别是开发组和测试组，开发组的资源配额为 32core CPU 及 256GiB 内存，测试组的资源配额为 96core CPU 及 768GiB 内存。</p>
<p>首先，创建开发组对应的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: development</div></pre></td></tr></table></figure>
<p>接着，创建用于限定开发组的 ResourceQuota 对象，注意 metadata.namespace 属性被设定为开发组的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ResourceQuota</div><div class="line">metadata:</div><div class="line">  name: quota-development</div><div class="line">  namespace: development</div><div class="line">spec:</div><div class="line">  hard:</div><div class="line">    cpu: &quot; 32 &quot;</div><div class="line">    memory: 256Gi</div><div class="line">    persistentvolumeclaims: &quot; 10 &quot;</div><div class="line">    pods: &quot; 100 &quot;</div><div class="line">    replicationcontrollers: &quot; 50 &quot;</div><div class="line">    resourcequotas: &quot; 1 &quot;</div><div class="line">    secrets: &quot; 20 &quot;</div><div class="line">    services: &quot; 50 &quot;</div></pre></td></tr></table></figure>
<p>测试组相应的 namespace 与 ResourceQuota 同上，略。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/29/k8s_quota/" class="archive-article-date">
  	<time datetime="2016-03-28T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-29</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_op_skills" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/25/k8s_op_skills/">Kubernetes 常用运维技巧</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇文章对日常工作中常用的 Kubernetes 系统运维操作和技巧进行总结和详细说明。（不定期更新！）</p>
<h2 id="Node-的隔离和恢复"><a href="#Node-的隔离和恢复" class="headerlink" title="Node 的隔离和恢复"></a>Node 的隔离和恢复</h2><p>在硬件升级、维护等情况下，我们需要将某些 Node 进行隔离，脱离 Kubernetes 集群的调度范围。 Kubernetes 提供了一种机制，既可以将 Node 纳入调度范围，也可以将 Node 脱离调度范围。</p>
<p>创建配置文件 unschedule_node.yaml，在 spec 部分指定 unschedulable 为 true：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Node</div><div class="line">metadata:</div><div class="line">  name: kubernetes-minion1</div><div class="line">  labels:</div><div class="line">    kubernetes.io/hostname: kubernetes-minion1</div><div class="line">spec:</div><div class="line">  unschedulable: true</div></pre></td></tr></table></figure>
<p>然后，通过<code>kubectl replace</code>命令完成对 Node 状态的修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl replace -f unschedule_node.yaml</div></pre></td></tr></table></figure>
<p>查看 Node 状态，可以观察到在 Node 的状态中增加了一项 SchedulingDisabled。</p>
<p>对于后续创建的 Pod，系统将不会再向该 Node 进行调度。</p>
<p>另一种方法时不使用配置文件，直接使用<code>kubectl patch</code>命令完成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl patch node kubernetes-minion1 -p &apos;&#123;&quot;spec&quot;: &#123;&quot;unschedulable&quot;: true&#125;&#125;&apos;</div></pre></td></tr></table></figure>
<p>需要注意的是，将某个 Node 脱离调度范围时，在其上运行的 Pod 并不会自动停止，需要手动停止在该 Node 上运行的 Pod。。</p>
<p>同样，如果需要将某个 Node 重新纳入集群调度范围，则将 unschedulable 设置为 false，再次执行<code>kubectl replace</code>或<code>kubectl patch</code>命令就能恢复系统对该 Node 的调度。</p>
<h2 id="Node-的扩容"><a href="#Node-的扩容" class="headerlink" title="Node 的扩容"></a>Node 的扩容</h2><p>在实际生产系统中会经常遇到服务器容量不足的情况，这时就需要购买新的服务器，然后将应用系统进行水平扩展来完成对系统的扩容。</p>
<p>在 Kubernetes 集群中，对于一个新 Node 的加入是非常简单的。可以在 Node 节点上安装 <code>Docker、Kubelet 和 kube-proxy</code>服务，然后将 Kubelet 和 kube-proxy 的启动参数中的 Master URL 指定为当前 Kubernets 集群 Master 的地址，最后启动这些服务。基于 Kubelet 的自动注册机制，新的 Node 将会自动加入现有的 Kubetnetes 集群中。</p>
<p>Kubenetes Master 在接受了新 Node 的注册后，会自动将其纳入当前集群的调度范围内，在之后创建容器时，就可以向新的 Node 进行调度了。</p>
<h2 id="Pod-动态扩容和缩放"><a href="#Pod-动态扩容和缩放" class="headerlink" title="Pod 动态扩容和缩放"></a>Pod 动态扩容和缩放</h2><p>在实际生产系统中，我们经常会遇到某个服务需要扩容的场景，也可能会遇到由于资源紧张或者工作负责降低需要减少服务实例数的场景。此时我们可以利用命令<code>kubectl scale rc</code>来完成这些任务。比如通过执行以下命令将 redis-slave RC 控制的 Pod 副本数量更新为 3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl scale rc redis-slave --replicas=3</div></pre></td></tr></table></figure>
<p>将 –replicas 设置为比当前 Pod 副本数量更小的数字，系统将会“杀掉”一些运行中的 Pod，即可实现应用集群缩容。</p>
<h2 id="更新资源对象的-Label"><a href="#更新资源对象的-Label" class="headerlink" title="更新资源对象的 Label"></a>更新资源对象的 Label</h2><p>Label（标签）作为用户可灵活定义的对象属性，在已创建的对象上，仍然可以随时通过<code>kubectl label</code>命令对其进行增加、修改、删除等操作。</p>
<p>例如，我们要给已创建的 Pod “redis-master-bobr0”添加一个标签 role=backend：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label pod redis-master-bobr0 role=backend</div></pre></td></tr></table></figure>
<p>删除一个 Label，只需在命令后最后指定 Label 的 key 名并与一个减号相连即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label pod redis-master-bobr0 role-</div></pre></td></tr></table></figure>
<p>修改一个 Label 的值，需要加上 –overwrite 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl lable pod redis-master-bobr0 role=master --overwrite</div></pre></td></tr></table></figure>
<h2 id="将-Pod-调度到指定的-Node"><a href="#将-Pod-调度到指定的-Node" class="headerlink" title="将 Pod 调度到指定的 Node"></a>将 Pod 调度到指定的 Node</h2><p>Kubernetes 的 Scheduler 服务（kube-scheduler 进程）负责实现 Pod 的调度，整个调度过程通过一系列复杂的算法最终为每个 Pod 计算出一个最佳的目标节点，这一过程是自动完成的，我们无法知道 Pod 最终会被调度到哪个节点上。有时我们可能需要将 Pod 调度到一个指定的 Node 上，此时，我们可以通过 Node 的标签（Label）和 Pod 的 nodeSelector 属性相匹配，来达到上述目的。</p>
<p>首先，我们可以通过<code>kubectl label</code>命令给目标 Node 打上一个特定的标签，下面是此命令的完整用法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;</div></pre></td></tr></table></figure>
<p>这里，我们为 kubenetes-minion1 节点打上一个 zone=north 的标签，表明它是“北方”的一个节点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label nodes kubernetes-minion1 zone=north</div></pre></td></tr></table></figure>
<p>上述命令行操作也可以通过修改资源定义文件的方式，并执行<code>kubectl replace -f xxx.yaml</code>命令来完成。</p>
<p>然后，在 Pod 的配置文件中加入 nodeSelector 定义，以 redis-master-controller.yaml 为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master</div><div class="line">  labels:</div><div class="line">    name: redis-master</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubeguide/redis-master</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div><div class="line">      nodeSelector:</div><div class="line">        zone: north</div></pre></td></tr></table></figure>
<p>运行<code>kubectl create -f</code>命令创建 Pod，scheduler 就会将该 Pod 调度到拥有 zone=north 标签的 Node 上去。</p>
<p>使用<code>kubectl get pods -o wide</code>命令可以验证 Pod 所在的 Node。</p>
<p>如果我们给多个 Node 都定义了相同的标签，则 scheduler 将会根据调度算法从这组 Node 中挑选一个可用的 Node 进行 Pod 调度。</p>
<p>这种基于 Node 标签的调度方式灵活性很高，比如我们可以把一组 Node 分别贴上“开发环境”“测试环境”“正式环境”这三组标签中的一种，此时一个 Kubernetes 集群就承载了 3 个环境，这将大大提高开发效率。</p>
<p>需要注意的是，如果我们指定了 Pod 的 nodeSelector 条件，且集群中不存在包含相应标签的 Node 时，即使还有其他可供调度的 Node，这个 Pod 也最终会调度失败。</p>
<h2 id="应用的滚动升级"><a href="#应用的滚动升级" class="headerlink" title="应用的滚动升级"></a>应用的滚动升级</h2><p>当集群中的某个服务需要升级时，我们需要停止目前与该服务相关的所有 Pod，然后重新拉取镜像并启动。如果集群规模比较大，则这个工作就变成了一个挑战，而且先全部停止然后逐步升级的方式会导致较长时间的服务不可用。Kubernetes 提供了 rolling-update(滚动升级)功能来解决上述问题。</p>
<p>滚动升级通过执行<code>kubectl rolling-update</code>命令一键完成，该命令创建了一个新的 RC，然后自动控制旧的 RC 中的 Pod 副本数量逐渐减少到 0，同时新的 RC 中的 Pod 副本数量从 0 逐步增加到目标值，最终实现了 Pod 的升级。需要注意的是，系统要求新的 RC 需要与旧的 RC 在相同的命名空间（Namespace）内，即不能把别人的资产偷偷转移到自家名下。</p>
<p>以 redis-master 为例，假设当前运行的 redis-master Pod 是 1.0 版本，则现在需要升级到 2.0 版本。</p>
<p>创建 redis-master-controller-v2.yaml 的配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master-v2</div><div class="line">  version: v2</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">    version: v2</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">        version: v2</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubenetes/redis-master:2.0</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div></pre></td></tr></table></figure>
<p>在配置文件中有几处需要注意：</p>
<ol>
<li>RC 的名字（name）不能与旧的 RC 的名字相同；</li>
<li>在 selector 中应至少有一个 Label 与旧的 RC 的 Label 不同，以标识其为新的 RC。本例中新增了一个名为 version 的 Label，以与旧的 RC 进行区分。</li>
</ol>
<p>运行<code>kubectl rolling-update</code>命令完成 Pod 的滚动升级：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master -f redis-master-controller-v2.yaml</div></pre></td></tr></table></figure>
<p>等所有新的 Pod 启动完成后，旧的 Pod 也被全部销毁，这样就完成了容器集群的更新。</p>
<p>另一种方法是不使用配置文件，直接用<code>kubectl rolling-update</code>命令，加上<code>--image</code>参数指定新版镜像名称来完成 Pod 的滚动升级：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master --image=redis-master:2.0</div></pre></td></tr></table></figure>
<p>如果在更新过程中发现配置有误，则用户可以中断更新操作，并通过执行<code>kubectl rolling-update --rollback</code>完成 Pod 版本的回滚：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master --image=redis-master:2.0 --rollback</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/25/k8s_op_skills/" class="archive-article-date">
  	<time datetime="2016-03-24T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-25</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_object_files" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/24/k8s_object_files/">Kubernetes 关键对象定义详解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Pod定义文件详解"><a href="#Pod定义文件详解" class="headerlink" title="Pod定义文件详解"></a>Pod定义文件详解</h2><p>Pod 的定义模版（yaml 格式）如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: string</div><div class="line">  namespace: string</div><div class="line">  labels:</div><div class="line">    - name: string</div><div class="line">  annotations:</div><div class="line">    - name: string</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: string</div><div class="line">     image: string</div><div class="line">     imagePullPolicy: [Always | Never | IfNotPresent]</div><div class="line">     command: [string]</div><div class="line">     workingDir: string</div><div class="line">     volumeMounts:</div><div class="line">       - name: string</div><div class="line">        mountPath: string</div><div class="line">        readOnly: boolean</div><div class="line">     ports:</div><div class="line">       - name: string</div><div class="line">        containerPort: int</div><div class="line">        hostPost: int</div><div class="line">        protocol: string</div></pre></td></tr></table></figure>
<p>对各属性的详细说明如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名称</th>
<th style="text-align:center">取值类型</th>
<th style="text-align:center">是否必选</th>
<th style="text-align:center">取值说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">version</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">v1</td>
</tr>
<tr>
<td style="text-align:left">kind</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Pod</td>
</tr>
<tr>
<td style="text-align:left">metadata</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">元数据</td>
</tr>
<tr>
<td style="text-align:left">metadata.name</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Pod 名称，需符合 RFC 1035 规范</td>
</tr>
<tr>
<td style="text-align:left">metadata.namespace</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">命名空间，在不指定系统时将使用名为 “defautl” 的命名空间</td>
</tr>
<tr>
<td style="text-align:left">metadata.labels[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义标签属性列表</td>
</tr>
<tr>
<td style="text-align:left">metadata.annotation[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义注解属性列表</td>
</tr>
<tr>
<td style="text-align:left">spec</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">详细描述</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Pod 中运行的容器的列表</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">容器名称，需符合 RFC 1035 规范</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].image</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">容器的镜像名，在 Node 上如果不存在该镜像，则 Kubelet 会先下载</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].imagePullPolicy</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">获取镜像的策略，可选值包括：Always、Never、IfNotPresent，默认值为 Always。Always：表示每次都下载镜像。IfNotPresent：表示如果本地有该镜像，就是用本地的镜像。Never：表示仅使用本地镜像。</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].command[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器的启动命令列表，如果不指定，则使用镜像打包时使用的 CMD 命令。</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].workingDir</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器的工作目录</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].volumeMounts[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">可供容器使用的共享存储卷列表</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].volumeMounts[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">引用 Pod 定义的共享存储卷的名称，需使用 volumes[] 部分定义的共享存储卷名称</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].volumeMounts[].mountPath</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">存储卷在容器内 Mount 的绝对路径，应少于 512 个字符</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].volumeMounts[].readOnly</td>
<td style="text-align:center">boolean</td>
<td style="text-align:center"></td>
<td style="text-align:center">是否为只读模式，默认为读写模式</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].ports[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器需要暴露的端口号列表</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].ports[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">端口名称</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].ports[].containerPort</td>
<td style="text-align:center">Int</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器需要监听的端口号</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].ports[].hostPort</td>
<td style="text-align:center">Int</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器所在主机需要监听的端口号，默认与 containerPort 相同</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].ports[].protocol</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">端口协议，支持 TCP 和 UDP，默认为 TCP</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].env[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">容器运行前需要设置的环境变量列表</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].env[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">环境变量名称</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].env[].value</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">环境变量的值</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].resources</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">资源限制条件</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].resources.limits</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">资源限制条件</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].resources.limits.cpu</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">CPU 限制条件，将用于 docker run –cpu-shares 参数</td>
</tr>
<tr>
<td style="text-align:left">spec.containers[].resources.limits.memory</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">内存限制条件，将用于 docker run –memory 参数</td>
</tr>
<tr>
<td style="text-align:left">spec.volumes[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">在该 Pod 上定义的共享存储卷列表</td>
</tr>
<tr>
<td style="text-align:left">spec.volumes[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">共享存储卷名称，需唯一，符合 RFC 1035 规范。容器定义部分 containers[].volumeMounts[].name 将引用该共享存储卷的名称</td>
</tr>
<tr>
<td style="text-align:left">spec.volumes[].emptyDir</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">默认的存储卷类型，表示与 Pod 同生命周期的一个临时目录，其值为一个空对象：emptyDir:{}。该类型与 hostPath 类型互斥，应只定义一种</td>
</tr>
<tr>
<td style="text-align:left">spec.volumes[].hostPath</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">使用 Pod 所在主机的目录，通过 volumes[].hostPath.path 指定。该类型与 emptyDir 类型互斥，应只定义一种</td>
</tr>
<tr>
<td style="text-align:left">spec.volumes[].hostPath.path</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">Pod 所在主机的目录，将被用于容器中 mount 的目录。</td>
</tr>
<tr>
<td style="text-align:left">spec.dnsPolicy</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">DNS 策略，可选值包括：Default、ClusterFirst</td>
</tr>
<tr>
<td style="text-align:left">spec.restartPolicy</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">该 Pod 内容器的重启策略，可选值为 Always、OnFailure、Never，默认值为 Always。Always：容器一旦终止运行，无论容器是如何终止的，Kubelet 都将重启它。OnFailure：只有容器以非零退出码终止时，Kubelet 才会重启该容器。如果容器正常结束（退出码为0），则 Kubelet 将不会重启它。Never：容器终止后，Kubelet 将退出码报告给 Master，不再重启它</td>
</tr>
<tr>
<td style="text-align:left">spec.nodeSelector</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">指定需要调度到的 Node 的 Label，以 key=value 格式指定</td>
</tr>
<tr>
<td style="text-align:left">spec.imagePullSecrets</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">Pull 镜像时使用的 secret 名称，以 name=secretkey 格式定义</td>
</tr>
</tbody>
</table>
<h2 id="RC-定义文件详解"><a href="#RC-定义文件详解" class="headerlink" title="RC 定义文件详解"></a>RC 定义文件详解</h2><p>RC（ReplicationController）定义文件模版（yaml格式）如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: string</div><div class="line">  namespace: string</div><div class="line">  labels:</div><div class="line">    - name: string</div><div class="line">  annotations:</div><div class="line">    - name: string</div><div class="line">spec:</div><div class="line">  replicas: number</div><div class="line">  selector: []</div><div class="line">  template: object</div></pre></td></tr></table></figure>
<p>对各属性的说明如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名称</th>
<th style="text-align:center">取值类型</th>
<th style="text-align:center">是否必选</th>
<th style="text-align:center">取值说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">version</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">v1</td>
</tr>
<tr>
<td style="text-align:left">kind</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">ReplicationController</td>
</tr>
<tr>
<td style="text-align:left">metadata</td>
<td style="text-align:center">object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">元数据</td>
</tr>
<tr>
<td style="text-align:left">metadata.name</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">ReplicationController 名称，需符合 RFC 1035 规范</td>
</tr>
<tr>
<td style="text-align:left">metadata.namespace</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">命名空间，不指定系统时将使用名为 “default” 的命名空间</td>
</tr>
<tr>
<td style="text-align:left">metadata.labels[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义标签属性列表</td>
</tr>
<tr>
<td style="text-align:left">metadata.annotaion[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义注解属性列表</td>
</tr>
<tr>
<td style="text-align:left">spec</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">详细描述</td>
</tr>
<tr>
<td style="text-align:left">spec.replicas</td>
<td style="text-align:center">number</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Pod 副本数量，设置为 0 表示不创建 Pod</td>
</tr>
<tr>
<td style="text-align:left">spec.selector[]</td>
<td style="text-align:center">Listt</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Label Selector 配置，将选择具有指定 Label 标签的 Pod 作为管理范围</td>
</tr>
<tr>
<td style="text-align:left">spec.template</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">容器的定义，与 Pod 的 spec 内容相同</td>
</tr>
</tbody>
</table>
<h2 id="Service定义文件详解"><a href="#Service定义文件详解" class="headerlink" title="Service定义文件详解"></a>Service定义文件详解</h2><p>Service 的定义文件模版（yaml格式）如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: string</div><div class="line">  namespace: string</div><div class="line">  labels:</div><div class="line">    - name: string</div><div class="line">  annotations:</div><div class="line">    - name: string</div><div class="line">spec:</div><div class="line">  selector: []</div><div class="line">  type: string</div><div class="line">  clusterIP: string</div><div class="line">  sessionAffinity: string</div><div class="line">  ports:</div><div class="line">    - name: string</div><div class="line">     port: int</div><div class="line">     targetPort: int</div><div class="line">     protocol: string</div><div class="line">  status:</div><div class="line">    loadBalancer:</div><div class="line">      ingress:</div><div class="line">        ip: string</div><div class="line">        hostname: string</div></pre></td></tr></table></figure>
<p>对各属性的说明如下：</p>
<table>
<thead>
<tr>
<th style="text-align:left">属性名称</th>
<th style="text-align:center">取值类型</th>
<th style="text-align:center">是否必选</th>
<th style="text-align:center">取值说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">version</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">v1</td>
</tr>
<tr>
<td style="text-align:left">kind</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Service</td>
</tr>
<tr>
<td style="text-align:left">metadata</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">元数据</td>
</tr>
<tr>
<td style="text-align:left">metadata.name</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Service 名称，需符合 RFC 1035 规范</td>
</tr>
<tr>
<td style="text-align:left">metadata.namespace</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">命名空间，不指定系统时将使用名为“default”的命名空间</td>
</tr>
<tr>
<td style="text-align:left">metadata.labels[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义标签属性列表</td>
</tr>
<tr>
<td style="text-align:left">metadata.annotation[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">自定义注解属性列表</td>
</tr>
<tr>
<td style="text-align:left">spec</td>
<td style="text-align:center">Object</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">详细描述</td>
</tr>
<tr>
<td style="text-align:left">spec.selector[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Label Selector 配置，将选择具有指定 Label 标签的 Pod 作为管理范围</td>
</tr>
<tr>
<td style="text-align:left">spec.type</td>
<td style="text-align:center">String</td>
<td style="text-align:center">Required</td>
<td style="text-align:center">Service 的类型，指定 Service 的访问方式，默认为 ClusterIP。ClusterIP：虚拟的服务器 IP 地址，该地址用于 Kubernetes 集群内部的 Pod 访问，在 Node 上 kube-proxy 通过设置的 iptables 规则进行转发。NodePort：使用宿主机的端口，使能够访问各 Node 的外部客户端通过 Node 的 IP 地址和端口号就能访问服务。LoadBalancer：使用外接负载均衡器完成到服务的负载分发啊 ，需要在 spec.status.loadBalancer 字段指定外部负载均衡器的 IP 地址，并同时定义 nodePort 和 clusterIP</td>
</tr>
<tr>
<td style="text-align:left">spec.type.clusterIP</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">虚拟服务 IP 地址，当 type=ClusterIP 时，如果不指定，则系统将自动分配；当 type=LoadBalancer 时，则需要指定</td>
</tr>
<tr>
<td style="text-align:left">spec.sessionAffinity</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">是否支持 Session，可选值为 ClientIP，默认为空。ClientIP：表示将同一个客户端（根据客户端的IP地址决定）的访问请求都转发到同一个后端 Pod</td>
</tr>
<tr>
<td style="text-align:left">spec.ports[]</td>
<td style="text-align:center">List</td>
<td style="text-align:center"></td>
<td style="text-align:center">Service 需要暴露的端口号列表</td>
</tr>
<tr>
<td style="text-align:left">spec.ports[].name</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">端口名称</td>
</tr>
<tr>
<td style="text-align:left">spec.ports[].port</td>
<td style="text-align:center">Int</td>
<td style="text-align:center"></td>
<td style="text-align:center">服务监听的端口号</td>
</tr>
<tr>
<td style="text-align:left">spec.ports[].targetPort</td>
<td style="text-align:center">Int</td>
<td style="text-align:center"></td>
<td style="text-align:center">需要转发到后段 Pod 的端口号</td>
</tr>
<tr>
<td style="text-align:left">spec.ports[].protocol</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">端口协议，支持 TCP 和 UDP，默认为 TCP</td>
</tr>
<tr>
<td style="text-align:left">status</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">当 spec.type=LoadBalancer 时，设置外部负载均衡器的地址</td>
</tr>
<tr>
<td style="text-align:left">status.loadBalancer</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">外部负载均衡器</td>
</tr>
<tr>
<td style="text-align:left">status.loadBalancer.ingress</td>
<td style="text-align:center">Object</td>
<td style="text-align:center"></td>
<td style="text-align:center">外部负载均衡器</td>
</tr>
<tr>
<td style="text-align:left">status.loadBalancer.ingress.ip</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">外部负载均衡器的 IP 地址</td>
</tr>
<tr>
<td style="text-align:left">status.loadBalancer.ingress.hostname</td>
<td style="text-align:center">String</td>
<td style="text-align:center"></td>
<td style="text-align:center">外部负载均衡器的主机名</td>
</tr>
</tbody>
</table>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/24/k8s_object_files/" class="archive-article-date">
  	<time datetime="2016-03-23T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-24</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-fleet" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/02/fleet/">Fleet 跨节点服务调度</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在 Systemd 的许多工具中，例如 systemctl 命令，都有一个 “–host”参数，它提供了一种可以跨主机操作另一个节点上服务的方法。然而，这种方式仍然具有比较大的局限性。</p>
<ul>
<li>节点间必须相互添加 SSH Key 信任，因为 Systemd 的远程操作是通过 SSH 连接进行的。</li>
<li>操作时每次都必须指明目的节点的用户名和主机地址（IP 地址）。</li>
<li>只能够远程控制已有的服务，但不能远程创建服务。</li>
<li>仅限简单的服务操作，对于同时牵连多个节点的情况无能为力。例如，将服务迁移到另一个节点上运行。</li>
</ul>
<p>为了更方便地在集群中部署和管理服务，CoreOS 基于 Systemd 的接口设计了服务调度工具 FLeet，它继承并扩展了 Unit 文件格式，使之更加适用于集群环境的服务配置。</p>
<h2 id="Fleet的基本操作"><a href="#Fleet的基本操作" class="headerlink" title="Fleet的基本操作"></a>Fleet的基本操作</h2><p>Fleet 服务实际上是由运行在每一个节点上的后台服务进程 fleetd 组成的。此外，Fleet 还提供了一个用于交互控制的工具：fleetctl。 </p>
<ul>
<li>fleetctl list-machines 查看整个集群的基本信息</li>
<li>fleetctl list-units 查看整个集群的所有服务</li>
<li>fleetctl ssh ID    跳转到指定节点（需要添加公钥）</li>
<li>fleetctl ssh ID CMD 跨节点执行命令</li>
</ul>
<h2 id="通过-Unit-文件运行跨节点调度的服务"><a href="#通过-Unit-文件运行跨节点调度的服务" class="headerlink" title="通过 Unit 文件运行跨节点调度的服务"></a>通过 Unit 文件运行跨节点调度的服务</h2><h4 id="Fleet-的-Unit-文件"><a href="#Fleet-的-Unit-文件" class="headerlink" title="Fleet 的 Unit 文件"></a>Fleet 的 Unit 文件</h4><p>在服务管理方面，Fleet 使用和 Systemd 相似的 Unit 文件进行配置。不同的地方在于，Fleet 额外支持一个 X-Fleet 配置段，用于指定服务可以在哪些节点上运行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Hello World</div><div class="line">After=docker.service</div><div class="line">Requires=docker.service</div><div class="line"></div><div class="line">[Service]</div><div class="line">TimeoutStartSec=0</div><div class="line">ExecStartPre=-/usr/bin/docker kill busybox1</div><div class="line">ExecStartPre=-/usr/bin/docker rm busybox1</div><div class="line">ExecStartPre=/usr/bin/docker pull busybox</div><div class="line">ExecStart=/usr/bin/docker run --name busybox1 busybox /bin/sh -c &quot;while true; do echo Hello World; sleep 1; done&quot;</div><div class="line">ExecStop=/usr/bin/docker kill busybox1</div><div class="line"></div><div class="line">[X-Fleet]</div><div class="line">X-Conflicts=hello*.service</div></pre></td></tr></table></figure>
<p>最后的 X-Fleet 配置段的 X-Conflicts 属性指定了这个 Hello 服务不能运行在“任意已经分配了任何名字以 hello 开头的服务”的节点上。</p>
<h4 id="在集群上运行服务"><a href="#在集群上运行服务" class="headerlink" title="在集群上运行服务"></a>在集群上运行服务</h4><p>由于 Fleet 需要在集群层面上对服务进行管理，因此它的服务管理流程与 Systemd 略有不同。最明显的区别是，Fleet 没有指定 Unit 文件必须放置在哪些目录下 ，而是直接通过参数的方式告诉 fleetctl 命令。</p>
<p><code>$ fleetctl start ${HOME}/hello.service</code></p>
<h4 id="Fleet-的-X-Fleet-段"><a href="#Fleet-的-X-Fleet-段" class="headerlink" title="Fleet 的 X-Fleet 段"></a>Fleet 的 X-Fleet 段</h4><ul>
<li>MachineID: 直接了当地告诉 Fleet 这个服务只能运行在特定节点上。注意，这里的值必须是完整的节点 ID，这个 ID 可以通过 “fleetctl list-machines -l”命令获得。</li>
<li>MachineOf: 值是另一个“.service”文件，表示当前服务必须与指定的这个服务运行在同一个节点上。</li>
<li>MachineMetadata: 值是一个节点的 Metadata 内容，例如“region=us-east-1”，这些 Metadata 是在启动节点时通过 Cloudinit 写进去的。这个参数可以使用多次，或者通过空格将多个值同时传进去。</li>
<li>Conflicts: 值是一个 .service 文件名或用于匹配文件名的通配字符串，Conflicts 参数也可以使用多次，并且其值可以使用通配符，例如 apache* 表示所有以“apache”开头的服务。</li>
<li>Global: 如果值为 true，则这个服务会被部署到集群中符合 MachineMetadata 限定条件的每一个节点上。注意，当 Global 值为 true 时，除了 MachineMetadata 以外的所有其它约束条件都会被忽略。</li>
</ul>
<h4 id="模版参数"><a href="#模版参数" class="headerlink" title="模版参数"></a>模版参数</h4><p>Fleet 的 Unit 模版文件与 Systemd 的模版文件基本一致，同样在文件名的末尾有一个特征式的@符号。</p>
<p>虽然 Fleet 没有特定的 Unit 文件存放目录，但是只要在通过“fleetctl start”或“fleetctl submit”命令指定 Unit 文件路径时加上后缀参数，Fleet 同样会自动匹配去掉后缀参数的模版文件。例如 “fleetctl submit ${HOME}/apache@8080.service”，就会匹配到 ${HOME} 目录下面的 apache@.service 模版文件。</p>
<p>几乎所有的 Unit 模版文件都会使用到“%i”参数，因为它代表的是运行具体服务时，写在@符号后面部分的内容，这个值对于区分各个服务实例具有十分重要的意义。</p>
<h2 id="集群中的服务生命周期"><a href="#集群中的服务生命周期" class="headerlink" title="集群中的服务生命周期"></a>集群中的服务生命周期</h2><ul>
<li>fleetctl submit ${PWD}/xxx.service   提交服务，将指定的 Unit 文件添加到 Fleet 的记录缓存中。</li>
<li>fleetctl cat xxx.service   打印已经缓存了的 Unit 文件内容</li>
<li>fleetctl load xxx.service   就绪</li>
<li>fleetctl start xxx.service   启动服务,并设置自动启动</li>
<li>fleetctl stop xxx.service    停止服务</li>
<li>fleetctl status xxx.service   查看服务状态</li>
<li>fleetctl journal xxx.service   查看服务日志</li>
<li>fleetctl journal –lines 20 xx.service  最新20行日志</li>
<li>fleetctl journal –follow xxx.service   跟随日志输出</li>
</ul>
<h2 id="服务热迁移"><a href="#服务热迁移" class="headerlink" title="服务热迁移"></a>服务热迁移</h2><p>除了自动选择部署的节点外，Fleet的另一个重要功能便是在节点出现故障时，自动将故障节点上运行的服务迁移到另一个健康节点上。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/02/fleet/" class="archive-article-date">
  	<time datetime="2016-03-01T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-02</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Fleet/">Fleet</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-systemd_part3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/01/systemd_part3/">Systemd 工具集</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>除了整齐划一的 Unit 文件，Systemd 的魔力还体现在它自带的一套犹如百宝箱一样的工具集上。</p>
<table>
<thead>
<tr>
<th style="text-align:left">命令</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">systemctl</td>
<td style="text-align:center">用于检查和控制各种系统服务和资源的状态</td>
</tr>
<tr>
<td style="text-align:left">bootctl</td>
<td style="text-align:center">用于查看和管理系统启动分区</td>
</tr>
<tr>
<td style="text-align:left">hostnamectl</td>
<td style="text-align:center">用于查看和修改系统的主机名和主机信息</td>
</tr>
<tr>
<td style="text-align:left">journalctl</td>
<td style="text-align:center">用于查看系统日志和各类应用服务日志</td>
</tr>
<tr>
<td style="text-align:left">localectl</td>
<td style="text-align:center">用于查看和管理系统的地区信息</td>
</tr>
<tr>
<td style="text-align:left">loginctl</td>
<td style="text-align:center">用于管理系统已登录用户和 Session 信息</td>
</tr>
<tr>
<td style="text-align:left">machinectl</td>
<td style="text-align:center">用于操作 Systemd 容器</td>
</tr>
<tr>
<td style="text-align:left">timedatectl</td>
<td style="text-align:center">用于查看和管理系统的时间和时区信息</td>
</tr>
<tr>
<td style="text-align:left">systemd-analyze</td>
<td style="text-align:center">显示此次系统启动时运行每个服务所消耗的时间，可以用于分析系统启动过程中的性能瓶颈</td>
</tr>
<tr>
<td style="text-align:left">systemd-ask-password</td>
<td style="text-align:center">辅助性工具，用幸好遮障用户的任意输入，然后返回实际输入的内容</td>
</tr>
<tr>
<td style="text-align:left">systemd-cat</td>
<td style="text-align:center">用于将其它命令的输出重定向到系统日志</td>
</tr>
<tr>
<td style="text-align:left">systemd-cgls</td>
<td style="text-align:center">递归地显示指定 CGroup 的继承链</td>
</tr>
<tr>
<td style="text-align:left">systemd-cgtop</td>
<td style="text-align:center">显示系统当前最耗资源的 CGroup 单元</td>
</tr>
<tr>
<td style="text-align:left">systemd-escape</td>
<td style="text-align:center">辅助性工具，用于去除指定字符串中不能作为 Unit 文件名的字符</td>
</tr>
<tr>
<td style="text-align:left">systemd-hwdb</td>
<td style="text-align:center">Systemd 的内部工具，用于更新硬件数据库</td>
</tr>
<tr>
<td style="text-align:left">systemd-delta</td>
<td style="text-align:center">对比当前系统配置与默认系统配置的差异</td>
</tr>
<tr>
<td style="text-align:left">systemd-detect-virt</td>
<td style="text-align:center">显示主机的虚拟化类型</td>
</tr>
<tr>
<td style="text-align:left">systemd-inhibit</td>
<td style="text-align:center">用于强制延迟或禁止系统的关闭、睡眠和待机时间</td>
</tr>
<tr>
<td style="text-align:left">systemd-machine-id-setup</td>
<td style="text-align:center">Systemd 的内部工具，用于给 Systemd 容器生成 ID</td>
</tr>
<tr>
<td style="text-align:left">systemd-notify</td>
<td style="text-align:center">Systemd 的内部工具，用于通知服务的状态变化</td>
</tr>
<tr>
<td style="text-align:left">systemd-nspawn</td>
<td style="text-align:center">用于创建 Systemd 容器</td>
</tr>
<tr>
<td style="text-align:left">systemd-path</td>
<td style="text-align:center">Systemd 的内部工具，用于显示系统上下文中的各种路径配置</td>
</tr>
<tr>
<td style="text-align:left">systemd-run</td>
<td style="text-align:center">用于将任意指定的命令包装成一个临时的后台服务运行</td>
</tr>
<tr>
<td style="text-align:left">systemd-stdio-bridge</td>
<td style="text-align:center">Systemd 的内部工具 ，用于将程序的标准输入输出重定向到系统总线</td>
</tr>
<tr>
<td style="text-align:left">systemd-tmpfiles</td>
<td style="text-align:center">Systemd 的内部工具，用于创建和管理临时文件</td>
</tr>
<tr>
<td style="text-align:left">systemd-tty-ask-password-agent</td>
<td style="text-align:center">用于响应后台服务进程发出的输入密码请求</td>
</tr>
</tbody>
</table>
<h4 id="主机名、时间、地区信息管理"><a href="#主机名、时间、地区信息管理" class="headerlink" title="主机名、时间、地区信息管理"></a>主机名、时间、地区信息管理</h4><blockquote>
<p>hostnamectl        显示主机名</p>
<p>hostnamectl set-hostname xxx        设置主机名</p>
<p>–host 和 –machine 参数可以远程查看和修改另一个主机或容器。 </p>
</blockquote>
<p><em>timedatectl 和 localectl 作用类似</em>。</p>
<h4 id="电源管理"><a href="#电源管理" class="headerlink" title="电源管理"></a>电源管理</h4><blockquote>
<p>systemctl poweroff | halt        关机</p>
<p>systemctl reboot        重启</p>
<p>systemctl suspend        待机</p>
<p>systemctl hibernate        休眠</p>
</blockquote>
<h4 id="启动时间和运行状态分析"><a href="#启动时间和运行状态分析" class="headerlink" title="启动时间和运行状态分析"></a>启动时间和运行状态分析</h4><blockquote>
<p>systemd-analyze time        打印最近一次启动消耗的时间</p>
<p>systemd-analyze blame | head      按启动耗时长短排序</p>
<p>systemd-analyze plot        生成详细 SVG 图</p>
<p>systemd-analyze dump      打印当前所有 Unit 状态</p>
<p>systemd-analyze dot      生成系统中所有 Unit 的依赖关系图</p>
<p>systemd-anylyze set-log-level emerg | alert | crit | err | warning | notice | info | debug</p>
<p>systemd-analyze verify xxx      检测 Unit 文件格式正确性</p>
</blockquote>
<h4 id="辅助性命令工具"><a href="#辅助性命令工具" class="headerlink" title="辅助性命令工具"></a>辅助性命令工具</h4><h6 id="systemd-ask-password"><a href="#systemd-ask-password" class="headerlink" title="systemd-ask-password"></a>systemd-ask-password</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ PASSWORD=$(systemd-ask-password &quot;Hint test here:&quot;)</div><div class="line">Hint test here: ***</div><div class="line">$ echo $(PASSWORD)</div><div class="line">abc</div></pre></td></tr></table></figure>
<h6 id="systemd-run"><a href="#systemd-run" class="headerlink" title="systemd-run"></a>systemd-run</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ sudo systemd-run --uid=root --gid=root du -sh /*</div></pre></td></tr></table></figure>
<p>当服务正常结束后（返回值为 0）会自动从 Systemd 的记录中消失，但其留下的日志仍然可以通过 journalctl 查看到。反之，如果服务异常结束（返回值非 0），则这个服务的记录会继续留在 Systemd 中，可以通过 “systemctl list-units”命令查看到。</p>
<p><code>$ sudo systemd-run --on-active=30 --time-property=AccuracySec=100ms /bin/touch/tmp/time-up</code>生成临时定时器。</p>
<p>此时系统创建了两个临时的 Unit 文件。需要注意一点，其中的服务 Unit 文件会在定时器时间到时，且响应的任务顺利完成后自动删除。然后，定时器的 Unit 文件并不会随着任务的完成而自动消失，需要手工执行“systemctl stop”命令来移除。</p>
<p><code>systemctl reset-failed</code>命令可以用来清理失败的临时服务。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/01/systemd_part3/" class="archive-article-date">
  	<time datetime="2016-02-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Systemd/">Systemd</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-systemd_part2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/01/systemd_part2/">Systemd 的系统资源管理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>对服务的管理知识 Systemd 系统管理功能的冰山一角。在 Systemd 的生态圈中，Unit 文件统一了过去各种不同的系统资源配置格式，例如服务的启／停、定时任务、设备自动挂载、网络配置、设备配置、虚拟内存配置等。</p>
<h2 id="Systemd-的-Unit-文件"><a href="#Systemd-的-Unit-文件" class="headerlink" title="Systemd 的 Unit 文件"></a>Systemd 的 Unit 文件</h2><p>Systemd 通过不同的文件后缀名来区分这些配置文件，下表列举了 Systemd 所支持的 12 中 Unit 文件类型。</p>
<table>
<thead>
<tr>
<th style="text-align:left">后缀名</th>
<th style="text-align:center">作用</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">.automount</td>
<td style="text-align:center">用于控制自动挂载文件系统。即当指定的目录被访问时，立即会被自动挂载</td>
</tr>
<tr>
<td style="text-align:left">.deviece</td>
<td style="text-align:center">对应 /dev 目录下的设备，主要用于定义设备之间的依赖关系</td>
</tr>
<tr>
<td style="text-align:left">.mount</td>
<td style="text-align:center">定义系统结构层次中的一个挂载点，可以替代过去的 /etc/fstab 配置文件</td>
</tr>
<tr>
<td style="text-align:left">.path</td>
<td style="text-align:center">用于监控指定目录或文件的变化，并触发其它 Unit 运行</td>
</tr>
<tr>
<td style="text-align:left">.scope</td>
<td style="text-align:center">这种 Unit 文件不是用户创建的，而是 Systemd 运行时自己产生的，描述一些系统服务的分组信息</td>
</tr>
<tr>
<td style="text-align:left">.service</td>
<td style="text-align:center">封装守护进程的启动、停止、重启和重载操作，是最常见的一种 Unit 文件</td>
</tr>
<tr>
<td style="text-align:left">.slice</td>
<td style="text-align:center">用于表示一个 CGroup 的树，通常用户不会自己创建这样的 Unit 文件</td>
</tr>
<tr>
<td style="text-align:left">.snapshot</td>
<td style="text-align:center">用于表示一个由 systemctl snapshot 命令创建的 Systemd Units 运行状态快照</td>
</tr>
<tr>
<td style="text-align:left">.socket</td>
<td style="text-align:center">监控来自于系统或网络的数据消息，用于实现基于数据自动触发服务启动</td>
</tr>
<tr>
<td style="text-align:left">.swap</td>
<td style="text-align:center">定义一个用于做虚拟内存的交换分区</td>
</tr>
<tr>
<td style="text-align:left">.target</td>
<td style="text-align:center">用于对 Unit 进行逻辑分组，引导其它 Unit 的执行</td>
</tr>
<tr>
<td style="text-align:left">.timer</td>
<td style="text-align:center">用于配置在特定时间出发的任务，替代了 Crontab 的功能</td>
</tr>
</tbody>
</table>
<h4 id="Systemd-的-Unit-文件存放目录"><a href="#Systemd-的-Unit-文件存放目录" class="headerlink" title="Systemd 的 Unit 文件存放目录"></a>Systemd 的 Unit 文件存放目录</h4><table>
<thead>
<tr>
<th style="text-align:left">路径</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">/etc/systemd/system</td>
<td style="text-align:center">系统或用户提供的配置文件</td>
</tr>
<tr>
<td style="text-align:left">/run/systemd/system</td>
<td style="text-align:center">软件运行时生成的配置文件</td>
</tr>
<tr>
<td style="text-align:left">/usr/lib64/systemd/system</td>
<td style="text-align:center">系统或第三方软件安装时添加的配置文件</td>
</tr>
</tbody>
</table>
<p>CoreOS 系统提供的 Unit 文件大多放在 <code>/usr/lib64/systemd/system</code> 目录中，而<strong>/usr/lib64/systemd/system 这个目录在 CoreOS 中属于系统只堵分区</strong>。如果需要修改系统服务的管理配置，可以将这个目录中相应 Unit 文件复制到 <code>/etc/systemd/system</code> 中修改即可。</p>
<h2 id="定时器"><a href="#定时器" class="headerlink" title="定时器"></a>定时器</h2><h4 id="定时器的-Unit-文件"><a href="#定时器的-Unit-文件" class="headerlink" title="定时器的 Unit 文件"></a>定时器的 Unit 文件</h4><p>定时器的 Unit 文件除了通用的 Unit 段和 Install 段以外，还有一个特别的 Timer 段，这个区段的属性主要定义了定时器的触发时机和触发的任务。</p>
<ul>
<li>OnActiveSec: 当“Timer 自己被启动后”的指定秒数时，触发定时器任务。</li>
<li>OnBootSec: 当“主机开始启动后”的指定秒数时，触发定时器任务。</li>
<li>OnstartupSec: 当“操作系统启动完成后”的指定秒数时，触发定时器任务。</li>
<li>OnUnitActiveSec: 当“定时器中的指定服务被启动后”的指定秒数时，触发定时器任务。</li>
<li>OnUnitInactiveSec: 当“定时器中的指定服务停止后”的指定秒数时，触发定时器任务。</li>
</ul>
<p><em>可以同时使用多个触发时机。</em></p>
<p>以上这些属性的值都可以是一个数字（即秒数），或一个合法的字符串时间，可以用的单位如下表所示。</p>
<table>
<thead>
<tr>
<th style="text-align:left">单位</th>
<th style="text-align:center">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">usec,us</td>
<td style="text-align:center">微秒</td>
</tr>
<tr>
<td style="text-align:left">msec,ms</td>
<td style="text-align:center">毫秒</td>
</tr>
<tr>
<td style="text-align:left">seconds,second,sec,s</td>
<td style="text-align:center">秒</td>
</tr>
<tr>
<td style="text-align:left">minuters,minute,min,m</td>
<td style="text-align:center">分钟</td>
</tr>
<tr>
<td style="text-align:left">hours,hour,hr,h</td>
<td style="text-align:center">小时</td>
</tr>
<tr>
<td style="text-align:left">days,day,d</td>
<td style="text-align:center">天</td>
</tr>
<tr>
<td style="text-align:left">weeks,week,w</td>
<td style="text-align:center">周</td>
</tr>
<tr>
<td style="text-align:left">months,month</td>
<td style="text-align:center">月</td>
</tr>
<tr>
<td style="text-align:left">years,year,y</td>
<td style="text-align:center">年</td>
</tr>
</tbody>
</table>
<ul>
<li><p>OnCalendar: 使用日历时间，在特定日期触发。常用的日期表示格式为 yyyy-MM-dd-hh:mm:ss,例如 2015-07-23 11:12:13。</p>
</li>
<li><p>Unit: 这个参数的值是一个服务 Unit 文件的名称。当定时器事件触发时，就会执行这个服务指定的指令。如果没有配置这个属性，默认会将与这个定时器同名（但后缀名是.service）的服务 Unit 文件作为被指定的任务。</p>
</li>
<li><p>AccuracySec: 这个值设定了触发事件的精度，默认为 1min。在精度范围内到期的定时器会在一个随机时间里被触发，防止在某个整点时刻出现大量并发的定时器事件。</p>
</li>
<li>Persistent: 值为 true 或 false。默认为 false，当设为 true 时，定时器启动时会检查从上次定时器结束到当前时刻是否有漏掉的 OnCalendar 触发事件，如果有则立即触发一次指定的任务；当设为 false 时，会忽略在定时器停止期间错过的 OnCalendar 事件。 </li>
<li>WakeSystem: 值为 true 或 false。默认为 false，当设为 true 时，如果定时器触发时系统处于挂起（Suspend）状态，则会自动唤醒系统并执行任务。当设为 false 时，会忽略在系统挂起期间的事件。</li>
</ul>
<h4 id="systemd-tmpfiles-clean-timer"><a href="#systemd-tmpfiles-clean-timer" class="headerlink" title="systemd-tmpfiles-clean.timer"></a>systemd-tmpfiles-clean.timer</h4><p>下面的 Unit 文件是 CoreOS 用于自动清理临时文件的定时器配置文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">description=Daily Cleanup of Temporary Directories</div><div class="line">Documentation=man:tmpfiles.d(5) man:systemd-tmpfiles(8)</div><div class="line"></div><div class="line">[Timer]</div><div class="line">OnBootSec=15min</div><div class="line">OnUnitActiceSec=1d</div></pre></td></tr></table></figure>
<h4 id="启动定时器"><a href="#启动定时器" class="headerlink" title="启动定时器"></a>启动定时器</h4><p><code>$ sudo systemctl start xxx.timer</code> 启动定时器</p>
<p><code>$ sudo systemctl enable xxx.timer</code> 注册为随系统启动</p>
<p><code>systemctl stop</code> 和 <code>systemctl disable</code> 同理。</p>
<h2 id="路径监控器"><a href="#路径监控器" class="headerlink" title="路径监控器"></a>路径监控器</h2><p>后缀名为“.path”的 Unit 文件是用来监控目录变化的。它的效果和定时器有些相似，也是在满足特定的条件时执行另一个作为指定任务的服务 Unit 文件。</p>
<h4 id="路径监控器的-Unit-文件"><a href="#路径监控器的-Unit-文件" class="headerlink" title="路径监控器的 Unit 文件"></a>路径监控器的 Unit 文件</h4><p>路径监控器的 Unit 文件的特有配置段是 Path，用于配置监控的事件、目录和需要执行的任务。</p>
<ul>
<li>PathExists: 列在这个属性后面的路径如果存在（被创建），就是触发任务。</li>
<li>PathExistsGlob: 功能和 PathExists 基本相同，但是这个属性后面的路径可以使用通配符。</li>
<li>PathChanged: 列在这个属性后面的文件和目录如果被修改或更新了，就会触发任务。</li>
<li>PathModified: 功能和 PathChanged 相似，不过前者只会在监听文件修改完成（文件句柄被释放）时触发任务，而这个属性后面的文件在每次修改被保存时都会触发任务。</li>
<li>DirectoryNotEmpty: 列在这个属性后面的目录如果不是空的，就是触发任务。</li>
</ul>
<p>在同一个目录监控器中，可以同时使用多个监听条件，当任意一个触发时，都会执行指定的服务。</p>
<ul>
<li><p>Unit: 这个参数的值是一个服务 Unit 文件的名称。当路径监控器的事件被触发时，就会执行这个服务指定的命令。如果没有配置这个属性，默认会将与这个监控器同名（但后缀名是.service）的服务 Unit 文件作为被指定的任务。</p>
</li>
<li><p>MakeDirectory: 值为 true 或 false。默认为 false，当设为 true 时，如果被监控的路径不存在，则会自动将其创建为一个目录（PathExists 事件监控的目录除外）。当设为 false 时，没有这个行为。</p>
</li>
<li>DirectoryMode: 这个属性用于指定自动创建的目录具有怎样的权限，默认值是 0755。</li>
</ul>
<h4 id="motdgen-path"><a href="#motdgen-path" class="headerlink" title="motdgen.path"></a>motdgen.path</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Watch for update engine configuration changes</div><div class="line"></div><div class="line">[Path]</div><div class="line">PathChanged=/etc/coreos/update.conf</div></pre></td></tr></table></figure>
<h2 id="数据监控器"><a href="#数据监控器" class="headerlink" title="数据监控器"></a>数据监控器</h2><p>后缀为“.socket”的 Unit 文件可以监控系统的指定 TCP／UDP 端口、系统消息队列、特殊设备、FIFO 管道或套接字文字的数据。</p>
<p>当检测到有数据来时，就启动指定服务。这样做的好处是，当操作系统启动时，许多对外提供服务的应用并不需要随系统立即开启，知道有真实的用户请求访问其监听的端口时，才会按需启动，从而节约了系统的开机时间。</p>
<h4 id="数据监控器的-Unit-文件"><a href="#数据监控器的-Unit-文件" class="headerlink" title="数据监控器的 Unit 文件"></a>数据监控器的 Unit 文件</h4><ul>
<li>ListenStream: 监听指定的 TCP 端口或 UNIX 套接字。（IP:PORT）</li>
<li>ListenDatagram: 监听指定的 UDP 端口。</li>
<li>ListenSequentialPacket: 监听指定的 Unix 套接字文件。</li>
<li>ListenFIFO: 监听指定的 FIFO（命名管道）文件。</li>
<li>ListenMessageQueue: 监听指定的 POSIX 消息队列。</li>
<li>ListenSpecial: 监听指定的字符设备文件或特殊设备文件。</li>
</ul>
<p>在同一个数据监控器中，可以同时使用多个监听数据来源，当任意一个有数据到来时，都会执行指定的服务。</p>
<ul>
<li><p>Service: 这个参数的值是一个服务 Unit 文件的名称。当监控到数据到达时，Systemd 就会启动这个服务并处理接收到的数据。如果没有配置这个属性，默认会将与这个监控器同名（但后缀名是.service）的服务 Unit 文件作为被指定的任务。</p>
</li>
<li><p>Accept: 值为 true 或 false。默认为 false，当设为 true 时，Systemd 会为每个连接请求创建一个新服务实例，通常只用于有连接的通信协议，列入监控 TCP 端口。当设为 false 时，Systemd会检查指定的服务是否已经在运行，如果没有运行，就会启动这个服务。</p>
</li>
<li>MaxConnections: 仅当“Accept=true”时有效，限制最大连接数，也就是最大的服务运行数量，超过这个数量的连接会被直接拒绝。</li>
<li>KeepAlive: 值为 true 或 false，只对 TCP 连接有效。默认为 false，当设为 true 时，建立的连接会被设置为长连接。</li>
<li>ExecStartPre 和 ExecStartPost: 在执行指定服务启动开始前和完成后需要执行的额外命令。</li>
<li>ExecStopPre 和 ExecStopPost: 在执行指定服务即将结束时和完整结束后需要执行的额外命令。</li>
<li>TimeoutSec: 指定 ExecStartPre、ExecStartPost、ExecStopPre 和 ExecStopPost 中每个命令的最长等待时间。</li>
<li>ReceiveBuffer 和 SendBuffer: 数据接收和发送缓存的大小。</li>
<li>RemoveOnStop: 值为 true 或 false，对系统消息队列、FIFO 管道或套接字文件有效。默认为 false，若设为 true，服务结束时会同时删除监控的文件。</li>
</ul>
<h4 id="sshd-socket"><a href="#sshd-socket" class="headerlink" title="sshd.socket"></a>sshd.socket</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=OpenSSH Server Socket</div><div class="line">Conflicts=sshd.service</div><div class="line"></div><div class="line">[Socket]</div><div class="line">ListenStream=22</div><div class="line">Accept=yes</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=sockets.target</div></pre></td></tr></table></figure>
<h2 id="挂载文件系统"><a href="#挂载文件系统" class="headerlink" title="挂载文件系统"></a>挂载文件系统</h2><p>后缀名为“.mount”的 Unit 文件用于记录和处理挂载文件系统。</p>
<h4 id="挂载点的-Unit-文件"><a href="#挂载点的-Unit-文件" class="headerlink" title="挂载点的 Unit 文件"></a>挂载点的 Unit 文件</h4><p>挂载点的 Unit 文件名必须以转义后的挂载目录命名。庄毅规则大致是将路径符号“／”替换为“－”，原本文件名中的“－”和其它非 ASCII 字符替换为“\x”开头的编号。</p>
<p>可以将挂载点路径去掉开头的分隔符作为“systemd-escape”命令的参数，从而获得转义的 Unix 文件名。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ systemd-escape media/my-mount-point</div><div class="line">media-my\x2dmount\x2dpoint</div></pre></td></tr></table></figure>
<ul>
<li>What: 被挂载设备的绝对路径。</li>
<li>Where: 挂载点目录的绝对路径，这个路径应该与该 Unit 文件名相对应。</li>
<li>Type: 挂载文件系统的类型。</li>
<li>Options: 挂载的参数，用逗号分隔。</li>
<li>SloppyOptions: 值为 true 或 false。默认为 false，若设为 true，当遇到无法识别的挂载参数时，会出错退出；若设为 false，则忽略无法识别的挂载参数。</li>
<li>DirectoryMode: 如果指定的挂载目录不存在时， Systemd 会将它自动创建出来。这个参数指定了目录的权限，默认为 0755。</li>
<li>TimeoutSec: 挂载超时的秒数。如果到了这个时间挂载仍然没有完成，则会判定为失败。</li>
</ul>
<h4 id="usr-share-oem-mount"><a href="#usr-share-oem-mount" class="headerlink" title="usr-share-oem.mount"></a>usr-share-oem.mount</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">DefaultDependencies=no</div><div class="line">Wants=addon-run@usr-share-oem.service addon-config@usr-share-oem.service</div><div class="line">Conflicts=umount.target</div><div class="line">Before=local-fs.target umount.target ldconfig.service</div><div class="line">ConditionVirtualization=!container</div><div class="line">ConditionPathExists=!/usr/.noupdate</div><div class="line"></div><div class="line">[Mount]</div><div class="line">What=/dev/disk/by-label/OEM</div><div class="line">Where=/usr/share/oem</div><div class="line">Type=ext4</div></pre></td></tr></table></figure>
<p>这个配置的效果等同于<code>mkdir -p /usr/share/oem; mount -t ext4 -o nodev,commit=600 /dev/disk/by-label/OEM /usr/share/oem</code>命令，但通过 Systemd 统一管理后，看起来更简洁了。</p>
<h2 id="自动挂载文件系统"><a href="#自动挂载文件系统" class="headerlink" title="自动挂载文件系统"></a>自动挂载文件系统</h2><p>后缀名为“.automount”的 Unit 文件用于当用户访问某个目录时，自动将设定的文件系统挂载上去。在过去的 Linux 中，这个任务是由 autofs 服务完成的。</p>
<h4 id="自动挂载点的-Unit-文件"><a href="#自动挂载点的-Unit-文件" class="headerlink" title="自动挂载点的 Unit 文件"></a>自动挂载点的 Unit 文件</h4><p>自动挂载点的配置相对简单，但它必须与一个同名的“.mount”挂载点 Unit 文件配合使用。且这类 Unit 文件的命名同样必须与挂载目录转移后的名称一致的约定。</p>
<ul>
<li>Where: 挂载点目录的绝对路径，这个路径应该与该 Unit 文件相对应。</li>
<li>DirectoryMode: 如果指定的挂载目录不存在时，Systemd 会将它自动创建出来。这个参数指定了目录的权限，默认是 0755。</li>
<li>TimeoutIdleSec: 这个配置可以指定一个超时时间。当挂载的目录在这段时间内都没有使用时，Systemd酒会尝试自动卸载它。将这个值设为0，则会禁用这个功能。默认这个功能是禁止的。</li>
</ul>
<h4 id="etc-systemd-system-usr-share-oem-automount"><a href="#etc-systemd-system-usr-share-oem-automount" class="headerlink" title="/etc/systemd/system/usr-share-oem.automount"></a>/etc/systemd/system/usr-share-oem.automount</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Descrition=Test automount</div><div class="line"></div><div class="line">[Automount]</div><div class="line">Where=/usr/share/oem</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<h2 id="交换分区（虚拟内存）"><a href="#交换分区（虚拟内存）" class="headerlink" title="交换分区（虚拟内存）"></a>交换分区（虚拟内存）</h2><p>除了普通的挂载点，过去的 /etc/fstab 配置文件还有一项功能是交换分区的挂载。这部分功能在 Systemd 中是用专门的 Unit 文件完成的，它的后缀名为“.swap”。</p>
<p>作为一种特殊的挂载点，这类 Unit 文件的命名也必须遵守与挂载目录转义后名称一直的约定。</p>
<h4 id="交换分区的-Unit-文件"><a href="#交换分区的-Unit-文件" class="headerlink" title="交换分区的 Unit 文件"></a>交换分区的 Unit 文件</h4><ul>
<li>What: 用于做交换分区的设备或文件的绝对路径。</li>
<li>Priority: 当有多个交换分区时，这个值回决定使用每个分区的优先级。</li>
<li>Options: 挂载交换分区的参数，这些参数会在实际挂载和卸载交换分区时，传递给 swapon 和 swapoff 命令。</li>
<li>TimeoutSec: 挂载交换分区的超时时间。如果在指定的时间内 swapon 命令仍然没有完成，则会被认为挂载失败。将这个值设为 0，则会禁用这个功能。默认这个功能是禁止的。</li>
</ul>
<h4 id="etc-systemd-system-swapfile-swap"><a href="#etc-systemd-system-swapfile-swap" class="headerlink" title="/etc/systemd/system/swapfile.swap"></a>/etc/systemd/system/swapfile.swap</h4><p>先创建一个大小为 2GB 的空白文件 swapfile。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ sudo dd if=/dev/zero of=/swapfile bs=2048 count=1MB</div><div class="line">$ sudo chmod 0600 /swapfile</div><div class="line">$ sudo mkswap /swapfile</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Test swap file</div><div class="line"></div><div class="line">[Swap]</div><div class="line">What=/swapfile</div><div class="line"></div><div class="line">[Install]</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/01/systemd_part2/" class="archive-article-date">
  	<time datetime="2016-02-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Systemd/">Systemd</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2019 dingmingk
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cgroup/" style="font-size: 12.5px;">Cgroup</a> <a href="/tags/Continuous/" style="font-size: 10px;">Continuous</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/Fleet/" style="font-size: 12.5px;">Fleet</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Jenkins/" style="font-size: 12.5px;">Jenkins</a> <a href="/tags/Kubernetes/" style="font-size: 20px;">Kubernetes</a> <a href="/tags/Life/" style="font-size: 12.5px;">Life</a> <a href="/tags/Linux/" style="font-size: 12.5px;">Linux</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Nginx/" style="font-size: 17.5px;">Nginx</a> <a href="/tags/Pipeline/" style="font-size: 10px;">Pipeline</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/SaltStack/" style="font-size: 10px;">SaltStack</a> <a href="/tags/Systemd/" style="font-size: 15px;">Systemd</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>