<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>他的国</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="他的国">
<meta property="og:url" content="https://dingmingk.github.io/index.html">
<meta property="og:site_name" content="他的国">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="他的国">
  
    <link rel="alternative" href="/atom.xml" title="他的国" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <script src="/style.js"></script>
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">dingmingk</a></h1>
		</hgroup>

		
		<p class="header-subtitle">做一个安静的美男子</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="3" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
		        
					<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">dingmingk</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">dingmingk</h1>
			</hgroup>
			
			<p class="header-subtitle">做一个安静的美男子</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
			        
						<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-k8s_pipeline" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/18/k8s_pipeline/">Kubernetes Pipeline</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Kubernetes-集群基于-Jenkins-的-CI-CD-流程实践"><a href="#Kubernetes-集群基于-Jenkins-的-CI-CD-流程实践" class="headerlink" title="Kubernetes 集群基于 Jenkins 的 CI/CD 流程实践"></a>Kubernetes 集群基于 Jenkins 的 CI/CD 流程实践</h2><p>通过在 Kubernetes 集群上创建并配置 Jenkins Server 实现应用开发管理的 CI/CD 流程，并且利用 Kubernetes-Jenkins-Plugin 实现动态按需扩展 jenkins-slave。</p>
<h3 id="安装-Kubernetes-集群"><a href="#安装-Kubernetes-集群" class="headerlink" title="安装 Kubernetes 集群"></a>安装 Kubernetes 集群</h3><p>首先，需要有一个 Kubernetes 集群，本地可以运行 Minikube。</p>
<p>安装 Helm。</p>
<h3 id="安装-Jenkins-Server"><a href="#安装-Jenkins-Server" class="headerlink" title="安装 Jenkins Server"></a>安装 Jenkins Server</h3><p><code>helm install stable/jenkins --set rbac.install=true</code></p>
<blockquote>
<p>详细的配置可以下载 <a href="https://github.com/kubernetes/charts.git" target="_blank" rel="external">https://github.com/kubernetes/charts.git</a> 仓库查看并修改。</p>
</blockquote>
<p>正常启动后可以看到如下提示，获取 Jenkins 地址和初始密码等。</p>
<p><img src="/assets/blogImg/k8s_pipeline_jenkins01.png" alt="k8s_pipeline_jenkins01"></p>
<h3 id="访问-Jenkins"><a href="#访问-Jenkins" class="headerlink" title="访问 Jenkins"></a>访问 Jenkins</h3><p><strong>查看服务</strong></p>
<p><code>kubectl get svc</code></p>
<p><img src="/assets/blogImg/k8s_pipeline_jenkins02.png" alt="k8s_pipeline_jenkins02"></p>
<p>浏览器打开 <a href="http://NodeIP:NodePort" target="_blank" rel="external">http://NodeIP:NodePort</a> 即可访问。</p>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>登录 Jenkins 后，新建一个流水线任务，将以下代码填入“Pipeline script”中，保存后运行。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">podTemplate(label: &apos;golang-pod&apos;,  containers: [</div><div class="line">    containerTemplate(</div><div class="line">            name: &apos;golang&apos;,</div><div class="line">            image: &apos;registry.cn-hangzhou.aliyuncs.com/spacexnice/golang:1.8.3-docker&apos;,</div><div class="line">            ttyEnabled: true,</div><div class="line">            command: &apos;cat&apos;</div><div class="line">        ),</div><div class="line">    containerTemplate(</div><div class="line">            name: &apos;jnlp&apos;,</div><div class="line">            image: &apos;registry.cn-hangzhou.aliyuncs.com/google-containers/jnlp-slave:alpine&apos;,</div><div class="line">            args: &apos;$&#123;computer.jnlpmac&#125; $&#123;computer.name&#125;&apos;,</div><div class="line">            command: &apos;&apos;</div><div class="line">        )</div><div class="line">  ]</div><div class="line">  ,volumes: [</div><div class="line">        /*persistentVolumeClaim(mountPath: &apos;/home/jenkins&apos;, claimName: &apos;jenkins&apos;, readOnly: false),*/</div><div class="line">        hostPathVolume(hostPath: &apos;/root/work/jenkins&apos;, mountPath: &apos;/home/jenkins&apos;),</div><div class="line">        hostPathVolume(hostPath: &apos;/var/run/docker.sock&apos;, mountPath: &apos;/var/run/docker.sock&apos;),</div><div class="line">        hostPathVolume(hostPath: &apos;/tmp/&apos;, mountPath: &apos;/tmp/&apos;),</div><div class="line">]) </div><div class="line">&#123;</div><div class="line">    node (&apos;golang-pod&apos;) &#123;</div><div class="line">        </div><div class="line">        container(&apos;golang&apos;) &#123;</div><div class="line">            git url: &apos;https://github.com/spacexnice/blog.git&apos; , branch: &apos;code&apos;</div><div class="line">            </div><div class="line">            stage(&apos;Build blog project&apos;) &#123;</div><div class="line">                </div><div class="line">                sh(&quot;make&quot;)</div><div class="line">        </div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>同时在服务器上运行 <code>watch kubectl get pods</code>可以看到 Jenkins Server 通过 Kubernetes 启动了相应的 Pod 来执行任务。</p>
<p><img src="/assets/blogImg/k8s_pipeline_jenkins03.png" alt="k8s_pipeline_jenkins03"></p>
<p><img src="/assets/blogImg/k8s_pipeline_jenkins04.png" alt="k8s_pipeline_jenkins04"></p>
<p><img src="/assets/blogImg/k8s_pipeline_jenkins05.png" alt="k8s_pipeline_jenkins05"></p>
<p>任务完成后，相应的 Pod 会被自动回收。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/18/k8s_pipeline/" class="archive-article-date">
  	<time datetime="2019-03-17T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-18</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Jenkins/">Jenkins</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pipeline/">Pipeline</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-container_network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/15/container_network/">容器网络</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="容器技术系列分享（五）"><a href="#容器技术系列分享（五）" class="headerlink" title="容器技术系列分享（五）"></a>容器技术系列分享（五）</h1><h2 id="容器网络"><a href="#容器网络" class="headerlink" title="容器网络"></a>容器网络</h2><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul>
<li>CNM</li>
<li>CNI</li>
<li>CNM vs CNI</li>
<li>第三方驱动对比</li>
</ul>
<p>目前关于容器网络接口的配置有两种标准：容器网络模型（CNM）和容器网络接口（CNI）。</p>
<h4 id="Container-Network-Model-CNM"><a href="#Container-Network-Model-CNM" class="headerlink" title="Container Network Model(CNM)"></a>Container Network Model(CNM)</h4><p>CNM 是一个被 Docker 提出的规范，现在已经被 Cisco Contiv, Kuryr, Open Virtual Networking(OVN), Porject Calico, VMware 和 Weave 这些公司和项目所采纳。</p>
<p><img src="/assets/blogImg/k8s_network_cnm01.png" alt="k8s_network_cnm01"></p>
<p>Libnetwork 是 CNM 的原生实现，它为 Docker Daemon 的网络驱动程序之间提供了接口。网络控制器负责将驱动和一个网络进行对接。每个驱动程序负责管理它所拥有的网络以及为该网络提供各种服务，例如 IPAM 等。由多个驱动支撑的多个网络可以同时并存。网络驱动可以按提供方被划分为原生驱动（libnetwork 内置的或 Docker 支持的）或者远程驱动（第三方插件）。原生驱动包括 none、bridge、overlay 以及 macvlan。</p>
<p><img src="/assets/blogImg/k8s_network_cnm02.png" alt="k8s_network_cnm02"></p>
<ul>
<li>Network Sandbox：一个容器内部的网络栈</li>
<li>Endpoint：一个通常成对出现的网络接口。一端在容器网络内，另一端在网格内。一个 Endpoint 可以加入一个网络。一个容器可以有多个 Endpoint。</li>
<li>Network：一个 Endpoint 的集合，该集合内的所有 Endpoint 可以互联互通。</li>
</ul>
<p>最后，CNM 还支持标签（labels），label 是以 key-value 定义的元数据，用户可以通过定义 label 这样的元数据来自定义 libnetwork 和驱动的行为。</p>
<h4 id="Container-Network-Interface-CNI"><a href="#Container-Network-Interface-CNI" class="headerlink" title="Container Network Interface(CNI)"></a>Container Network Interface(CNI)</h4><p>CNI 是由 CoreOS 提出的一个容器网络规范，已采纳该规范的包括 Apache Mesos, Cloud Foundry, Kubernetes, Kurma 和 rkt。另外 Contiv Networking，Project Calico 和 Weave 这些项目也为 CNI 提供插件。</p>
<p><img src="/assets/blogImg/k8s_network_cni01.png" alt="k8s_network_cni01"></p>
<p>CNI 的规范比较小巧，它规定了一个容器 runtime 和网络插件之间的简单契约，这个契约通过 JSON 的语法定义了 CNI 插件所需要提供的输入和输出。</p>
<p>一个容器可以被加入到被不同插件所驱动的多个网络之中，一个网络有自己对应的插件和唯一的名称。CNI 插件负责为容器配置网络，包括两个基本接口：</p>
<p><em>配置网络</em></p>
<p>AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error)</p>
<p><em>清理网络</em></p>
<p>DelNetwork(net NetworkConfig, rt RuntimeConf) error</p>
<h4 id="CNM-vs-CNI"><a href="#CNM-vs-CNI" class="headerlink" title="CNM vs CNI"></a>CNM vs CNI</h4><p>CNM 和 CNI 两种方案都使用了驱动模型或者插件模型来为容器创建网络栈，这样的设计使得用户可以自由选择，两者都支持多个网络驱动被同事使用，也允许容器加入一个或多个网络，两者也都允许容器 runtime 在它自己的命名空间中启动网络。</p>
<p>这种模块化驱动的方式可以说对运维人员更有吸引力，因为运维人员可以比较灵活的选择适合现有模式的驱动。两种方案都提供了独立的扩展点，也就是插件的接口，这使得网络驱动可以创建、配置和连接网络，也使得 IPAM 可以配置、发现和管理 IP 地址。这种分离让编排变得容易。</p>
<p>CNM 模式下的网络驱动不能访问容器的网络命名空间。这样做的好处是 libnetwork 可以为冲突解决提供仲裁。比如两个独立的网络驱动提供同样的静态路由配置，但是却指向不同的下一跳IP地址。与此不同，CNI允许驱动访问容器的网络命名空间。CNI正在研究在类似情况下如何提供仲裁。</p>
<p>CNI支持与第三方 IPAM 的集成，可以用于任何容器 runtime。CNM 从设计上就仅仅支持 Docker。由于 CNI 简单的设计，许多人认为编写 CNI 插件会比编写 CNM 插件来得简单。</p>
<p>这些模型增进了系统的模块化，增加了用户的选择。也促进了第三方网络插件以创新来提供更高级的网络功能。</p>
<h4 id="第三方驱动对比"><a href="#第三方驱动对比" class="headerlink" title="第三方驱动对比"></a>第三方驱动对比</h4><p><img src="/assets/blogImg/k8s_network02.png" alt="k8s_network02"></p>
<p>除了上图 5 种网络驱动，其实还有很多，Kubernetes 官方介绍了大概 23 种网络插件。接下来详细对比其中性能较好的几种。</p>
<table>
<thead>
<tr>
<th style="text-align:left">方案</th>
<th style="text-align:center">Calico</th>
<th style="text-align:center">Contiv</th>
<th style="text-align:right">macvlan</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">优点</td>
<td style="text-align:center">纯三层的数据中心解决方案（不依赖overlay），对 OpenStack、Kubernetes、AWS、GCE都比较友好。该方案集成简单。</td>
<td style="text-align:center">能够和非容器环境兼容协作，不依赖物理网络具体细节，支持 Policy、ACI、QoS租户，支持物理网卡 sriov 和 offload。该方案配置比较复杂。</td>
<td style="text-align:right">Docker 原生，性能衰减少，可控性高，隔离性好。</td>
</tr>
<tr>
<td style="text-align:left">缺点</td>
<td style="text-align:center">操作管理比较复杂，需要定制开发。</td>
<td style="text-align:center">集成配置较复杂，相关资料少，学习成本较高，需要定制开发。</td>
<td style="text-align:right">集群规模很大时，存在 ARP 广播风暴和交换机 MAC 表超限风险，另外需要自研 QoS 功能。</td>
</tr>
<tr>
<td style="text-align:left">结论</td>
<td style="text-align:center">Calico 基于 BGP 协议的 Overlay SDN 解决方案，企业生产环境如果可以开启 BGP 协议，可以考虑 Calico BGP 方案。</td>
<td style="text-align:center">功能强型好，对 CNI 和 CNM 模型支持性好，集群配置较复杂，学习研究成本较高，可以考虑此方案。</td>
<td style="text-align:right">初期较少的开发量即可上线（之后需要自研 QoS），通过合理的设计容量规划来规避相关风险时，可以考虑此方案。</td>
</tr>
</tbody>
</table>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/15/container_network/" class="archive-article-date">
  	<time datetime="2019-03-14T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-15</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_cronjob" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/13/k8s_cronjob/">Kubernetes CronJob</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Kubernetes-CronJob"><a href="#Kubernetes-CronJob" class="headerlink" title="Kubernetes CronJob"></a>Kubernetes CronJob</h2><h3 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h3><p>cronjob.yaml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">apiVersion: batch/v1beta1</div><div class="line">kind: CronJob</div><div class="line">metadata:</div><div class="line">  name: hello</div><div class="line">spec:</div><div class="line">  schedule: &quot;*/1 * * * *&quot;</div><div class="line">  jobTemplate:</div><div class="line">    spec:</div><div class="line">      template:</div><div class="line">        spec:</div><div class="line">          containers:</div><div class="line">          - name: hello</div><div class="line">            image: busybox</div><div class="line">            args:</div><div class="line">            - /bin/sh</div><div class="line">            - -c</div><div class="line">            - date; echo Hello from the Kubernetes cluster</div><div class="line">          restartPolicy: OnFailure</div></pre></td></tr></table></figure>
<p>创建 CronJob</p>
<p><code>kubectl create -f cronjob.yaml</code></p>
<p>查看 CronJob</p>
<p><code>kubectl get cronjob hello</code></p>
<p><img src="/assets/blogImg/k8s_cronjob01.png" alt="k8s_cronjob01"></p>
<p>关注 Job 状态</p>
<p><code>kubectl get jobs --watch</code></p>
<p><img src="/assets/blogImg/k8s_cronjob02.png" alt="k8s_cronjob02"></p>
<p>删除 CronJob</p>
<p><code>kubectl delete cronjob hello</code></p>
<p><a href="https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/" target="_blank" rel="external">更多详细操作</a></p>
<h3 id="集成"><a href="#集成" class="headerlink" title="集成"></a>集成</h3><p>针对php项目容器化改造过程中，原有crontab任务较多的问题，在迁移到 k8s 集群时可无缝替换成 CronJob 方式管理。</p>
<p>后续可基于 CronJob 开发客户端工具，实现简单高效的分布式任务系统。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/13/k8s_cronjob/" class="archive-article-date">
  	<time datetime="2019-03-12T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-13</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_logging" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/13/k8s_logging/">Kubernetes Logging Architecture</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Kubernetes-Cluster-level-logging-architectures"><a href="#Kubernetes-Cluster-level-logging-architectures" class="headerlink" title="Kubernetes Cluster-level logging architectures"></a>Kubernetes Cluster-level logging architectures</h2><p>Kubernetes 官方不提供原生的集群日志方案，但是提供了一些解决思路：</p>
<ul>
<li>在每个 Node 上运行一个日志客户端。</li>
<li>在应用的 Pod 里以 sidecar 形式运行一个日志容器。</li>
<li>应用直接把日志推送到后端日志存储里。</li>
</ul>
<h3 id="Using-a-node-logging-agent"><a href="#Using-a-node-logging-agent" class="headerlink" title="Using a node logging agent"></a>Using a node logging agent</h3><p><img src="/assets/blogImg/k8s_logging_nodeAgent.png" alt="k8s_logging_nodeAgent"></p>
<p>一台 Node 上所有容器的 <em>stdout/stderr</em> 日志会被统一保存到宿主机的目录，只需要以 DaemonSet 的形式在每个 Node 上运行一个日志收集客户端即可将这台 Node 上所有容器的 <em>标准输出/标准错误</em> 日志收集起来存到后端存储里。</p>
<p>这里的日志收集客户端建议 <a href="https://www.fluentd.org/" target="_blank" rel="external">fluentd</a>。</p>
<h3 id="Using-a-sidecar-container-with-the-logging-agent"><a href="#Using-a-sidecar-container-with-the-logging-agent" class="headerlink" title="Using a sidecar container with the logging agent"></a>Using a sidecar container with the logging agent</h3><p>使用 Sidecar 方式有两种方案：</p>
<ul>
<li>Sidecar 容器将应用日志导流到标准输出或标准错误。</li>
</ul>
<p><img src="/assets/blogImg/k8s_logging_sidecar_stream.png" alt="k8s_logging_sidecar_stream"></p>
<p>这种方式有利于适配 kubelet，或每台 Node 已经运行日志收集客户端的情况。Sidecar 容器可以从文件、socket 或 journald 收集日志，为应用里的每个日志模块运行一个特定的 sidecar 容器，整个 Pod 共享一个共享目录，最终把每个日志模块的日志对应的输出到各自的 sidecar 容器的标准输出或标准错误。</p>
<p>这是一个在容器内部写两个日志文件的应用范例,然后是同一个 Pod 里运行两个 sidecar 容器分别对两个日志文件进行收集：</p>
<p>two-files-counter-pod-streaming-sidecar.yaml </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: counter</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">  - name: count</div><div class="line">    image: busybox</div><div class="line">    args:</div><div class="line">    - /bin/sh</div><div class="line">    - -c</div><div class="line">    - &gt;</div><div class="line">      i=0;</div><div class="line">      while true;</div><div class="line">      do</div><div class="line">        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;</div><div class="line">        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;</div><div class="line">        i=$((i+1));</div><div class="line">        sleep 1;</div><div class="line">      done</div><div class="line">    volumeMounts:</div><div class="line">    - name: varlog</div><div class="line">      mountPath: /var/log</div><div class="line">  - name: count-log-1</div><div class="line">    image: busybox</div><div class="line">    args: [/bin/sh, -c, &apos;tail -n+1 -f /var/log/1.log&apos;]</div><div class="line">    volumeMounts:</div><div class="line">    - name: varlog</div><div class="line">      mountPath: /var/log</div><div class="line">  - name: count-log-2</div><div class="line">    image: busybox</div><div class="line">    args: [/bin/sh, -c, &apos;tail -n+1 -f /var/log/2.log&apos;]</div><div class="line">    volumeMounts:</div><div class="line">    - name: varlog</div><div class="line">      mountPath: /var/log</div><div class="line">  volumes:</div><div class="line">  - name: varlog</div><div class="line">    emptyDir: &#123;&#125;</div></pre></td></tr></table></figure>
<p>然后，你就可以使用 <code>kubectl logs counter count-log-1</code>查看 1.log 的内容。同理2。</p>
<p>Node 上的日志客户端会自动采集这两个 sidecar容器的标准输出/标准错误，实际也就是采集了 1.log 和 2.log 两个日志文件。</p>
<hr>
<ul>
<li>Sidecar 容器运行一个日志收集客户端，主动去收集应用日志。</li>
</ul>
<p><img src="/assets/blogImg/k8s_logging_sidecar_agent.png" alt="k8s_logging_sidecar_agent"></p>
<p>这种方式适用于 Node 上没有日志收集客户端的情况。需要注意的是这种方式存在过多资源消耗的问题，而且无法使用 kubectl logs 命令直接查看应用日志。</p>
<p>这里举例使用 fluentd 作为 sidecar logging agent 收集日志：</p>
<p>fluentd-sidecar-config.yaml </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: fluentd-config</div><div class="line">data:</div><div class="line">  fluentd.conf: |</div><div class="line">    &lt;source&gt;</div><div class="line">      type tail</div><div class="line">      format none</div><div class="line">      path /var/log/1.log</div><div class="line">      pos_file /var/log/1.log.pos</div><div class="line">      tag count.format1</div><div class="line">    &lt;/source&gt;</div><div class="line"></div><div class="line">    &lt;source&gt;</div><div class="line">      type tail</div><div class="line">      format none</div><div class="line">      path /var/log/2.log</div><div class="line">      pos_file /var/log/2.log.pos</div><div class="line">      tag count.format2</div><div class="line">    &lt;/source&gt;</div><div class="line"></div><div class="line">    &lt;match **&gt;</div><div class="line">      type google_cloud</div><div class="line">    &lt;/match&gt;</div></pre></td></tr></table></figure>
<p>two-files-counter-pod-agent-sidecar.yaml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: counter</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">  - name: count</div><div class="line">    image: busybox</div><div class="line">    args:</div><div class="line">    - /bin/sh</div><div class="line">    - -c</div><div class="line">    - &gt;</div><div class="line">      i=0;</div><div class="line">      while true;</div><div class="line">      do</div><div class="line">        echo &quot;$i: $(date)&quot; &gt;&gt; /var/log/1.log;</div><div class="line">        echo &quot;$(date) INFO $i&quot; &gt;&gt; /var/log/2.log;</div><div class="line">        i=$((i+1));</div><div class="line">        sleep 1;</div><div class="line">      done</div><div class="line">    volumeMounts:</div><div class="line">    - name: varlog</div><div class="line">      mountPath: /var/log</div><div class="line">  - name: count-agent</div><div class="line">    image: k8s.gcr.io/fluentd-gcp:1.30</div><div class="line">    env:</div><div class="line">    - name: FLUENTD_ARGS</div><div class="line">      value: -c /etc/fluentd-config/fluentd.conf</div><div class="line">    volumeMounts:</div><div class="line">    - name: varlog</div><div class="line">      mountPath: /var/log</div><div class="line">    - name: config-volume</div><div class="line">      mountPath: /etc/fluentd-config</div><div class="line">  volumes:</div><div class="line">  - name: varlog</div><div class="line">    emptyDir: &#123;&#125;</div><div class="line">  - name: config-volume</div><div class="line">    configMap:</div><div class="line">      name: fluentd-config</div></pre></td></tr></table></figure>
<h3 id="Exposing-logs-directly-from-the-application"><a href="#Exposing-logs-directly-from-the-application" class="headerlink" title="Exposing logs directly from the application"></a>Exposing logs directly from the application</h3><p><img src="/assets/blogImg/k8s_logging_directly.png" alt="k8s_logging_directly"></p>
<p>这是最简单的方式，不过涉及应用的改造。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/13/k8s_logging/" class="archive-article-date">
  	<time datetime="2019-03-12T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-13</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s01" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/13/k8s01/">Kubernetes 基础</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="容器技术系列分享（四）"><a href="#容器技术系列分享（四）" class="headerlink" title="容器技术系列分享（四）"></a>容器技术系列分享（四）</h1><h2 id="Kubernetes-基础"><a href="#Kubernetes-基础" class="headerlink" title="Kubernetes 基础"></a>Kubernetes 基础</h2><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul>
<li>Kubernetes 是什么</li>
<li>为什么要用 Kubernetes</li>
<li>一个简单的例子</li>
<li>Kubernetes 基本概念和术语<ul>
<li>Master</li>
<li>Node</li>
<li>Pod</li>
<li>Label</li>
<li>Replication Controller</li>
<li>Deployment</li>
<li>Horizontal Pod Autoscaler</li>
<li>StatefulSet</li>
<li>Service</li>
<li>Volume</li>
<li>Persisten Volume</li>
<li>Namespace</li>
<li>Annotation</li>
</ul>
</li>
</ul>
<h4 id="Kubernetes-是什么"><a href="#Kubernetes-是什么" class="headerlink" title="Kubernetes 是什么"></a>Kubernetes 是什么</h4><p>Kubernetes 是一个基于容器技术的分布式架构方案，它是谷歌内部系统 Borg 的一个开源版本。</p>
<p>Kubernetes是一个开放的开发平台，它不局限于任何一种语言，任何服务都可以映射为 Kubernetes 的 Service，并通过标准的 TCP 通信协议进行交互。</p>
<p>Kubernetes 具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多粒度的资源配额管理能力。同时，Kubernetes 提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的多个环节。</p>
<h4 id="为什么要用-Kubernetes"><a href="#为什么要用-Kubernetes" class="headerlink" title="为什么要用 Kubernetes"></a>为什么要用 Kubernetes</h4><p>Docker 容器化技术当前已经被很多公司所采用，其从单机走向集群已成为必然，而云计算的蓬勃发展正在加速这一过程。Kubernetes 作为当前唯一被业界广泛认可和看好的 Docker 分布式系统解决方案，可以遇见，在未来会有大量的新系统选择它。</p>
<p>使用 Kubernetes 可以收获哪些好处？</p>
<ul>
<li>首先，最直接的感受就是可以“轻装上阵”地开发复杂系统了。以前动不动就需要十几个人而且团队里需要不少技术达人一起分工协助才能设计实现的和运维的分布式系统，在采用 Kubernetes 解决方案后，只需要一个劲精悍的小团队就能轻松应付。</li>
<li>其次，使用 Kubernetes 就是在全面拥抱微服务架构。微服务架构的核心是将一个巨大的单体应用分解为许多小的互相连接的微服务，一个微服务背后可能有多个实例副本在支撑，副本的数量可能会随着系统的负荷变化而进行调整，内嵌的负载均衡器在这里发挥了重要作用。微服务架构使得每个服务都可以由专门的开发团队来开发，开发者可以自由选择开发技术，这对于大规模团队来说很有价值，另外每个微服务独立开发、升级、扩展，因此系统具备很高的稳定性和快速迭代能力。谷歌将微服务架构的基础设施直接打包到 Kubernetes 解决方案中，让我们有机会直接应用微服务架构解决复杂业务系统的架构问题。</li>
<li>然后，我们的系统可以随时随地整体“搬迁”到别的机房或公有云上。Kubernetes 最初的目标就是运行在谷歌自家的公有云 GCE 中，未来会支持更多的公有云及基于 OpenStack 的私有云。同时，在 Kubernetes 的架构方案中，底层网络的细节完全被屏蔽，基于服务的 ClusterIP 甚至都无需我们改变运行期的配置文件，就能将系统从物理环境中无缝迁移到公有云中，或者在服务高峰期将部分服务对应的 Pod 副本放入公有云中以提升系统的吞吐量。</li>
<li>最后，Kubernetes 系统架构具备了超强的横向扩容能力。对于互联网公司来说，用户规模就等价于资产，谁拥有更多的用户，谁就能在竞争中胜出，因此超强的横向扩容能力是互联网业务系统的关键指标之一。不用修改代码，一个 Kubernetes 集群即可从只包含几个 Node 的小集群平滑扩展到拥有上百个 Node 的大规模集群，我们利用 Kubernetes 提供的工具，甚至可以在线完成集群扩容。只要我们的微服务设计的好，结合硬件或者公有云资源的线性增加，系统就能够承受大量用户并发访问所带来的巨大压力。</li>
</ul>
<h4 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h4><p>这是一个简单的 Java Web 应用，运行在 Tomcat 里。JSP 页面通过 JDBC 直接访问 MySQL 数据库并展示数据。</p>
<p>此应用需要启动两个容器： Web App 容器和 MySQL 容器，并且 Web App 容器需要访问 MySQL 容器。</p>
<h5 id="启动-MySQL-应用"><a href="#启动-MySQL-应用" class="headerlink" title="启动 MySQL 应用"></a>启动 MySQL 应用</h5><p>首先为 MySQL 服务创建一个 RC 定义文件: mysql-rc.yaml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    app: mysql</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: mysql</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: mysql</div><div class="line">        image: mysql</div><div class="line">        ports:</div><div class="line">        - containerPort: 3306</div><div class="line">        env:</div><div class="line">        - name: MYSQL_ROOT_PASSWORD</div><div class="line">          value: &quot;123456&quot;</div></pre></td></tr></table></figure>
<p>创建好 mysql-rc.yaml 文件后，将它发布到 Kubernetes 集群中：</p>
<p><code>kubectl create -f mysql-rc.yaml</code></p>
<p>接下来，我们用 kubectl 命令查看刚刚创建的 RC：</p>
<p><code>kubectl get rc</code></p>
<p>查看 Pod 的创建情况：</p>
<p><img src="/assets/blogImg/k8s01_javaweb01.png" alt="k8s01_javaweb01"></p>
<p>我们通过 <em>docker ps</em> 命令查看正在运行的容器，发现提供 MySQL 服务的 Pod 容器以及创建并正常运行了。</p>
<h5 id="创建-MySQL-服务"><a href="#创建-MySQL-服务" class="headerlink" title="创建 MySQL 服务"></a>创建 MySQL 服务</h5><p>为 MySQL 服务创建一个 Service 定义文件 mysql-svc.yaml: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: mysql</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">    - port: 3306</div><div class="line">  selector:</div><div class="line">    app: mysql</div></pre></td></tr></table></figure>
<p>运行 kubectl 命令，创建 service：</p>
<p><code>kubectl create -f mysql-svc.yaml</code></p>
<p>运行 kubectl 命令，查看刚刚创建的 service：</p>
<p><code>kubectl get svc</code></p>
<p><img src="/assets/blogImg/k8s01_javaweb02.png" alt="k8s01_javaweb02"></p>
<p>注意到这里分配了一个 CLUSTER-IP，这是一个虚拟地址，Kubernetes 集群中其他新创建的 Pod 就可以通过 Service 的 Cluster IP + Port 来连接它了。</p>
<h5 id="启动-Tomcat-应用"><a href="#启动-Tomcat-应用" class="headerlink" title="启动 Tomcat 应用"></a>启动 Tomcat 应用</h5><p>我们用和启动 MySQL 同样的步骤启动 Tomcat。首先，创建对应的 RC 文件 myweb-rc.yaml:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: myweb</div><div class="line">spec:</div><div class="line">  replicas: 2</div><div class="line">  selector:</div><div class="line">    app: myweb</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: myweb</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: myweb</div><div class="line">          image: java-web:v1</div><div class="line">          ports:</div><div class="line">          - containerPort: 8080</div></pre></td></tr></table></figure>
<p><code>kubectl create -f myweb-rc.yaml</code></p>
<p><code>kubectl get rc</code></p>
<p><code>kubectl get pods</code></p>
<p><img src="/assets/blogImg/k8s01_javaweb03.png" alt="k8s01_javaweb03"></p>
<h5 id="创建-Tomcat-服务"><a href="#创建-Tomcat-服务" class="headerlink" title="创建 Tomcat 服务"></a>创建 Tomcat 服务</h5><p>为 Tomcat 服务创建一个 Service 定义文件 myweb-svc.yaml: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: myweb</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">  ports:</div><div class="line">    - port: 8080</div><div class="line">      nodePort: 30003</div><div class="line">  selector:</div><div class="line">    app: myweb</div></pre></td></tr></table></figure>
<p><code>kubectl create -f myweb-svc.yaml</code></p>
<p><code>kubectl get service</code></p>
<p><img src="/assets/blogImg/k8s01_javaweb04.png" alt="k8s01_javaweb04"></p>
<p>$$$ 结果</p>
<h4 id="Kubernetes-基本概念和术语"><a href="#Kubernetes-基本概念和术语" class="headerlink" title="Kubernetes 基本概念和术语"></a>Kubernetes 基本概念和术语</h4><p>Kubernetes 中的大部分概念如 Node、Pod、Replication Controller、Service 等都可以看作一种 “资源对象”，几乎所有的资源对象都可以通过 Kubernetes 提供的 kubectl 工具（或者 API 编程调用）执行增、删、改、查等操作并将其保存在 Etcd 中持久化存储。从这个角度看，Kubernetes 其实是一个高度自动化的资源控制系统，它通过跟踪对比 Etcd 库里保存的“资源期望状态”与当前环境中的“实际资源状态”的差异来实现自动控制和自动纠错的高级功能。</p>
<p>在介绍资源对象之前，我们先了解一下 Kubernetes 集群的两种管理角色：Master 和 Node。</p>
<h5 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h5><p>Kubernetes 里的 Master 指的是集群控制节点，每个 Kubernetes 集群里需要有一个 Master 节点来负责整个集群的管理和控制，基本上 Kubernetes 的所有控制命令都发给它，它来负责具体的执行过程。Master 节点通常会占据一个独立的服务器（高可用部署建议用3台服务器），其主要原因是它太重要了，是整个集群的“首脑”，如果宕机或不可用，那么对集群内容器应用的管理都将失效。</p>
<p>Master 节点上运行着以下一组关键进程：</p>
<ul>
<li>Kubernetes API Server (kube-apiserver)：提供了 HTTP Rest 接口的关键服务进程，是 Kubernetes 里所有资源的增、删、改、查等操作的唯一入口，也是集群控制的入口进程。</li>
<li>Kubernetes Controller Manager (kube-controller-manager)：Kubernetes 里所有资源对象的自动化控制中心。</li>
<li>Kubernetes Scheduler (kube-scheduler)：负责资源调度（Pod 调度）的进程。</li>
</ul>
<p>另外，Kubernetes 集群还需要一组 Etcd 服务，因为 Kubernetes 里所有资源对象的数据都保存在 Etcd 中。</p>
<h5 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h5><p>除了 Master， Kubernetes 集群中的其他机器被称为 Node 节点。与 Master 一样，Node 节点可以是一台物理主机，也可以是一台虚拟机。Node 节点才是 Kubernetes 集群中的工作负载节点，每个 Node 都会被 Master 分配一些工作负载（Docker 容器），当某个 Node 宕机时，其上的工作负载会被 Master 自动转移到其他节点上去。</p>
<p>每个 Node 节点上运行以下一组关键进程：</p>
<ul>
<li>kubelet：负责 Pod 对应的容器的创建、启停等任务，同时与 Master 节点密切协作，实现集群管理的基本功能。</li>
<li>kube-proxy：实现 Kubernetes Service 的通信与负载均衡机制的重要组件。</li>
<li>Docker Engine（docker）：Docker 引擎，负责本机的容器创建和管理工作。</li>
</ul>
<p>Node 节点可以在运行期间动态增加到 Kubernetes 集群中，前提是这个节点上已经正确安装、配置和启动了上述关键进程，在默认情况下 kubelet 会向 Master 注册自己，这也是 Kubernetes 推荐的 Node 管理方式。一旦 Node 被纳入集群管理范围，kubelet 进程就会定时向 Master 节点汇报自身的情报，例如操作系统、Docker 版本、机器的 CPU 和内存情况，以及当前有哪些 Pod 在运行等，这样 Master 可以获知每个 Node 的资源使用情况，并实现高效均衡的资源调度策略。而某个 Node 炒过指定时间不上报信息时，会被 Master 判定为 “失联”，Node 的状态被标记为不可用（Not Ready），随后 Master 会触发工作负载转移的自动流程。</p>
<p><code>kubectl get nodes</code></p>
<p>$$$ 结果</p>
<p><code>kubectl describe node k8s-node-1</code></p>
<p>$$$ 结果</p>
<p>上述命令展示了 Node 的如下关键信息：</p>
<ul>
<li>Node 基本信息：名称、标签、创建时间等。</li>
<li>Node 当前的运行状态，Node 启动后会做一系列的自检工作，比如磁盘是否满了，如果满了就标注  OutOfDisk=True，否则继续检查内存是否不足（如果内存不足，就标注 MemoryPressure=True），最后一切正常，就设置为 Ready 状态（Ready=True），该状态表示 Node 处于健康状态， Master 将可以在其上调度新的任务了（如启动 Pod）。</li>
<li>Node 的主机地址与主机名</li>
<li>Node 上的资源总量：描述 Node 可用的系统资源，包括 CPU、内存、最大可调度 Pod 数量等。</li>
<li>Node 可分配资源量：描述 Node 当前可用于分配的资源量。</li>
<li>主机系统信息：包括主机的唯一标识 UUID、Linux Kernel 版本号、操作系统类型与版本、Kubernetes 版本号、kubelet 与 kube-proxy 版本号等。</li>
<li>当前正在运行的 Pod 列表概要信息。</li>
<li>已分配的资源使用概要信息，例如资源申请的最低、最大允许使用量占系统总量的百分比。</li>
<li>Node 相关的 Event 信息。</li>
</ul>
<h5 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h5><p>Pod 是 Kubernetes 的最重要也最基本的概念。每个 Pod 都有一个特殊的被称为“根容器” 的 Pause 容器。Pause 容器对应的镜像属于 Kubernetes 平台的一部分，除了 Pause 容器，每个 Pod 还包含一个或多个紧密相关的用户业务容器。</p>
<p><img src="/assets/blogImg/k8s01_pod01.png" alt="k8s01_pod01"></p>
<blockquote>
<p>为什么 Kubernetes 会设计出一个全新的 Pod 概念并且 Pod 有这样特殊的组成结构？</p>
</blockquote>
<ul>
<li>原因之一：在一组容器作为一个单元的情况下，我们难以对“整体”简单地进行判断及有效地进行行动。比如，一个容器死亡了，此时算是整体死亡么？是 N/M 的死亡率么？引入业务无关并且不易死亡的 Pause 容器作为 Pod 的根容器，以它的状态代表整个容器组的状态，就简单、巧妙地解决了这个难题。</li>
<li>原因之二：Pod 里的多个业务容器共享 Pause 容器的 IP，共享 Pause 容器挂载的 Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。</li>
</ul>
<p>Kubernetes 为每个 Pod 都分配了唯一的 IP 地址，称之为 PodIP，一个 Pod 里的多个容器共享 PodIP 地址。Pod 的 IP 加上 Pod 里的容器端口（containerPort），就组成了一个概念————Endpoint，它代表此 Pod 里的一个服务进程的对外通信地址。一个 Pod 也存在着具有多个 Endpoint 的情况。</p>
<p><img src="/assets/blogImg/k8s01_pod02.png" alt="k8s01_pod02"></p>
<h5 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h5><p>Label 是 Kubernetes 系统中另外一个核心概念。一个 Label 是一个 key=value 的键值对，其中 Key 与 value 由用户自己指定。Label 可以附加到各种资源上，例如 Node、Pod、Service、RC 等，一个资源对象可以定义任意数量的 Label，同一个 Label 也可以被添加到任意数量的资源对象上去，Label 通常在资源对象定义时确定，也可以在创建对象后动态添加或者删除。</p>
<p>我们可以通过给指定的资源对象捆绑一个或多个不同的 Label 来实现多维度的资源分组管理功能，以便于灵活、方便地进行资源分配、调度、配置、部署等管理工作。例如：部署不同版本的应用到不同的环境中；或者监控和分析应用（日志记录、监控、告警）等。一些常用的 Label 示例如下：</p>
<ul>
<li>版本标签：”release”:”stable”,”release”:”canary”…</li>
<li>环境标签：”environment”:”dev”,”environment”:”production”…</li>
<li>架构标签：”tier”:”frontend”,”tier”:”backend”…</li>
<li>分区标签：”partition”:”customerA”,”partition”:”customerB”…</li>
<li>质量管控标签：”track”:”daily”,”track”:”weekly”…</li>
</ul>
<p>给某个资源对象定义一个 Label，就相当于给它打了一个标签，随后可以通过 Label Selector（标签选择器）查询和筛选拥有某些 Label 的资源对象，Kubernetes 通过这种方式实现了类似 SQL 的简单又通用的查询机制。</p>
<p>当前有两种 Label Selector 的表达式：基于等式的（Equality-based）和基于集合的（Set-based），前者采用“等式类”的表达式匹配标签，下面是一些具体的例子：</p>
<ul>
<li>name = redis-slave：匹配所有具有标签 name=redis-slave 的资源对象。</li>
<li>env != production：匹配所有不具有标签 env=production 的资源对象，比如 env=test 就满足。</li>
</ul>
<p>使用集合操作的表达式匹配标签例子：</p>
<ul>
<li>name in (redis-master, redis-slave)：匹配所有具有标签 name=redis-master 或者 name=redis-slave 的资源对象。</li>
<li>name not in (php-frontend)：匹配所有不具有标签 name=php-frontend 的资源对象。</li>
</ul>
<p>可以通过多个 Label Selector 表达式的组合实现复杂的条件选择，多个表达式之间用“，”进行分割即可，几个条件之间是 “AND” 的关系，即同时满足多个条件。</p>
<p>新出现的管理对象如 Deployment、ReplicaSet、DaemonSet 和 Job 则可以在 Selector 中使用基于集合的筛选条件定义，例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">selector:</div><div class="line">  matchLabels:</div><div class="line">    app: myweb</div><div class="line">  matchExpressions:</div><div class="line">    - &#123;key: tier, operator: In, values: [frontend]&#125;</div><div class="line">    - &#123;key: environment, operator: NotIn, values: [dev]&#125;</div></pre></td></tr></table></figure>
<p>matchLabels 用于定义一组 Label，与直接写在 Selector 中作用相同；matchExpressions 用于定义一组基于集合的筛选条件，可用的条件运算符包括：In、NotIn、Exists 和 DoesNotExist。</p>
<p>如果同时设置了 matchLabels 和 matchExpressions，则两组条件为 “AND” 关系，即所有条件需要同时满足才能完成 Selector 的筛选。</p>
<p>Label Selector 在 Kubernetes 中的重要使用场景有以下几处：</p>
<ul>
<li>kube-controller 进程通过资源对象 RC 上定义的 Label Selector 来筛选要监控的 Pod 副本的数量，从而实现 Pod 副本的数量始终符合预期设定的全自动控制流程。</li>
<li>kube-proxy 进程通过 Service 的 Label Selector 来选择对应的 Pod，自动建立起每个 Service 到对应 Pod 的请求转发路由表，从而实现 Service 的智能负载均衡机制。</li>
<li>通过对某些 Node 定义特定的 Label，并且在 Pod 定义文件中使用 NodeSelector 这种标签调度策略，kube-scheduler 进程可以实现 Pod “定向调度” 的特性。</li>
</ul>
<h5 id="Replication-Controller"><a href="#Replication-Controller" class="headerlink" title="Replication Controller"></a>Replication Controller</h5><p>RC 是 Kubernetes 系统中的核心概念之一，简单来说，它其实是定义了一个期望的场景，即声明某种 Pod 的副本数量在任意时刻都符合某个预期值，所以 RC 的定义包括如下部分：</p>
<ul>
<li>Pod 期待的副本数（replicas）</li>
<li>用于筛选目标 Pod 的 Label Selector</li>
<li>当 Pod 的副本数量小于预期数量时，用于创建新 Pod 的 Pod 模版（template）</li>
</ul>
<p>下面是一个完整的 RC 定义的例子，即确保拥有 tier=frontend 标签的这个 Pod（运行 Tomcat 容器）在整个 Kubernetes 集群中始终只有一个副本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: frontend</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    tier: frontend</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: app-demo</div><div class="line">        tier: frontend</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: tomcact-demo</div><div class="line">        image: tomcat</div><div class="line">        imagePullPolicy: IfNotPresent</div><div class="line">        env:</div><div class="line">        - name: GET_HOSTS_FROM</div><div class="line">          value: dns</div><div class="line">        ports:</div><div class="line">        - containerPort: 80</div></pre></td></tr></table></figure>
<p>当我们定义了一个 RC 并提交到 Kubernetes 集群中以后，Master 节点上的 Controller Manager 组件就得到通知，定期巡检系统中当前存活的目标 Pod，并确保目标 Pod 实例的数量刚好等于此 RC 的期望值，如果有过多的 Pod 副本在运行，系统就会停掉一些 Pod，否则系统就会再自动创建一些 Pod。通过 RC，Kubernetes 实现了用户应用集群的高可用性，并且大大减少了系统管理员在传统 IT 环境中需要完成的许多手工运维工作（如主机监控脚本、应用监控脚本、故障恢复脚本等）。</p>
<p>在 RC 运行时，我们可以通过修改 RC 的副本数量，来实现 Pod 的动态缩放（Scaling）功能：</p>
<p><code>kubect scale rc frontend --replicas=3</code></p>
<p>在 Kubernetes v1.2时，Replication Controller升级成了 Replica Set，新功能支持集合的 Label selector（Set-based selector）。不过一般很少单独使用Replica Set，它主要被 Deployment 这个更高层的资源对象所使用，从而形成一整套 Pod 创建、删除、更新的编排机制。下面是等价于前面 RC 例子的 Replica Set 的定义（省去了 Pod 模版部分的内容）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: ReplicaSet</div><div class="line">metadata:</div><div class="line">  name: frontend</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      tier: frontend</div><div class="line">    matchExpressions:</div><div class="line">      - &#123;key: tier, operator: In, values: [frontend]&#125;</div><div class="line">  template:</div><div class="line">  ......</div></pre></td></tr></table></figure>
<p>最后我们总结一下关于 RC（Replica Set）的一些特性与作用：</p>
<ul>
<li>在大多数情况下，我们通过定义一个 RC 实现 Pod 的创建过程及副本数量的自动控制。</li>
<li>RC 里包括完整的 Pod 定义模版。</li>
<li>RC 通过 Label Selector 机制实现对 Pod 副本的自动控制。</li>
<li>通过改变 RC 里的 Pod 副本数量，可以实现 Pod 的扩容或缩容功能。</li>
<li>通过改变 RC 里 Pod 模版中的镜像版本，可以实现 Pod 的滚动升级功能。</li>
</ul>
<h5 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h5><p>Deployment 是 Kubernetes v1.2 引入的新概念，引入的目的是为了更好地解决 Pod 的编排问题。为此，Deployment 在内部使用了 Replica Set 来实现目的，无论从 Deployment 的作用与目的、它的 YAML 定义，还是从它的具体命令行操作来看，我们都可以把它看作 RC 的一次升级，两者的相似度超过 90%。</p>
<p>Deployment 相对于 RC 的一个最大升级是可以随时知道当前 Pod “部署” 的进度。实际上由于一个 Pod 的创建、调度、绑定节点及在目标 Node 上启动对应的容器这一完整过程需要一定的时间，所以我们期待系统启动 N 个 Pod 副本的目标状态，实际上是一个连续变化的“部署过程”导致的最终状态。</p>
<p>Deployment 的典型使用场景有以下几个：</p>
<ul>
<li>创建一个 Deployment 对象来生成对应的 Replica Set 并完成 Pod 副本的创建过程。</li>
<li>检查 Deployment 的状态来看部署动作是否完成（Pod 副本的数量是否达到预期的值）。</li>
<li>更新 Deployment 以创建新的 Pod （比如镜像升级）。</li>
<li>如果当前 Deployment 不稳定，则回滚到一个早先的 Deployment 版本。</li>
<li>暂停 Deployment 以便于一次性修改多个 PodTemplateSpec 的配置项，之后再恢复 Deployment，进行新的发布。</li>
<li>扩展 Deployment 以应对高负载。</li>
<li>查看 Deployment 的状态，以此作为发布是否成功的指标。</li>
<li>清理不再需要的旧版本 ReplicaSets。</li>
</ul>
<p>举个例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">metadata:</div><div class="line">  name: frontend</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    matchLabels:</div><div class="line">      tier: frontend</div><div class="line">    matchExpressions:</div><div class="line">      - &#123;key: tier, operator: In, values: [frontend]&#125;</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        app: app-demo</div><div class="line">        tier: frontend</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: tomcat-demo</div><div class="line">        image: tomcat</div><div class="line">        imagePullPolicy: IfNotPresent</div><div class="line">        ports:</div><div class="line">        - containerPort: 8080</div></pre></td></tr></table></figure>
<p>创建 Deployment：</p>
<p><code>kubectl create -f tomcat-deployment.yaml</code></p>
<p>$$$ 结果</p>
<p>查看 Deployment 信息：</p>
<p><code>kubectl get deployments</code></p>
<p>$$$ 结果</p>
<p>对上述输出中涉及的数量解释如下：</p>
<ul>
<li>DESIRED: Pod 副本数量的期望值，即 Deployment 里定义的 Replica。</li>
<li>CURRENT: 当前 Replica 的值，实际上是 Deployment 所创建的 Replica Set 里的 Replica 值，这个值不断增加，直到达到 DESIRED 为止，表明整个部署过程完成。</li>
<li>UP-TO-DATE: 最新版本的 Pod 的副本数量，用于指示在滚动升级的过程中，有多少个副本已经成功升级。</li>
<li>AVAILABLE: 当前集群中可用的 Pod 副本数量，即集群中当前存活的 Pod 数量。</li>
</ul>
<p>查看 Replica Set，可以看到它的命名与 Deployment 的名字有关系：</p>
<p><code>kubectl get rs</code></p>
<p>$$$ 结果</p>
<p>查看创建的 Pod，我嗯发现 Pod 的命名以 Deployment 对应的 Replica Set 的名字为前缀，这种命名很清晰地表明了一个 Replica Set 创建了哪些 Pod，对于 Pod 滚动升级这种复杂的过程来说，很容易排查错误：</p>
<p><code>kubectl get pods</code></p>
<p>$$$ 结果</p>
<p>Pod 的管理对象，除了 RC 和 Deployment，还包括 ReplicaSet、DaemonSet、StatefulSet、Job 等，分别用于不同的应用场景中。</p>
<h5 id="Horizontal-Pod-Autoscaler"><a href="#Horizontal-Pod-Autoscaler" class="headerlink" title="Horizontal Pod Autoscaler"></a>Horizontal Pod Autoscaler</h5><p>前面我们提到过，通过手动执行 kubectl scale 命令，可以实现 Pod 扩容或缩容。而 Kubernetes v1.1 版本中发布了一个新特性————Horizontal Pod Autoscaling（Pod 横向自动扩容，简称 HPA）。</p>
<p>HPA 与之前的 RC、Deployment 一样，也属于一种 Kubernetes 资源对象。通过追踪分析 RC 控制的所有目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 的副本数。当前，HPA 可以有以下两种方式作为 Pod 负载的度量指标：</p>
<ul>
<li>CPUUtilizationPercentage</li>
<li>应用程序自定义的度量指标，比如服务在每秒内的相应的请求数（TPS 或 QPS）。</li>
</ul>
<p>CPUUtilizationPercentage 是一个算数平均值，即目标 Pod 所有副本自身的 CPU 利用率的平均值。一个 Pod 自身的 CPU 利用率是该 Pod 当前 CPU 的使用量除以它的 Pod Request 的值，比如我们定义一个 Pod 的 Pod Request 为 0.4，而当前 Pod 的 CPU 使用量为 0.2，则它的 CPU 使用率为50%，如此一来，我们就可以算出一个 RC 控制的所有 Pod 副本的 CPU 利用率的算术平均值了。如果某一时刻 CPUUtilizationPercentage 的值超过 80%，则意味着当前的 Pod 副本数很可能不足以支撑接下来更多的请求，需要进行动态扩容，而当请求高峰时段过去后， Pod 的 CPU 利用率又会降下来，此时对应的 Pod 副本数应该自动减少到一个合理的水平。</p>
<p>下面是 HPA 定义的一个具体例子：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: autoscaling/v1</div><div class="line">kind: HorizontalPodAutoscaler</div><div class="line">metadata:</div><div class="line">  name: php-apache</div><div class="line">  namespace: default</div><div class="line">spec:</div><div class="line">  maxReplicas: 10</div><div class="line">  minReplicas: 1</div><div class="line">  scaleTargerRes:</div><div class="line">    kind: Deployment</div><div class="line">    name: php-apache</div><div class="line">  targerCPUUtilizationPercentage: 90</div></pre></td></tr></table></figure>
<h5 id="StatefulSet"><a href="#StatefulSet" class="headerlink" title="StatefulSet"></a>StatefulSet</h5><p>在 Kubernetes 系统中，Pod 的管理对象 RC、Deployment、DaemonSet 和 Job 都是面向无状态的服务。但现实中有很多服务是有状态的，特别是一些复杂的中间件集群，例如 MySQL 集群、MongoDB 集群、ZooKeeper 集群等，这些应用集群有以下一些共同点：</p>
<ul>
<li>每个节点都有固定的身份 ID，通过这个 ID，集群中的成员可以互相发现并且通信。</li>
<li>集群的规模是比较固定的，集群规模不能随意变动。</li>
<li>集群里的每个节点都是有状态的，通常会持久化数据到永久存储中。</li>
<li>如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损。</li>
</ul>
<p>如果用 RC/Deployment 控制 Pod 副本数的方式来实现上述有状态的集群，我们会发现第 1 点是无法满足的，因为 Pod 的名字是随机产生的，Pod 的 IP 地址也是在运行期才确定且可能有变动的，我们实现无法为每个 Pod 确定唯一不变的 ID。另外，为了能够在其他节点上恢复某个失败的节点，这种集群中的 Pod 需要挂接某种共享存储，为了解决这个问题，Kubernetes 从 v1.4 版本开始引入了 PetSet 这个新的资源对象，并且在 v1.5 版本是更名为 StatefulSet，StatefulSet 从本质上来说，可以看作 Deployment/RC 的一个特殊变种，它有如下一些特性：</p>
<ul>
<li>StatefulSet 里的每个 Pod 都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设 StatefulSet 的名字叫 kafka，那么第一个 Pod 叫 kafka-0，第二个叫 kafka-1，以此类推。</li>
<li>StatefulSet 控制的 Pod 副本的启停顺序是受控的，操作第 n 个 Pod 时，前 n-1 个 Pod 已经是运行且准备好的状态。</li>
<li>StatefulSet 里的 Pod 采用稳定的持久化存储卷，通过 PV/PVC 来实现，删除 Pod 时默认不会删除与 StatefulSet 相关的存储卷（为了保证数据的安全）。</li>
</ul>
<p>StatefulSet 除了要与 PV 卷捆绑使用以存储 Pod 的状态数据，还要与 Headless Service 配合使用，即在每个 StatefulSet 的定义中要声明它属于哪个 Headless Service。Headless Service 与普通 Service 的关键区别在于，它没有 Cluster IP，如果解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应的全部 Pod 的 Endpoint 列表。StatefulSet 在 Headless Service 的基础上又为 StatefulSet 控制的每个 Pod 实例创建了一个 DNS 域名，这个域名的格式为：<code>$(podname).$(headless service name)</code>。</p>
<p>比如一个 3 节点的 Kafka 的 StatefulSet 集群对应的 Headless Service 的名字为 kafka，则 StatefulSet 里面的 3 个 Pod 的 DNS 名称分别为 kafka-0.kafka、kafka-1.kafka、kafka-2.kafka，这些 DNS 名称可以直接在集群的配置文件中固定下来。</p>
<h5 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h5><p>Service 也是 Kubernetes 里的最核心的资源对象之一，Kubernetes 里的每个 Service 其实就是我们经常提起的微服务架构中的一个“微服务”，之前提到的 Pod、RC 等资源对象都是为它服务的。</p>
<p><img src="/assets/blogImg/k8s01_service.png" alt="k8s01_service"></p>
<p>从图中可以看到，Kubernetes 的 Service 定义了一个服务的访问入口地址，前端的应用（Pod）通过这个入口地址访问其背后的一组由 Pod 副本组成的集群实例，Service 与其后端 Pod 副本集群之间则是通过 Label Selector 来实现“无缝对接”的。而 RC 的作用实际上是保证 Service 的服务能力和服务质量始终处于预期的标准。</p>
<p>通过分析、识别并建模系统中的所有服务为微服务————Kubernetes Service，最终我们的系统由多个提供不同业务能力而又彼此独立的微服务单元所组成，服务之间通过 TCP/IP 进行通信，从而形成了强大而又灵活的弹性网格，拥有了强大的分布式能力、弹性扩展能力、容错能力，与此同时，我们的程序架构也变得简单和直观许多。</p>
<p><img src="/assets/blogImg/k8s01_microService.png" alt="k8s01_microService"></p>
<p>Kubernetes集群中，运行在每个 Node 上的 kube-proxy 进程其实就是一个智能的软件负载均衡器，它负责把对 Service 的请求转发到后端的某个 Pod 实例上，并在内部实现服务的负载均衡与会话保持机制。但 Service 不是共用一个负载均衡器的 IP 地址，而是每个 Service 分配了一个全局唯一的虚拟 IP 地址，这个虚拟 IP 被称为 ClusterIP。这样一来，每个服务就变成了具备唯一 IP 地址的“通信节点”，服务调用就变成了最基础的 TCP 网络通信问题。</p>
<p>我们知道，Pod 的 Endpoint 地址会随着 Pod 的销毁和重新创建而发生改变，因为新 Pod 的 IP 地址与之前旧 Pod 的不同。而 Service 一旦被创建，Kubernetes 就会自动为它分配一个可用的 Cluster IP，而且在 Service 的整个生命周期内，它的 Cluster IP 不会发生改变。于是，服务发现这个棘手的问题在 Kubernetes的架构里也得以轻松解决：只要用 Service 的 Name 与 Service 的 Cluster IP 地址做一个 DNS 域名映射即可完美解决问题。</p>
<h5 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h5><p>Volume 是 Pod 中能够被多个容器访问的共享目录。Kubernetes 的 Volume 概念、用途和目的与 Docker 的 Volume 比较类似，但两者不能等价。首先，Kubernetes 中的 Volume 定义在 Pod 上，然后被一个 Pod 里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume 中的数据也不会丢失。最后，Kubernetes 支持多种类型的 Volume，例如 GlusterFS、Ceph 等先进的分布式文件系统。</p>
<p>Volume 的使用也比较简单，在大多数情况下，我们先在 Pod 上声明一个 Volume，然后在容器里引用该 Volume 并 Mount 到容器里的某个目录上。比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">......</div><div class="line">template:</div><div class="line">  metadata:</div><div class="line">    labels:</div><div class="line">      app: app-demo</div><div class="line">      tier: frontend</div><div class="line">  spec:</div><div class="line">    volumes:</div><div class="line">      - name: datavol</div><div class="line">        emptyDir: &#123;&#125;</div><div class="line">    containers:</div><div class="line">    - name: tomcat-demo</div><div class="line">      image: tomcat</div><div class="line">      volumeMounts:</div><div class="line">        - mountPath: /mydata-data</div><div class="line">          name: datavol</div><div class="line">      imagePullPolicy: IfNotPresent</div><div class="line">......</div></pre></td></tr></table></figure>
<p>除了可以让一个 Pod 里的多个容器共享文件、让容器的数据写到宿主机的磁盘上或者写文件到网络存储中，Kubernetes 的 Volume 还扩展出了一种非常有实用价值的功能，即容器配置文件集中化定义与管理，这是通过 ConfigMap 这个资源对象来实现的。</p>
<p>Kubernetes 提供了非常丰富的<a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes" target="_blank" rel="external">Volume 类型</a>，这里说明两个最基础的：</p>
<p><strong>emptyDir</strong></p>
<p>一个 emptyDir Volume 是在 Pod 分配到 Node 时创建的。从它的名字就可以看出，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是 Kubernetes 自动分配的一个目录，当 Pod 从 Node 上移除时，emptyDir 中的数据也会被永久删除。emptyDir 的一些用途如下：</p>
<ul>
<li>临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留。</li>
<li>长时间任务的中间过程 CheckPoint 的临时保存目录。</li>
<li>一个容器需要从另一个容器中获取数据的目录（多容器共享目录）</li>
</ul>
<p><strong>hostPath</strong></p>
<p>hostPath 为在 Pod 上挂载宿主机上的文件或目录，它通常可以用于以下几方面：</p>
<ul>
<li>容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统进行存储。</li>
<li>需要访问宿主机上 Docker 引擎内部数据结构的容器应用时，可以通过定义 hostPath 为宿主机 /var/lib/docker 目录，使容器内部应用可以直接访问 Docker 的文件系统。</li>
</ul>
<p>在使用这种类型的 Volume 时，需要注意以下几点：</p>
<ul>
<li>在不同的 Node 上具有相同配置的 Pod 可能会因为宿主机上的目录和文件不同而导致对 Volume 上目录和文件的访问结果不一致。</li>
<li>如果使用了资源配额管理，则 Kubernetes 无法将 hostPaht 在宿主机上使用的资源纳入管理。</li>
</ul>
<h5 id="Persistent-Volume"><a href="#Persistent-Volume" class="headerlink" title="Persistent Volume"></a>Persistent Volume</h5><p>前面说到的 Volume 是定义在 Pod 上的，属于“计算资源”的一部分。而实际上，“网络存储”是相对独立于“计算资源”而存在的一种实体资源。比如在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中划出一个“网盘”并挂载到虚拟机上。Persistent Volume（简称 PV）和与之相关联的 Persistent Volume Claim（简称 PVC）也起到了类似的作用。</p>
<p>PV 可以理解成 Kubernetes 集群中的某个网络存储中对应的一块存储，它与 Volume 很类似，但有以下区别：</p>
<ul>
<li>PV 只能是网络存储，不属于任何 Node，但可以在每个 Node 上访问。</li>
<li>PV 并不是定义在 Pod 上的，而是独立于 Pod 之外定义。</li>
<li>PV 目前支持的类型包括：NFS、iSCSI、RBD、CephFS、Cinder、GlusterFS 等，还有各家的公有云存储。</li>
</ul>
<p>下面给出了 NFS 类型 PV 的一个定义文件，声明了需要 5Gi 的存储空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolume</div><div class="line">metadata:</div><div class="line">  name: pv0003</div><div class="line">spec:</div><div class="line">  capacity:</div><div class="line">    storage: 5Gi</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  nfs:</div><div class="line">    path: /somepath</div><div class="line">    server: 172.17.0.2</div></pre></td></tr></table></figure>
<p>比较重要的是 PV 的 accessModes 属性，目前有以下类型：</p>
<ul>
<li>ReadWriteOnce：读写权限、并且只能被单个 Node 挂载。</li>
<li>ReadOnlyMany：只读权限、允许被多个 Node 挂载。</li>
<li>ReadWriteMany：读写权限、允许被多个 Node 挂载。</li>
</ul>
<p>如果某个 Pod 向申请某种类型的 PV，则首先需要定义一个 PersistentVolumeClaim（PVC）对象：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: PersistentVolumeClain</div><div class="line">metadata:</div><div class="line">  name: myclaim</div><div class="line">spec:</div><div class="line">  accessModes:</div><div class="line">    - ReadWriteOnce</div><div class="line">  resources:</div><div class="line">    requests:</div><div class="line">      storage: 8Gi</div></pre></td></tr></table></figure>
<p>然后，在 Pod 的 Volume 定义中引用上述 PVC 即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">volumes:</div><div class="line">  - name: mypd</div><div class="line">    persistentVolumeClaim:</div><div class="line">      claimName: myclaim</div></pre></td></tr></table></figure>
<p>最后，我们说说 PV 的状态，PV 是有状态的对象，它有以下几种状态：</p>
<ul>
<li>Available： 空闲状态。</li>
<li>Bound： 已经绑定到某个 PVC 上。</li>
<li>Released：对应的 PVC 已经删除，但资源还没有被集群收回。</li>
<li>Failed：PV 自动回收失败。</li>
</ul>
<h5 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h5><p>Namespace（命名空间）是 Kubernetes 系统中的另一个非常重要的概念，Namespace 在很多情况下用于实现多租户的资源隔离。Namespace 通过将集群内部的资源对象“分配”到不同的 Namespace 中，形成逻辑上分组的不同项目、小组、或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。</p>
<p>Namespace 的定义很简单，如下所示的 yaml 定义了名为 development 的 Namespace：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: development</div></pre></td></tr></table></figure>
<p>一旦创建了 Namespace，我们在创建资源对象时就可以指定这个资源对象属于哪个 Namespace。比如在下面的例子中，我们定义了一个名为 busybox 的 Pod，放入 development 这个 Namespace里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: busybox</div><div class="line">  namespace: development</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">  - image: busybox</div><div class="line">    command:</div><div class="line">      - sleep</div><div class="line">      - &quot;3600&quot;</div><div class="line">    name: busybox</div></pre></td></tr></table></figure>
<p>当我们给每个租户创建一个 Namespace 来实现多租户的资源隔离时，还能结合 Kubernetes 的资源配额管理，限定不同租户能占用的资源，例如 CPU 使用量、内存使用量等。</p>
<h5 id="Annotation"><a href="#Annotation" class="headerlink" title="Annotation"></a>Annotation</h5><p>Annotation(注解）与 Label 类似，也使用 key/value 键值对的形式进行定义。不同的是 Label 具有严格的命名规范，它定义的是 Kubernetes 对象的元数据（Metadata），并且用于 Label Selector。而 Annotation 则是用户任意定义的“附加”信息，以便于外部工具进行查找，很多时候，Kubernetes 的模块自身会通过 Annotation 的方式标记资源对象的一些特殊信息。</p>
<p>通常来说，用 Annotation来记录的信息如下：</p>
<ul>
<li>build 信息、release 信息、Docker 镜像信息等，例如时间戳、release id 号、PR 号、镜像 hash 值、docker registry 地址等。</li>
<li>日志库、监控库、分析库等资源库的地址信息。</li>
<li>程序吊饰工具信息，例如工具名称、版本号等。</li>
<li>团队的联系信息，例如电话号码、负责人名称、网址等。</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/13/k8s01/" class="archive-article-date">
  	<time datetime="2019-03-12T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-13</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-docker02" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/06/docker02/">Docker 进阶</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="容器技术系列分享（三）"><a href="#容器技术系列分享（三）" class="headerlink" title="容器技术系列分享（三）"></a>容器技术系列分享（三）</h1><h2 id="Docker-进阶"><a href="#Docker-进阶" class="headerlink" title="Docker 进阶"></a>Docker 进阶</h2><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ul>
<li>Docker Image</li>
<li>Docker Network</li>
<li>Docker Volume</li>
<li>Docker Process</li>
<li>Docker security</li>
</ul>
<h4 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h4><p>Docker镜像应该是小而快的。假设你在BusyBox镜像中预编译Go二进制文件，他们就会变得又大又复杂。如果不能构建一个良好的Dockerfile来帮助你提高构建缓存命中率，那么你的镜像构建过程将会变得相当的缓慢。</p>
<p>比如一个用于软件安装的bash脚本，里面堆砌着大量的curl、wget等命令语句，大家在写Dockerfile的时候通常就会像写这个bash脚本一样，将一系列的Docker命令堆砌在其中，这种Dockerfile在构建镜像的时候是比较低效和缓慢的。</p>
<h5 id="秩序"><a href="#秩序" class="headerlink" title="秩序"></a>秩序</h5><p>有频率的改变Dockerfile中命令的排序，观察分析运行命令所耗费的时间及与其他镜像共享资源的方式。</p>
<p>比如像WORKDIR、CMD、ENV这些命令应该在底部，而RUN apt-get -y update更新应该在上面，因为它需要更长时间来运行，也可以与所有的镜像共享。</p>
<p>任何ADD（或其它缓存失效的命令）命令应该尽可能地在Dockerfile底部，在那里可以做出很多改变，后续命令缓存失效。</p>
<h5 id="合适的基础镜像"><a href="#合适的基础镜像" class="headerlink" title="合适的基础镜像"></a>合适的基础镜像</h5><p>比如Ruby2运行Ruby应用程序，Python3运行Python应用程序，但这两个镜像使用不同的基础镜像，所以你需要下载和构建不同的基础镜像。然而，如果使用Ubuntu运行这两个程序，就只需要下载一次基础镜像。</p>
<h5 id="优化层级"><a href="#优化层级" class="headerlink" title="优化层级"></a>优化层级</h5><p>在一个Dockerfile中每个命令都会在原来的基础上生成一层镜像，你可以使用三十多层命令，也可以通过组合RUN命令，并使用一行EXPOSE命令列出所有的开放端口，这样可以有效减少镜像的层数。</p>
<p>通过将RUN命令分组，可以在容器间分享更多的层。当然如果有一组命令可以多个容器通用，那么应该创建一个独立的基础镜像，它包含所建立的所有镜像。</p>
<p>对于每一层来说你都可以跨多个镜像分享，这样可以节省大量的磁盘空间。</p>
<h5 id="容器体积"><a href="#容器体积" class="headerlink" title="容器体积"></a>容器体积</h5><p>在创建容器并考虑到体积问题的时候，不要为了节省空间去使用体积小的镜像，尽量使用你将要提供数据的应用程序打成的镜像。如果你这样做了，并且提交了磁盘数据，你不仅在容器中储存了你的数据，而且对实际应用程序的调试也非常有用。</p>
<h5 id="消耗"><a href="#消耗" class="headerlink" title="消耗"></a>消耗</h5><p>当你已经构建了一个镜像，在运行它的时候发现有一个package缺少了，把它添加到Dockerfile的底部，而不是添加到顶部的run apt-get命令那里。这意味着你能尽快的重新构建这个镜像了。一旦你的镜像可以正常工作，你可以再提交源码之前重新优化整理Dockerfile。</p>
<h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"># 1 - Common Header / Packages</div><div class="line">FROM ubuntu:trusty</div><div class="line">MAINTAINER Jin Dingming &lt;jindm@2345.com&gt;</div><div class="line"></div><div class="line">RUN apt-get -yq update \</div><div class="line">&amp;&amp; apt-get -yqq install \</div><div class="line">wget \</div><div class="line">curl \</div><div class="line">git \</div><div class="line">software-properties-common</div><div class="line"></div><div class="line"># 2 - Python</div><div class="line">RUN \</div><div class="line">apt-get -yqq install \</div><div class="line">python-dev \</div><div class="line">python-pip \</div><div class="line">python-pysqlite2 \</div><div class="line">python-mysqldb</div><div class="line"></div><div class="line"># 3 - Apache</div><div class="line">RUN \</div><div class="line">apt-get -yqq install \</div><div class="line">apache2 \</div><div class="line">apache2-utils</div><div class="line"></div><div class="line"># 4 - Apache ENVs</div><div class="line">ENV APACHE_CONFDIR /etc/apache2</div><div class="line">ENV APACHE_ENVVARS $APACHE_CONFDIR/envvars</div><div class="line">ENV APACHE_RUN_USER www-data</div><div class="line">ENV APACHE_RUN_GROUP www-data</div><div class="line">ENV APACHE_RUN_DIR /var/run/apache2</div><div class="line">ENV APACHE_PID_FILE $APACHE_RUN_DIR/apache2.pid</div><div class="line">ENV APACHE_LOCK_DIR /var/lock/apache2</div><div class="line">ENV APACHE_LOG_DIR /var/log/apache2</div><div class="line"></div><div class="line"># 5 - Graphite and Deps</div><div class="line">RUN \</div><div class="line">apt-get -yqq install \</div><div class="line">libapache2-mod-python \</div><div class="line">python-cairo \</div><div class="line">python-jinja2 \</div><div class="line">sqlite3</div><div class="line"></div><div class="line">RUN \</div><div class="line">pip install whisper \</div><div class="line">carbon \</div><div class="line">graphite-web \</div><div class="line">&apos;Twisted&lt;12.0&apos; \</div><div class="line">&apos;django&lt;1.6&apos; \</div><div class="line">django-tagging</div><div class="line"></div><div class="line"># 6 - Other</div><div class="line">EXPOSE 80 2003 2004 7002</div><div class="line"></div><div class="line">WORKDIR /app</div><div class="line"></div><div class="line">VOLUME /opt/graphite/data</div><div class="line"></div><div class="line"># Define default command.</div><div class="line">CMD [&quot;/app/bin/start_graphite&quot;]</div><div class="line"></div><div class="line"># 7 - First use of ADD</div><div class="line">ADD . /app</div><div class="line"></div><div class="line"># 8 - Final setup</div><div class="line">RUN mkdir -p /app/wsgi \</div><div class="line">&amp;&amp; useradd -d /app -c &apos;application&apos; -s &apos;/bin/false&apos; graphite \</div><div class="line">&amp;&amp; chmod +x /app/bin/* \</div><div class="line">&amp;&amp; chown -R graphite:graphite /app \</div><div class="line">&amp;&amp; chown -R graphite:graphite /opt/graphite \</div><div class="line">&amp;&amp; rm -f /etc/apache2/sites-enabled/* \</div><div class="line">&amp;&amp; mv /app/apache-graphite.conf /etc/apache2/sites-enabled/apache-graphite.conf</div></pre></td></tr></table></figure>
<h5 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h5><ol>
<li><p>Common Header/Packages</p>
<p> 这是最常见的共享层，在同一个主机上运行所有镜像应该从它开始。可以看到这里添加了一些诸如curl和git的操作，他们不是必须的，但是对调试很有用，而且因为他们在分享层，所以不会占用太多空间。</p>
</li>
<li><p>Python</p>
</li>
<li><p>Apache</p>
<p> 2-3是语言规范层。这里的python和apache，把谁放在前面并没有硬性规定，主要看业务契合度。</p>
</li>
<li><p>Apache Envs</p>
<p> 在Apache安装好之后直接配置环境依赖，以便于其他镜像构建时尽量多应用缓存。</p>
</li>
<li><p>Graphite and Deps</p>
<p> 这里包含一些特定的apt和pip资源包，利用&amp;&amp;符号连接多个单一命令能减少镜像层级数量。</p>
</li>
<li><p>Other</p>
<p> 这部分配置镜像本身的属性，如端口映射、目录挂载等。</p>
</li>
<li><p>First ADD</p>
<p> 最后，将需要的应用程序打包进镜像。</p>
</li>
<li><p>Final setup</p>
<p> 容器的启动命令。</p>
</li>
</ol>
<h4 id="Docker-Network"><a href="#Docker-Network" class="headerlink" title="Docker Network"></a>Docker Network</h4><p>Docker 网络为容器安全提供了隔离，不同的网络方案在效率和复杂度上也有区别，选择一个最合适的网络方案对应用性能和后期维护成本都有至关重要的影响。</p>
<h5 id="默认网络（Default-Networks）"><a href="#默认网络（Default-Networks）" class="headerlink" title="默认网络（Default Networks）"></a>默认网络（Default Networks）</h5><p>当你安装好Docker后，它会自动创建三个网络，你可以使用<code>docker network ls</code>命令列举它们:</p>
<p><img src="/assets/blogImg/docker_networks.png" alt="docker_networks"></p>
<ul>
<li>bridge 网络</li>
</ul>
<p>Docker安装好后都会提供默认的bridge网络即docker0网络。如果不指定<code>docker run --net=&lt;network&gt;</code>的话，Docker daemon会默认将容器连接到这个网络。在宿主机中可以看到这个网络。</p>
<p><img src="/assets/blogImg/docker_network_bridge.png" alt="docker_network_bridge"></p>
<ul>
<li>none 网络</li>
</ul>
<p>none 网络会添加容器到一个容器自己的网络栈，但是并没有网络接口。</p>
<p><img src="/assets/blogImg/docker_network_none.png" alt="docker_network_none"></p>
<ul>
<li>host 网络</li>
</ul>
<p>host 网络添加一个容器到宿主机的网络栈中，你会发现容器中的网络配置和宿主机一样。</p>
<h5 id="自定义网络（User-defined-networks）"><a href="#自定义网络（User-defined-networks）" class="headerlink" title="自定义网络（User-defined networks）"></a>自定义网络（User-defined networks）</h5><p>除了Docker提供的默认网络，用户还可以创建自定义网络以便提供更好的容器网络隔离，Docker为创建自定义网络提供了一些默认的 network driver。你可以创建一个新的 bridge network 或者 overlay network，也可以创建一个network plugin或者remote network。</p>
<p>用户也可以创建多个网络，把容器连接到不止一个网络中。容器仅可以同网络内的容器进行通信而不能跨网络通信。</p>
<ul>
<li>自定义bridge网络</li>
</ul>
<p>最简单的用户自定义网络就是创建一个bridge网络。</p>
<p><img src="/assets/blogImg/docker_network_isolatedbridge.png" alt="docker_network_isolatedbridge"></p>
<p>创建完之后，就可以指定新创建的容器运行在这个网络上。</p>
<p><img src="/assets/blogImg/docker_network_isolatedbridgerun.png" alt="docker_network_isolatedbridgerun"></p>
<h5 id="overlay网络（An-overlay-network）"><a href="#overlay网络（An-overlay-network）" class="headerlink" title="overlay网络（An overlay network）"></a>overlay网络（An overlay network）</h5><p>Docker的overlay网络驱动提供原生开箱即用（out-of-the-box）的跨主机网络。完成这个支持是基于 <em>libnetwork</em> 和 <em>libkv</em>，libnetwork是一个内置基于VXLAN overlay网络驱动的一个库。</p>
<p>overlay网络需要一个可用的key-value存储服务，目前Docker的libkv支持Consul、Etcd、Zookeeper。</p>
<p><img src="/assets/blogImg/docker_network_overlay.png" alt="docker_network_overlay"></p>
<p><strong>overlay网络方案对比</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">Flannel</th>
<th style="text-align:center">Calico</th>
<th style="text-align:center">macvlan</th>
<th style="text-align:center">Open vSwitch</th>
<th style="text-align:right">route</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">方案特性</td>
<td style="text-align:center">通过虚拟设备flannel0实现对docker0的管理</td>
<td style="text-align:center">基于BGP协议的纯三层网路方案</td>
<td style="text-align:center">基于Linux Kernel的macvlan技术</td>
<td style="text-align:center">基于隧道的虚拟路由器技术</td>
<td style="text-align:right">基于Linux Kernel的vRoute技术</td>
</tr>
<tr>
<td style="text-align:left">网络要求</td>
<td style="text-align:center">三层互通</td>
<td style="text-align:center">三层互通</td>
<td style="text-align:center">二层互通</td>
<td style="text-align:center">三层互通</td>
<td style="text-align:right">二层互通</td>
</tr>
<tr>
<td style="text-align:left">配置难度</td>
<td style="text-align:center">简单，基于Etcd。</td>
<td style="text-align:center">简单，基于Etcd。</td>
<td style="text-align:center">简单，直接使用宿主机网络，需要仔细规划IP范围。</td>
<td style="text-align:center">复杂，需要手工配置各节点的bridge。</td>
<td style="text-align:right">简单，使用宿主机vRoute功能，需要仔细规划每个Node的IP地址范围。</td>
</tr>
<tr>
<td style="text-align:left">网络性能</td>
<td style="text-align:center">host-gw&gt;VXLAN&gt;UDP</td>
<td style="text-align:center">BGP模式性能损失小，IPIP模式较小</td>
<td style="text-align:center">性能损失可忽略</td>
<td style="text-align:center">性能损失较小</td>
<td style="text-align:right">性能损失小</td>
</tr>
<tr>
<td style="text-align:left">网络限制</td>
<td style="text-align:center">无</td>
<td style="text-align:center">在不支持BGP协议的网络下无法使用</td>
<td style="text-align:center">基于macvlan的容器无法与宿主机网络通信</td>
<td style="text-align:center">无</td>
<td style="text-align:right">在无法实现大二层互通的网络环境下无法使用</td>
</tr>
</tbody>
</table>
<h5 id="自定义网络插件（Custom-network-plugin）"><a href="#自定义网络插件（Custom-network-plugin）" class="headerlink" title="自定义网络插件（Custom network plugin）"></a>自定义网络插件（Custom network plugin）</h5><p>你可以编写自己的网络驱动，驱动是和Docker daemon运行在同一台主机上的一个进程，并且由Docker plugin系统调用和使用。</p>
<p>网络插件和其他的Docker插件一样受到一些限制和安装规则。所有的插件使用Plugin API，有自己的生命周期，包含：安装、启动、停止、激活。</p>
<p>自定义的网络驱动安装好后，可以像使用内置的网络驱动一样使用它，例如：</p>
<p><code>docker network create --driver weave mynet</code></p>
<h5 id="Docker内置DNS服务器（Embedded-DNS-Server）"><a href="#Docker内置DNS服务器（Embedded-DNS-Server）" class="headerlink" title="Docker内置DNS服务器（Embedded DNS Server）"></a>Docker内置DNS服务器（Embedded DNS Server）</h5><p>Docker daemon会为每个连接到自定义网络的容器运行一个内置的DNS服务提供自动的服务发现。域名解析的请求会首先被内置的DNS服务器拦截，如果内置的DNS服务器不能解析这个请求，它才会被转发到外部的容器配置的DNS服务器。基于这个机制，容器的resolv.conf文件会将DNS服务器配置为127.0.0.1，即内置DNS服务器监听的地址。</p>
<h4 id="Docker-Volume"><a href="#Docker-Volume" class="headerlink" title="Docker Volume"></a>Docker Volume</h4><p><img src="/assets/blogImg/docker_volume.jpeg" alt="docker_volume"></p>
<p>Volume数据卷是Docker的一个重要概念。数据卷是可供一个或多个容器使用的特殊目录，可以为容器应用存储提供有价值的特性：</p>
<ul>
<li>持久化数据与容器的生命周期解耦：在容器删除之后数据卷中的内容可以保持。Docker 1.9之后引进的<em>named volume</em>可以更加方便地管理数据卷的生命周期，数据卷可以被独立地创建和删除。</li>
<li>数据卷可以用于实现容器之间的数据共享</li>
<li>可以支持不同类型的数据存储实现</li>
</ul>
<p>Docker缺省提供了对宿主机本地文件卷的支持，可以将宿主机的目录挂载到容器之中。由于没有容器分层文件系统带来的性能损失，本地文件卷非常适合一些需要高性能数据访问的场景，比如MySQL的数据库文件存储。</p>
<p>同时Docker支持通过volume plugin实现不同类型的数据卷，可以更加灵活解决不同应用负载的存储需求。</p>
<p>但是Docker数据卷的权限管理经常令人困惑，后面就结合实例介绍Docker数据卷权限管理中的常见问题和解决方法。</p>
<h5 id="从Jenkins挂载本地数据卷错误谈起"><a href="#从Jenkins挂载本地数据卷错误谈起" class="headerlink" title="从Jenkins挂载本地数据卷错误谈起"></a>从Jenkins挂载本地数据卷错误谈起</h5><p>首先，我们使用Jenkins官方镜像启动一个容器，并检查日志。</p>
<p>可以发现容器日志一切正常。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker run -d -p 8080:8080 -p 50000:50000 --name jenkins jenkins</div><div class="line">docker logs jenkins</div></pre></td></tr></table></figure>
<p>但是，为了持久化Jenkins的数据，当我们把宿主机目录挂载到容器中时，问题出现了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">docker rm -f jenkins</div><div class="line">docker run -d -p 8080:8080 -p 50000:50000 -v $(pwd)/data:/var/jenkins_home --name jenkins jenkins</div><div class="line">docker logs jenkins</div></pre></td></tr></table></figure>
<p>错误日志如下：</p>
<p><img src="/assets/blogImg/docker_volume_jenkins01.png" alt="docker_volume_jenkins01"></p>
<p>不映射本地数据卷时，查看jenkins容器的当前用户是jenkins,且”/var/jenkins_home”目录权限归属于jenkins用户。</p>
<p><img src="/assets/blogImg/docker_volume_jenkins02.png" alt="docker_volume_jenkins02"></p>
<p>而映射本地数据卷时，”/var/jenkins_home”目录的拥有者变成了root用户。</p>
<p><img src="/assets/blogImg/docker_volume_jenkins03.png" alt="docker_volume_jenkins03"></p>
<p>这就解释了为什么当jenkins用户的进程访问“/var/jenkins_home“时，会出现Permission denied的问题。</p>
<p>我们再检查一下宿主机上的数据卷目录，当前路径下“data”目录的拥有者是root，这是因为这个目录是Docker进程缺省创建出来的。</p>
<p><img src="/assets/blogImg/docker_volume_jenkins04.png" alt="docker_volume_jenkins04"></p>
<p>发现问题之后，相应的解决办法也很简单：把当前目录的拥有者赋值给 <em>uid 1000</em>，再启动jenkins容器就一切正常了。</p>
<p>虽然问题解决了，但思考并没有结束。因为当使用本地数据卷时，jenkins容器会依赖宿主机目录权限的正确性，这会给自动化部署带来额外的工作。如何让jenkins容器为数据卷自动地设置正确的权限？这个问题对很多以non-root方式运行的应用也都有借鉴意义。</p>
<h5 id="为-non-root-应用正确地挂载本地数据卷"><a href="#为-non-root-应用正确地挂载本地数据卷" class="headerlink" title="为 non-root 应用正确地挂载本地数据卷"></a>为 non-root 应用正确地挂载本地数据卷</h5><p>基本思路有两个：</p>
<ul>
<li>一个是利用Data Container的方法在容器见共享数据卷。这样就规避了解决宿主机上数据卷的权限问题。由于在1.9版本之后，Docker提供了named volume来取代纯数据容器，所以还需要真正地解决这个问题。</li>
<li>另外一个思路就是让容器中进程以root用户启动，在容器启动脚本中利用chown命令来修正数据卷文件权限，之后切换到non-root用户来执行程序。</li>
</ul>
<p>参照第二个思路来解决之前jenkins的问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">FROM jenkins:latest</div><div class="line">USER root</div><div class="line">RUN GOSU_SHA=5ec5d23079e94aea5f7ed92ee8a1a34bbf64c2d4053dadf383992908a2f9dc8a \</div><div class="line">  &amp;&amp; curl -sSL -o /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.9/gosu-$(dpkg --print-architecture)&quot; \</div><div class="line">  &amp;&amp; chmod +x /usr/local/bin/gosu \</div><div class="line">  &amp;&amp; echo &quot;$GOSU_SHA  /usr/local/bin/gosu&quot; | sha256sum -c - </div><div class="line">COPY entrypoint.sh /entrypoint.sh</div><div class="line">ENTRYPOINT [&quot;/entrypoint.sh&quot;]</div></pre></td></tr></table></figure>
<p>这是一个基于jenkins镜像的Dockerfile：它会切换到root用户并在镜像中添加<em>gosu</em>命令，和新的入口点”/entrypoint.sh”。</p>
<blockquote>
<p>gosu 是经常出现在官方Docker镜像中的一个小工具。它是su和sudo命令的轻量级替代品，并解决了它们在tty和信号传递中的一些问题。</p>
</blockquote>
<p>新入口点的entrypoint.sh内容如下：它会为JENKINS_HOME目录设置jenkins的拥有权限，并且再利用gosu命令切换到jenkins用户来执行jenkins应用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">#! /bin/bash</div><div class="line">set -e</div><div class="line">chown -R 1000 &quot;$JENKINS_HOME&quot;</div><div class="line">exec gosu jenkins /bin/tini -- /usr/local/bin/jenkins.sh</div></pre></td></tr></table></figure>
<h4 id="Docker-Process"><a href="#Docker-Process" class="headerlink" title="Docker Process"></a>Docker Process</h4><p>Docker在进程管理上有一些特殊之处，如果不注意这些细节就会带来一些隐患。另外Docker鼓励“一个容器一个进程（one process per container)”的方式。这种方式非常适合以单进程为主的微服务架构的应用。然而由于一些传统的应用是由若干紧耦合的多个进程构成的，这些进程难以拆分到不同的容器中，所以在单个容器内运行多个进程便成了一种折衷方案；此外在一些场景中，用户期望利用Docker容器来作为轻量级的虚拟化方案，动态的安装配置应用，这也需要在容器中运行多个进程。而在Docker容器中正确运行多进程应用将会带来更多的挑战。</p>
<h5 id="容器的PID-namespace"><a href="#容器的PID-namespace" class="headerlink" title="容器的PID namespace"></a>容器的PID namespace</h5><p>在Docker中，进程管理的基础就是Linux内核中的PID名空间技术。在不同PID名空间中，进程ID是独立的；即在两个不同名空间下的进程可以有相同的PID。</p>
<p>Linux内核为所有的PID名空间维护了一个树状结构：最顶层的是系统初始化时创建的root namespace（根名空间），再创建的新PID namespace就称之为child namespace（子名空间），而原先的PID名空间就是新创建的PID名空间的parent namespace（父名空间）。通过这种方式，系统中的PID名空间会形成一个层级体系。父节点可以看到子节点中的进程，并可以通过信号等方式对子节点中的进程产生影响。反过来，子节点不能看到父节点名空间中的任何内容，也不可能通过kill或ptrace影响父节点或其他名空间中的进程。</p>
<p>在Docker中，每个Container都是Docker Daemon的子进程，每个Container进程缺省都具有不同的PID名空间。通过名空间技术，Docker实现容器间的进程隔离。另外Docker Daemon也会利用PID名空间的树状结构，实现了对容器中的进程交互、监控和回收。注：Docker还利用了其他名空间（UTS，IPC，USER）等实现了各种系统资源的隔离，由于这些内容和进程管理关联不多，本文不会涉及。</p>
<p>当创建一个Docker容器的时候，就会新建一个PID名空间。容器启动进程在该名空间内PID为1。当PID1进程结束之后，Docker会销毁对应的PID名空间，并向容器内所有其它的子进程发送SIGKILL。</p>
<h5 id="如何指明容器PID1进程"><a href="#如何指明容器PID1进程" class="headerlink" title="如何指明容器PID1进程"></a>如何指明容器PID1进程</h5><p>在Docker容器中的初始化进程（PID1进程）在容器进程管理上具有特殊意义。它可以被Dockerfile中的 <em>ENTRYPOINT</em> 或 <em>CMD</em> 所指明；也可以被docker run命令的启动参数所覆盖。了解这些细节可以帮助我们更好地了解PID1进程的行为。</p>
<p>关于ENTRYPOINT和CMD指令的不同，可以参见官方的Dockerfile说明和最佳实践</p>
<p><a href="https://docs.docker.com/engine/reference/builder/#entrypoint" target="_blank" rel="external">https://docs.docker.com/engine/reference/builder/#entrypoint</a></p>
<p><a href="https://docs.docker.com/engine/reference/builder/#cmd" target="_blank" rel="external">https://docs.docker.com/engine/reference/builder/#cmd</a></p>
<p>值得注意的一点是：在ENTRYPOINT和CMD指令中，提供两种不同的进程执行方式 shell 和 exec。</p>
<p>在shell方式中，CMD/ENTRYPOINT指令以如下方式定义：</p>
<p><code>CMD executable param1 param2</code></p>
<p>这种方式中的PID1进程是以<em>/bin/sh -c “executable param1 param2”</em>方式启动的</p>
<p><code>CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]</code></p>
<p>注意这里的可执行命令和参数是利用JSON字符串数组的格式定义的，这样PID1进程会以<em>executable param1 param2</em>方式启动的。另外，在<em>docker run</em>命令中指明的命令行参数也是以 exec 方式启动的。</p>
<p>为了解释两种不同运行方式的区别，我们利用不同的Dockerfile分别创建两个Redis镜像</p>
<p>“Dockerfile_shell”文件内容如下，会利用shell方式启动redis服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:18.04</div><div class="line">RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*</div><div class="line">EXPOSE 6379</div><div class="line">CMD &quot;/usr/bin/redis-server&quot;</div></pre></td></tr></table></figure>
<p>“Dockerfile_exec”文件内容如下，会利用exec方式启动redis服务</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:18.04</div><div class="line">RUN apt-get update &amp;&amp; apt-get -y install redis-server &amp;&amp; rm -rf /var/lib/apt/lists/*</div><div class="line">EXPOSE 6379</div><div class="line">CMD [&quot;/usr/bin/redis-server&quot;]</div></pre></td></tr></table></figure>
<p>然后给予他们构建两个镜像“myredis:shell”和“myredis:exec”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker build -t myredis:shell -f Dockerfile_shell .</div><div class="line">docker build -t myredis:exec -f Dockerfile_exec .</div></pre></td></tr></table></figure>
<p>运行”myredis:shell”镜像，我们可以发现它的启动进程(PID1)是/bin/sh -c “/usr/bin/redis-server”，并且它创建了一个子进程/usr/bin/redis-server *:6379。</p>
<p>而运行“myredis:exec”镜像，我们可以发现它的启动进程是/usr/bin/redis-server *:6379，并没有其他子进程存在。</p>
<p><img src="/assets/blogImg/docker_process_redis.png" alt="docker_process_redis"></p>
<p>由此我们可以清楚的看到，以exec和shell方式执行命令可能会导致容器的PID1进程不同。然而这又有什么问题呢？</p>
<p>原因在于：PID1进程对于操作系统而言具有特殊意义。操作系统的PID1进程是init进程，以守护进程方式运行，是所有其他进程的祖先，具有完整的进程生命周期管理能力。在Docker容器中，PID1进程是启动进程，它也会负责容器内部进程管理的工作。而这也将导致进程管理在Docker容器内部和完整操作系统上的不同。</p>
<h5 id="进程信号处理"><a href="#进程信号处理" class="headerlink" title="进程信号处理"></a>进程信号处理</h5><p>信号是Unix/Linux中进程间异步通信机制。Docker提供了两个命令docker stop和docker kill来向容器中的PID1进程发送信号。</p>
<p>当执行docker stop命令时，docker会首先向容器的PID1进程发送一个SIGTERM信号，用于容器内程序的退出。如果容器在收到SIGTERM后没有结束， 那么Docker Daemon会在等待一段时间（默认是10s）后，再向容器发送SIGKILL信号，将容器杀死变为退出状态。这种方式给Docker应用提供了一个优雅的退出(graceful stop)机制，允许应用在收到stop命令时清理和释放使用中的资源。而docker kill可以向容器内PID1进程发送任何信号，缺省是发送SIGKILL信号来强制退出应用。</p>
<blockquote>
<p>从Docker 1.9开始，Docker支持停止容器时向其发送自定义信号，开发者可以在Dockerfile使用STOPSIGNAL指令，或docker run命令中使用–stop-signal参数中指明，缺省是SIGTERM。</p>
</blockquote>
<p>我们来看看不同的PID1进程，对进程信号处理的不同之处。首先，我们使用docker stop命令停止由 exec 模式启动的“myredis2”容器，并检查其日志。</p>
<p><img src="/assets/blogImg/docker_process_redis02.png" alt="docker_process_redis02"></p>
<p>我们发现对“myredis2”容器的stop命令几乎立即生效；而且在容器日志中，我们看到了“Received SIGTERM scheduling shutdown”的内容，说明redis-server进程接收到了SIGTERM消息，并优雅地推出。</p>
<p>我们再对利用shell模式启动的“myredis”容器发出停止操作，并检查其日志。</p>
<p><img src="/assets/blogImg/docker_process_redis03.png" alt="docker_process_redis03"></p>
<p>我们发现对”myredis”容器的stop命令暂停了一会儿才结束，而且在日志中我们没有看到任何收到SIGTERM信号的内容。原因其PID1进程sh没有对SIGTERM信号的处理逻辑，所以它忽略了所接收到的SIGTERM信号。当Docker等待stop命令执行10秒钟超时之后，Docker Daemon发送SIGKILL强制杀死sh进程，并销毁了它的PID名空间，其子进程redis-server也在收到SIGKILL信号后被强制终止。如果此时应用还有正在执行的事务或未持久化的数据，强制进程退出可能导致数据丢失或状态不一致。</p>
<p>通过这个示例我们可以清楚的理解PID1进程在信号管理的重要作用。所以，</p>
<ul>
<li>容器的PID1进程需要能够正确的处理SIGTERM信号来支持优雅退出。</li>
<li>如果容器中包含多个进程，需要PID1进程能够正确的传播SIGTERM信号来结束所有的子进程之后再推出。</li>
<li>确保PID1进程是期望的进程。缺省sh/bash进程没有提供SIGTERM的处理，需要通过shell的脚本来设置正确的PID1进程，或捕获SIGTERM信号。</li>
</ul>
<p>另外需要注意的是：由于PID1进程的特殊性，Linux内核为他做了特殊处理。如果它没有提供某个信号的处理逻辑，那么与其在同一个PID名空间下的进程发送给它的该信号都会被屏蔽。这个功能的主要作用是防止init进程被误杀。我们可以验证在容器内部发出的SIGKILL信号无法杀死PID1进程.</p>
<p><img src="/assets/blogImg/docker_process_redis04.png" alt="docker_process_redis04"></p>
<h5 id="孤儿进程与僵尸进程管理"><a href="#孤儿进程与僵尸进程管理" class="headerlink" title="孤儿进程与僵尸进程管理"></a>孤儿进程与僵尸进程管理</h5><blockquote>
<p>(新版本已优化僵尸进程回收)</p>
</blockquote>
<p>熟悉Unix/Linux进程管理的同学对多进程应用并不陌生。</p>
<p>当一个子进程终止后，它首先会变成一个“失效(defunct)”的进程，也称为“僵尸（zombie）”进程，等待父进程或系统收回（reap）。在Linux内核中维护了关于“僵尸”进程的一组信息（PID，终止状态，资源使用信息），从而允许父进程能够获取有关子进程的信息。如果不能正确回收“僵尸”进程，那么他们的进程描述符仍然保存在系统中，系统资源会缓慢泄露。</p>
<p>大多数设计良好的多进程应用可以正确的收回僵尸子进程，比如NGINX master进程可以收回已终止的worker子进程。如果需要自己实现，则可利用如下方法：</p>
<ol>
<li>利用操作系统的waitpid()函数等待子进程结束并清除它的僵尸进程。</li>
<li>由于当子进程成为“defunct”进程时，父进程会收到一个SIGCHLD信号，所以我们可以在父进程中指定信号处理的函数来忽略SIGCHLD信号，或者自定义收回处理逻辑。</li>
</ol>
<p>下面这些文章详细介绍了对僵尸进程的处理方法</p>
<ul>
<li><a href="http://www.microhowto.info/howto/reap_zombie_processes_using_a_sigchld_handler.html" target="_blank" rel="external">http://www.microhowto.info/howto/reap_zombie_processes_using_a_sigchld_handler.html</a></li>
<li><a href="http://lbolla.info/blog/2014/01/23/die-zombie-die" target="_blank" rel="external">http://lbolla.info/blog/2014/01/23/die-zombie-die</a></li>
</ul>
<p>如果父进程已经结束了，那些依然在运行中的子进程会成为“孤儿（orphaned）”进程。在Linux中Init进程(PID1)作为所有进程的父进程，会维护进程树的状态，一旦有某个子进程成为了“孤儿”进程后，init就会负责接管这个子进程。当一个子进程成为“僵尸”进程之后，如果其父进程已经结束，init会收割这些“僵尸”，释放PID资源。</p>
<p>然而由于Docker容器的PID1进程是容器启动进程，它们会如何处理那些“孤儿”进程和“僵尸”进程？</p>
<p>下面我们做几个试验来验证不同的PID1进程对僵尸进程不同的处理能力</p>
<p>首先在myredis2容器中启动一个bash进程，并创建子进程“sleep 1000”</p>
<p><img src="/assets/blogImg/docker_process_redis05.png" alt="docker_process_redis05"></p>
<p>在另一个终端窗口，查看当前进程，我们可以发现一个sleep进程是bash进程的子进程。</p>
<p><img src="/assets/blogImg/docker_process_redis06.png" alt="docker_process_redis06"></p>
<p>我们杀死bash进程之后查看进程列表，这时候bash进程已经被杀死。这时候sleep进程(PID为49)，虽然已经结束，而且被PID1进程（redis-server）接管，但是其没有被父进程回收，成为僵尸状态。</p>
<p>这是因为PID1进程“redis-server”没有考虑过作为init对僵尸子进程的回收的场景。</p>
<hr>
<p>同样的实验对myredis容器测试，发现“bash”和“sleep 1000”进程都已经被杀死和回收。这是因为sh/bash等应用可以自动清理僵尸进程。</p>
<blockquote>
<p>如果在容器中运行多个进程，PID1进程需要有能力接管“孤儿”进程并回收“僵尸”进程。Docker从1.13版本开始提供了 docker run –init 参数，可以运行一个init来启动容器，并且提供信号传播和进程回收的作用。</p>
</blockquote>
<h5 id="进程监控"><a href="#进程监控" class="headerlink" title="进程监控"></a>进程监控</h5><p>在Docker中，如果docker run命令中指明了<a href="https://docs.docker.com/engine/reference/run/?spm=a2c4e.11153940.blogcont5545.18.2e60417f3DjeS6#restart-policies-restart" target="_blank" rel="external">restart policy</a>,Docker daemon会监控PID1进程，并根据策略自动重启已结束的容器。</p>
<table>
<thead>
<tr>
<th style="text-align:left"></th>
<th style="text-align:center">Flannel</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">no</td>
<td style="text-align:center">不自动重启，缺省值。</td>
</tr>
<tr>
<td style="text-align:left">on-failure[:max-retries]</td>
<td style="text-align:center">当PID1进程退出值非0时，自动重启容器；可以指定最大重试次数。</td>
</tr>
<tr>
<td style="text-align:left">always</td>
<td style="text-align:center">永远自动重启容器；当Docker daemon启东市，会自动启动容器。</td>
</tr>
<tr>
<td style="text-align:left">unless-stopped</td>
<td style="text-align:center">永远自动重启容器；当Docker daemon启动时，如果之前容器不为stoped状态就自动启动容器。</td>
</tr>
</tbody>
</table>
<blockquote>
<p>注意：为防止频繁重启故障应用导致系统过载，Docker会在每次重启过程中会延迟一段时间。Docker重启进程的延迟时间从100ms开始并每次加倍，如100ms，200ms，400ms等等。</p>
</blockquote>
<p>利用Docker内置的restart策略可以大大简化应用进程监控的负担。但是Docker Daemon只是监控PID1进程，如果容器在内包含多个进程，仍然需要开发人员来处理进程监控。</p>
<p>还有大家熟悉的<a href="http://supervisord.org/introduction.html" target="_blank" rel="external">Supervisor</a>，<a href="https://mmonit.com/monit/" target="_blank" rel="external">Monit</a>等进程监控工具，它们可以方便的在容器内部实现进程监控。Docker提供了相应的<a href="https://docs.docker.com/engine/reference/run/" target="_blank" rel="external">文档</a>来介绍，网上也有很多资料。</p>
<p>另外利用Supervisor等工具作为PID1进程是在容器中支持多进程管理的主要实现方式；和简单利用shell脚本fork子进程相比，采用Supervisor等工具有很多好处：</p>
<ul>
<li>一些传统的服务不能以PID1进程的方式执行，利用Supervisor可以方便的适配</li>
<li>Supervisor这些监控工具大多提供了对SIGTERM的信号传播支持，可以支持子进程优雅的退出。</li>
</ul>
<p>然而值得注意的是：Supervisor这些监控工具大多没有完全提供Init支持的进程管理能力，如果需要支持子进程回收的场景需要配合正确的PID1进程来完成</p>
<h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>进程管理在Docker容器中和在完整的操作系统有一些不同之处。在每个容器的PID1进程，需要能够正确的处理SIGTERM信号来支持容器应用的优雅退出，同时要能正确的处理孤儿进程和僵尸进程。必要的时候使用Docker新提供的 docker run –init 参数可以解决相应问题。</p>
<p>在Dockerfile中要注意shell模式和exec模式的不同。通常而言我们鼓励使用exec模式，这样可以避免由无意中选择错误PID1进程所引入的问题。</p>
<p>在Docker中“一个容器一个进程的方式”并非绝对化的要求，然而在一个容器中实现对于多个进程的管理必须考虑更多的细节，比如子进程管理，进程监控等等。所以对于常见的需求，比如日志收集，性能监控，调试程序，我们依然建议采用多个容器组装的方式来实现。</p>
<h4 id="Docker-security"><a href="#Docker-security" class="headerlink" title="Docker security"></a>Docker security</h4><p>主要从四块区域思考Docker安全：</p>
<ul>
<li>Linux内核层面的安全：namespace和cgroups</li>
<li>Docker daemon的攻击面</li>
<li>容器配置时的漏洞</li>
<li>内核的安全强化功能，以及如何它们如何与容器交互</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/06/docker02/" class="archive-article-date">
  	<time datetime="2019-03-05T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-06</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-continuous" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/04/continuous/">持续交付实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>传统交付过程中遇到的问题</li>
<li>变革软件交付方式的技术：Docker</li>
<li>应用 Docker 化交付的过程实践</li>
</ol>
<h3 id="研发过程的困境"><a href="#研发过程的困境" class="headerlink" title="研发过程的困境"></a>研发过程的困境</h3><ul>
<li>任何一家互联网或者软件公司，随着产品规模的扩大，市场需求的变化，都会逐步的发现产品版本管理混乱，运维人员总是在兜底， 不知道开发/测试/集成/预发布/生产等等环境到底经历过几代运维人员之手，所以环境压根没人敢动。</li>
<li>因为市场永远在变化，需求一定在变化，人员也在变化，导致了研发过程中遇到的这样那样的问题。因此，大多数企业都用CI/CD这个解决方案来应对，如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_cicd.png" alt="continuous_cicd"></p>
<ul>
<li>另外，我认为的持续交付概念如下：</li>
</ul>
<blockquote>
<p><strong>在一起</strong>就是集成，每次集成都应该有<strong>反馈</strong>。<br>只有不停的集成才是持续集成。越少持续，每次反馈<strong>代价越大</strong>。<br><strong>多次集成</strong>产生一次交付。</p>
</blockquote>
<hr>
<ul>
<li>CI/CD 是无法提升你的代码质量的，是无法解决你代码中的Bug的，但能够提升效率和质量的原因是: 他能把问题发现在前面，让小问题提前暴露出来。</li>
<li>我们说做持续集成最重要的是<strong>有效反馈</strong>和<strong>持续</strong>，因为CI就像体检服务一样，好比有个胖子要减肥，体检服务不能让他吃的更少动的更多，但他如果每天都称一下体重，就能随时知道自己身体的状态，随时知道每天该干什么， 这就是持续的重要性。</li>
<li>如果他不做这个事儿，很可能等到我年度体检的时候才发现，TMD脂肪肝又加重了。。 同理如果每次代码提交都能自动和其他代码集成，和测试环境集成，就不会出现最终发布的时候出现各种各样的问题，也就是刚才说的运维总在兜底的问题。</li>
<li>CI过程的<strong>有效反馈</strong>也很重要，每次集成都应该给出准确的问题定位和建议，谁的代码merge出现冲突，谁提交的commit导致UT失败，谁应该立刻去解决什么样的问题，这都是有效的反馈。就好比胖子中午没吃饭，去称一下体重，体重秤告诉他：还凑合。那这个反馈让他晚饭是吃。。还是不吃呢？。。这就是无效反馈。</li>
</ul>
<p>简单来说，持续交付的pipeline就像下面的管道图一样：</p>
<p><img src="/assets/blogImg/continuous_pipeline.png" alt="continuous_pipeline"></p>
<blockquote>
<p>当然这个图里的每个节点（stage）的定义并不适用于所有应用，每个stage 是不同角色，运行需要耗费不同的成本，那么只要保证每个 Stage 是一个独立有效的反馈就是正确的持续交付pipeline。</p>
</blockquote>
<p>那么，构建出能够运行这样pipeline的一个环境，都需要什么东西：</p>
<p><img src="/assets/blogImg/continuous_process.png" alt="continuous_process"></p>
<ul>
<li>如上图， 你需要有代码托管服务（存储），运行CI中的单元测试，编译打包服务（环境）， 如果你的应用已经托管在公共云上，还要涉及到网络问题。也就是你核心要解决的除了需要服务本身，关键是解决“存储，环境和网络”这三个问题。</li>
</ul>
<hr>
<p>现在，当你辛辛苦苦做好了这些过程之后，仍然会遇到一些问题：</p>
<ul>
<li>每次build，是需要不同的build环境的</li>
</ul>
<p><strong>编译环境维护困难</strong></p>
<ul>
<li>每次集成 Test，是需要依赖其他环境，被依赖的环境不受提交者的控制</li>
</ul>
<p><strong>依赖环境维护困难</strong></p>
<ul>
<li>每个package， 在不同的环境，run的结果是不一样的</li>
</ul>
<p><strong>切换环境调试困难</strong></p>
<ul>
<li>每个package，是无法回溯的</li>
</ul>
<p><strong>运行包的版本维护困难</strong></p>
<ul>
<li>每个环境，是不同的维护者（开发环境，测试环境，生产/产品环境）</li>
</ul>
<p><strong>统一环境标准困难</strong></p>
<ul>
<li>每个环境，除了维护者，是无法清楚知道环境的搭建过程的</li>
</ul>
<p><strong>环境回溯，更是难上加难</strong></p>
<hr>
<p>Why ？ 为什么会遇到这样那样的问题？ 为什么开发人员经常抱怨： “明明我的程序在测试环境已经调试好了，为什么一上生产环境就运行不了？”</p>
<blockquote>
<p>归根结底的原因是：</p>
<blockquote>
<p><strong>开发人员交付的只是软件代码本身， 而运维人员需要维护的是一整套运行环境，以及运行环境之间的依赖关系。</strong></p>
</blockquote>
</blockquote>
<p><img src="/assets/blogImg/continuous_confusion.png" alt="continuous_confusion"></p>
<h3 id="变革软件交付方式的技术：Docker"><a href="#变革软件交付方式的技术：Docker" class="headerlink" title="变革软件交付方式的技术：Docker"></a>变革软件交付方式的技术：Docker</h3><ul>
<li>有人说：“交付方式的变革，改变了全球的经济格局”</li>
</ul>
<p><img src="/assets/blogImg/continuous_box.png" alt="continuous_box"></p>
<ul>
<li>那么，在软件开发领域，Docker ( An open platform for distributed applications for developers and sysadmins) , 就是变革软件交付方式的技术。 </li>
</ul>
<p><img src="/assets/blogImg/continuous_ship.gif" alt="continuous_ship"></p>
<hr>
<p>回到最初的问题， 我们找到了开发和运维之间问题的关键，找到了写代码和维护生产环境之间的核心差别， 那么我们试想一下。</p>
<blockquote>
<p>如果我们能像描述代码依赖关系一样，描述代码运行所需的环境依赖呢？ 如果又能像描述应用之间的依赖关系一样，描述环境之间的依赖呢？</p>
</blockquote>
<ul>
<li>假定，我们的代码中有一个文件，定义了运行需要的环境依赖栈（就像pom.xml文件中定义了java应用的jar包依赖一样）</li>
<li>构建时，我们能根据整个文件，将所有软件依赖栈安装到一个镜像中，镜像是只读的。任何变更都会新产生一个新的镜像而不会更改原先的镜像。</li>
<li>并且只要这个镜像不变，镜像起来的容器之内的环境也不变。</li>
<li>那我们是不是可以像把代码，依赖，测试脚本，环境依赖，环境描述等等这些东西装到集装箱中一样， 集装箱作为一个整体来传递， 作为一个整体在不同的平台上运行， 集装箱不变，任何平台上运行的结果都不变。 YY思路如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_mind.png" alt="continuous_mind"></p>
<hr>
<p><strong>如果我们能轻松的交付整个软件依赖栈，是不是刚才说到的在不同环境调试的问题就能大大减少或者不复存在了?</strong></p>
<p>这个YY过程正好被Docker技术所覆盖， 我们看一下Docker提供什么样的能力，能满足刚才的YY：</p>
<ol>
<li><p>描述环境的能力</p>
<p> 提供了描述运行栈，并且自定义Build 过程的能力。Code中的描述文件就 Dockerfile。     </p>
</li>
<li>分层文件系统<br> Image可以像Git一样进行管理，并且每一层都是只读的，对环境的每个操作都会被记录，并且可回溯。</li>
<li>Docker Registry<br> 提供了管理Image存储系统，可以存储，传递，并且对Image进行版本管理。</li>
<li>屏蔽Host OS 差异<br> 解决了环境差异，保证在任何环境下的运行都是一致的（只要满足运行docker的linux 内核）。</li>
</ol>
<p>这几种能力天然的帮助我们解决环境描述和传递的问题，因此docker能够做到<strong>Build Once， Run EveryWhere ！</strong></p>
<ul>
<li>因此，软件的交付方式，变成了最简单的 Build – Ship – Run， 如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_BSR.png" alt="continuous_BSR"></p>
<h3 id="应用Docker化交付的过程实践"><a href="#应用Docker化交付的过程实践" class="headerlink" title="应用Docker化交付的过程实践"></a>应用Docker化交付的过程实践</h3><p>首先先看个例子，用docker做持续交付能带来的好处。我用docker官方网站上的案例： BBC News。</p>
<ul>
<li>简单来说，一个全球新闻中心，内容的变化是最快的， BBC 公司内部的第一个问题是涉及10几种CI环境，26000 Jobs，500Dev人员。</li>
<li>第二个核心问题是，CI任务需要等待，无法并行。</li>
</ul>
<p>经过Docker化改造之后：</p>
<p><img src="/assets/blogImg/continuous_BBC.png" alt="continuous_BBC"></p>
<blockquote>
<p>最明显的改变，开发可以自己定义自己的开发语言，自己所需的build，集成测试环境，以及应用运行所需的依赖环境。</p>
</blockquote>
<hr>
<p><strong>既然效果这么明显， 该怎么做呢？</strong></p>
<p>基本思路如下：</p>
<ul>
<li>安装好Docker环境</li>
<li>Docker 化你的应用运行环境</li>
<li>Docker 化你的应用编译，UT环境</li>
<li>Docker 化你的应用运行的依赖环境</li>
</ul>
<hr>
<h4 id="第一步，如何安装运行一个Docker环境"><a href="#第一步，如何安装运行一个Docker环境" class="headerlink" title="第一步，如何安装运行一个Docker环境"></a>第一步，如何安装运行一个Docker环境</h4><p>官方提供了详细的文档：</p>
<p><a href="https://docs.docker.com/install/" target="_blank" rel="external">docker_install</a></p>
<hr>
<h4 id="第二步，如何将自己的应用运行在Docker容器中"><a href="#第二步，如何将自己的应用运行在Docker容器中" class="headerlink" title="第二步，如何将自己的应用运行在Docker容器中"></a>第二步，如何将自己的应用运行在Docker容器中</h4><p>这句话可以翻译为： 如何将我的应用环境通过Dockerfile描述出来？</p>
<p>假如我的应用是一个Java Web 应用，需要Java运行环境和Tomcat 容器 ，那么大概我的环境所需下面这些东西：</p>
<ul>
<li>某Linux发行版操作系统</li>
<li>基础软件（起码有个能解压缩包的吧）</li>
<li>openjdk 7 &amp;&amp; 配置 Java Home 等环境变量</li>
<li>Tomcat 7 &amp;&amp; 配置 环境变量</li>
<li>应用包 target.war</li>
<li>应用包 启动参数 JVM</li>
<li>Web Server 指定端口 8080</li>
<li>启动tomcat</li>
</ul>
<p>转化为成Dockerfile 的语言大致如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">FROM buildpack-deps:jessie-curl</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y unzip  openjdk-7-jre-headless=“$JAVA_DEBIAN_VERSION”  </div><div class="line"></div><div class="line">ENV LANG C.UTF-8</div><div class="line"></div><div class="line">ENV JAVA_VERSION 7u91</div><div class="line">ENV JAVA_DEBIAN_VERSION 7u91-2.6.3-1~deb8u1</div><div class="line"></div><div class="line">ENV CATALINA_HOME /usr/local/tomcat</div><div class="line">ENV PATH $CATALINA_HOME/bin:$PATH</div><div class="line">RUN mkdir -p &quot;$CATALINA_HOME&quot;</div><div class="line">WORKDIR $CATALINA_HOMEENV TOMCAT_VERSION 7.0.68</div><div class="line">ENV TOMCAT_TGZ_URL  https://xxxx/apache-tomcat-$TOMCAT_VERSION.tar.gz</div><div class="line"></div><div class="line">RUN set -x \</div><div class="line">    &amp;&amp; curl -fSL &quot;$TOMCAT_TGZ_URL&quot; -o tomcat.tar.gz \</div><div class="line">    &amp;&amp; curl -fSL &quot;$TOMCAT_TGZ_URL.asc&quot; -o tomcat.tar.gz.asc \</div><div class="line">    &amp;&amp; gpg --batch --verify tomcat.tar.gz.asc tomcat.tar.gz \</div><div class="line">    &amp;&amp; tar -xvf tomcat.tar.gz --strip-components=1 \</div><div class="line">    &amp;&amp; rm bin/*.bat \</div><div class="line">    &amp;&amp; rm tomcat.tar.gz*</div><div class="line"></div><div class="line">EXPOSE 8080</div><div class="line">CMD [&quot;catalina.sh&quot;, &quot;run&quot;]</div></pre></td></tr></table></figure>
<ul>
<li>可以看出 ，Dockerfile 第一步永远是From 某个镜像， 开始安装了一些基础包（这里是Jre7）， 又设置了java的环境变量， 之后安装tomat（这里是7.0），再声明启动8080端口，最后运行tomcat的启动脚本结束，在最后结束之前将我的Web 应用.war包COPY或者ADD进去即可。</li>
</ul>
<hr>
<p>我们再看一个Nodejs的环境:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:14.04</div><div class="line"></div><div class="line">COPY sources.list /etc/apt/sources.list</div><div class="line">COPY .npmrc /root/.npmrc</div><div class="line"></div><div class="line">RUN apt-get update &amp;&amp; apt-get -y install curl automake tar libtool make wget xz-utils supervisor</div><div class="line"></div><div class="line">ENV NODE_VERSION 0.12.5</div><div class="line">ENV NPM_VERSION 2.11.3</div><div class="line"></div><div class="line">RUN curl -SLO &quot;https://npm.taobao.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz&quot; \</div><div class="line">    &amp;&amp; tar -xzf &quot;node-v$NODE_VERSION-linux-x64.tar.gz&quot; -C /usr/local --strip-components=1 \</div><div class="line">    &amp;&amp; npm install -g npm@&quot;$NPM_VERSION&quot; \</div><div class="line">    &amp;&amp; npm cache clear</div><div class="line"></div><div class="line">RUN rm -rf ~/.node-gyp \</div><div class="line">    &amp;&amp; mkdir ~/.node-gyp \</div><div class="line">    &amp;&amp; tar zxf node-v$NODE_VERSION-linux-x64.tar.gz -C ~/.node-gyp \</div><div class="line">    &amp;&amp; rm &quot;node-v$NODE_VERSION-linux-x64.tar.gz&quot; \</div><div class="line">    &amp;&amp; mv ~/.node-gyp/node-v$NODE_VERSION-linux-x64 ~/.node-gyp/$NODE_VERSION \</div><div class="line">    &amp;&amp; printf &quot;9\n&quot;&gt;~/.node-gyp/$NODE_VERSION/installVersion</div><div class="line"></div><div class="line">CMD [&quot;node&quot;]</div></pre></td></tr></table></figure>
<ul>
<li>关于这个环境，COPY了本地的sources.list和.npmrc 到容器中，是更换了安装源为mirrors.aliyun.com 和 NPM源为npm.taobao.org ， 国内源更快。 其他就是安装了基本的Nodejs 运行环境</li>
</ul>
<p>那么通过这两个例子，我们发现Dockerfile 还是写起来很麻烦的（其实也不麻烦，就是刚刚说的装要装的东西，配置，运行这三步）。 那么，刚刚说到每一个Dockerfile的第一行都是FROM另一个镜像， 那么思考一下：</p>
<ul>
<li>如果有一个安装好Java的环境 ？</li>
<li>如果有一个安装好Java和Tomcat的环境 ？</li>
<li>如果是微服务，对环境只依赖Java/Node基础环境，是不是所有应用都可以共用1个环境?</li>
</ul>
<p>通过这些思考，得到如下寻找docker镜像的过程：</p>
<ul>
<li>寻找java镜像 ，选择镜像版本， 检查 Dockerfile</li>
<li>寻找tomcat镜像，选择 Tomcat &amp; Java 版本， 检查 Dockerfile</li>
<li>测试运行 ： docker run -ti —rm -v /home/app.war:/canhin/webapp/ tomcat:7-jre7</li>
</ul>
<blockquote>
<p>说句题外话，这个思路同样适用于公司内部，因为Dockerfile 明确划分出了开发和运维的边界， 如果公司有统一的运维标准，比如某个操作系统的某个版本， 某种确定的Web Server， 这样开发只需要From 运维提供的镜像来描述自己的应用环境特殊的部分就好了。 如果大家的环境都一样，调试和测试的过程中，只需要把应用代码通过-v 的参数挂载进去运行就好了， 这样世界就变的很简单和清楚了。</p>
</blockquote>
<p>那么当我需要一个Java 7， Tomcat 7的环境的时候， 直接选择一个官方的tomcat 7 - jre7 镜像即可 ， 比如 <a href="https://hub.docker.com/_/tomcat?tab=description" target="_blank" rel="external">https://hub.docker.com/_/tomcat?tab=description</a> 这个。</p>
<hr>
<h4 id="第三步，用Docker描述我的编译环境"><a href="#第三步，用Docker描述我的编译环境" class="headerlink" title="第三步，用Docker描述我的编译环境"></a>第三步，用Docker描述我的编译环境</h4><p>编译/CI环境往往在公司规模越来越大的时候， 变得越来越麻烦， 因为不同语言，不同类型的应用对编译环境的要求都不一样。 就像刚才说到的BBC News的例子，一个大公司几十种编译环境的存在是很正常的。</p>
<blockquote>
<p>那么，编译环境Docker化最大的好处是： 自定义，可扩展，可复制。</p>
</blockquote>
<ul>
<li>试想一下， 假如你的应用编译只需要依赖标准的Jdk 1.7 和 Maven 2， 或者你是python应用编译过程其实只是需要安装依赖， 那么你可以跟很多人共用编译镜像。</li>
<li>但假如你的应用是Nodejs ，编译依赖特定的C库， 或者是C++之类的编译环境一定要和运行环境一致等等，那就需要定制自己的编译环境了。</li>
</ul>
<p>这里我做一个最简单的用于编译java的镜像示例：</p>
<ul>
<li>编译镜像的Dockerfile 示例：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">FROM registry.aliyuncs.com/acs-sample/centos:7</div><div class="line">RUN yum update  yum install -y open-jdk-1.7.0_65-49 </div><div class="line">COPY build.sh /build.sh</div><div class="line">COPY settings.xml /home/apache-maven-2.2.1/conf/</div><div class="line">ENTRYPOINT [“./build.sh&quot;]</div></pre></td></tr></table></figure>
<ul>
<li><p>上述Dockerfile的build.sh示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /ws ;  mvn -e -U clean package -Dmaven.test.skip=true $@</div><div class="line">cp target/*.war docker/ || exit 0</div></pre></td></tr></table></figure>
</li>
<li><p>运行方式示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone git@github.com:dingmingk/myproject.git  ~/myprj ; cd ~/myprj</div><div class="line">docker run --rm -v `pwd`:/ws -v ~/.m2/repo:/buf build_maven:1.0</div></pre></td></tr></table></figure>
</li>
<li><p>解释一下这个过程:</p>
</li>
</ul>
<p>我的编译环境需要CentOS7系统， 安装JDK1.7 ， 然后把maven的setting（这里主要配置指向其他私有nexus和编译脚本拷贝进去。<br>编译脚本也很简单，就是maven编译打包命令，并且把最终生成的war拷贝到一个定义好的docker目录下，这个目录随便定义。<br>最后是运行方式，即把源代码挂载到容器里进行编译，同时可以选择把本地的.m2缓存到镜像内加快编译速度</p>
<hr>
<p>这里提两个小提示，都是经验之谈：</p>
<blockquote>
<p>建议： build app 和 build docker image 建议分开进行， 即先进行应用本身的编译，再将输出物拷贝到镜像内（但脚本语言可以例外） 因为：</p>
</blockquote>
<ul>
<li>镜像分层概念导致源码可能泄露：因为DockerImage 每一层都会保存一个版本， 即便是ADD代码进去，编译后再rm掉，也可以通过获取ADD这一层镜像拿到源码，因为镜像是运行在各个环境中，是不应该包含源代码信息的。</li>
<li>镜像最小化原则：编译环境可能需要和运行环境不一样的东西，比如Maven的配置，Nodejs的一些C库的依赖， 都不需要在运行环境中体现，所以本着镜像应该最小化原则，不需要的东西最好都不要放进去，也应该分开进行这个步骤。</li>
<li>所以，整个过程还是分为build app和build docker image 两个过程，类似下面这个简单流程 </li>
</ul>
<p><img src="/assets/blogImg/continuous_simple.png" alt="continuous_simple"></p>
<hr>
<blockquote>
<p>建议： Dockerfile 不要放到代码根目录下</p>
</blockquote>
<ul>
<li>避免大量文件传给docker deamon ： docker build会先加载Dockerfile同级目录下所有文件进去，如果有不需要ADD/COPY到镜像里的文件不应该放到Dockerfile目录下， 可以试一下把Dockerfile放到系统/根目录下，这时build 十有八九就会让docker deamon挂掉。</li>
</ul>
<hr>
<h4 id="第四步，用Docker描述UT环境"><a href="#第四步，用Docker描述UT环境" class="headerlink" title="第四步，用Docker描述UT环境"></a>第四步，用Docker描述UT环境</h4><p>简单思路： 运行Docker 镜像环境，安装测试所需依赖 ，运行Docker容器，运行测试命令/脚本</p>
<p>用一个travis-ci官方的例子来说明容器测试这件事，先看下面一个ruby的镜像：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:14.04</div><div class="line"></div><div class="line">MAINTAINER carlad &quot;https://github.com/carlad&quot;</div><div class="line"></div><div class="line"># Install packages for building ruby</div><div class="line">RUN apt-get update</div><div class="line">RUN apt-get install -y --force-yes build-essential wget git</div><div class="line">RUN apt-get install -y --force-yes zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev</div><div class="line">RUN apt-get clean</div><div class="line"></div><div class="line">RUN wget -P /root/src http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.2.tar.gz</div><div class="line">RUN cd /root/src; tar xvf ruby-2.2.2.tar.gz</div><div class="line">RUN cd /root/src/ruby-2.2.2; ./configure; make install</div><div class="line"></div><div class="line">RUN gem update --system</div><div class="line">RUN gem install bundler</div><div class="line"></div><div class="line">RUN git clone https://github.com/travis-ci/docker-sinatra /root/sinatra</div><div class="line">RUN cd /root/sinatra; bundle install</div><div class="line"></div><div class="line">EXPOSE 4567</div></pre></td></tr></table></figure>
<ul>
<li>简单来说就是标准的一个ruby镜像，启动4567端口。那么通过这个镜像进行的测试过程如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">sudo: required</div><div class="line"></div><div class="line">language: ruby</div><div class="line"></div><div class="line">services:</div><div class="line">  - docker</div><div class="line"></div><div class="line">before_install:</div><div class="line">  - docker build -t carlad/sinatra .</div><div class="line">  - docker run -d -p 127.0.0.1:80:4567 carlad/sinatra /bin/sh -c &quot;cd /root/sinatra; bundle exec foreman start;&quot;</div><div class="line">  - docker ps -a</div><div class="line">  - docker run carlad/sinatra /bin/sh -c &quot;cd /root/sinatra; bundle exec rake test&quot;</div><div class="line"></div><div class="line">script:</div><div class="line">  - bundle exec rake test</div></pre></td></tr></table></figure>
<ul>
<li><p>这个其实就是大家可以在本地进行的一个过程，在before install部分内可以看到过程是：</p>
<p>  先build出运行环境的镜像</p>
<p>  运行这个镜像，看看服务能否正常启动</p>
<p>  查看容器是否存活（保证容器不是运行一下就挂了退出）</p>
<p>  运行测试</p>
</li>
</ul>
<p>再来看一个python的例子，也很好理解：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">language: python</div><div class="line">python:</div><div class="line">  - 2.7</div><div class="line"></div><div class="line">services:</div><div class="line">  - docker</div><div class="line"></div><div class="line">install:</div><div class="line">  - docker build -t blog .</div><div class="line">  - docker run -d -p 127.0.0.1:80:80 --name blog blog</div><div class="line"></div><div class="line">before_script:</div><div class="line">  - pip install -r requirements.txt</div><div class="line">  - pip install mock</div><div class="line">  - pip install requests</div><div class="line">  - pip install feedparser</div><div class="line"></div><div class="line">script:</div><div class="line">  - docker ps | grep -q blog</div><div class="line">  - python tests.py</div></pre></td></tr></table></figure>
<p>简单来说就是运行容器，安装依赖，运行测试脚本。或者直接通过下面一行命令进行</p>
<p><code>docker run -v mycode:/ws mytestimage:master /bin/sh -c &quot;python3 djanus/manage.py test djanus mobilerpc &quot;</code></p>
<blockquote>
<p>tips: 这里不是说推荐大家用travis-ci ，但travis-ci 制定了一种语法标准， 非常清楚的能够看到整个过程。</p>
</blockquote>
<hr>
<h3 id="用Docker-Compose描述依赖环境"><a href="#用Docker-Compose描述依赖环境" class="headerlink" title="用Docker-Compose描述依赖环境"></a>用Docker-Compose描述依赖环境</h3><p>刚刚说了单独一个容器运行测试的情况， 但实际情况可能是即便是运行测试，也需要依赖proxy，依赖db，依赖redis等。 简单来说一般web应用会需要下面的结构:</p>
<p><img src="/assets/blogImg/continuous_need.png" alt="continuous_need"></p>
<p>这个结构很简单也很常见， 那在传统思想里，要运行UT或者集成测试，需要依赖的组件，都是去搭建。 搭一个mysql，配置mysql ，运行mysql 这种思路。</p>
<ul>
<li><p>但是在docker的思想里，是声明的概念，就是说我需要一个mysql 去存一些数据进行测试， 这个mysql运行在哪里我根本不care 。 同样的思路告诉docker：</p>
<p>  I need 负载均衡（haproxy，Nginx）</p>
<p>  I need 数据库（mysql)</p>
<p>  I need 文件存储(通过-v , ossfs)</p>
<p>  I need 缓存服务(redis,kv-store)</p>
<p>  I need …</p>
</li>
<li><p>这时，用于编排多个Docker Image 的服务，docker-compose 就出现了，官方文档里用三张最简单的图表明了compose是怎么用的：</p>
</li>
</ul>
<p><img src="/assets/blogImg/continuous_compose.png" alt="continuous_conpose"></p>
<ul>
<li>就是说，我运行一次测试， 需要mysql， 那我就启动一个mysql容器就行，通过link 的方式将我的app链接上，配置一个密码即可，至于其他的信息，我根本不需要，或者说不关心。</li>
</ul>
<p>再举一个例子，假设一个php的Wordpress 应用， 除了应用本身还需要一个db ，他的编排文件（docker-compose.yml）如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">web:</div><div class="line">  image: registry.aliyuncs.com/acs-sample/wordpress:4.3</div><div class="line">  ports:</div><div class="line">    - &apos;80&apos;</div><div class="line">  volumes:</div><div class="line">    - &apos;wp_upload:/var/www/html/wp-content/uploads&apos;</div><div class="line">  environment:</div><div class="line">    WORDPRESS_AUTH_KEY: changeme</div><div class="line">    WORDPRESS_SECURE_AUTH_KEY: changeme</div><div class="line">    WORDPRESS_LOGGED_IN_KEY: changeme</div><div class="line">    WORDPRESS_NONCE_KEY: changeme</div><div class="line">    WORDPRESS_AUTH_SALT: changeme</div><div class="line">    WORDPRESS_SECURE_AUTH_SALT: changeme</div><div class="line">    WORDPRESS_LOGGED_IN_SALT: changeme</div><div class="line">    WORDPRESS_NONCE_SALT: changeme</div><div class="line">    WORDPRESS_NONCE_AA: changeme</div><div class="line">  command: run test script </div><div class="line">  links:</div><div class="line">    - &apos;db:mysql&apos;</div><div class="line">  labels:</div><div class="line">    aliyun.logs: /var/log</div><div class="line">    aliyun.probe.url: http://container/license.txt</div><div class="line">    aliyun.probe.initial_delay_seconds: &apos;10&apos;</div><div class="line">    aliyun.routing.port_80: http://wordpress</div><div class="line">    aliyun.scale: &apos;3&apos;</div><div class="line">db:</div><div class="line">  image: registry.aliyuncs.com/acs-sample/mysql:5.7</div><div class="line">  environment:</div><div class="line">    MYSQL_ROOT_PASSWORD: password</div><div class="line">  restart: always</div><div class="line">  labels:</div><div class="line">    aliyun.logs: /var/log/mysql</div></pre></td></tr></table></figure>
<ul>
<li>除了自身的配置，文件挂载之外，声明的mysql 就是官方5.7的版本，只需要设置一个密码即可， 这样直接运行起来无论是提供服务， 还是运行测试， 都非常的方便。</li>
</ul>
<blockquote>
<p>tips1： compose的好处还在于将配置从Dockerfile中提取出来，比如在测试/生产环境所需要的配置差别， 就可以放到compose里，在不同环境运行的时候换不同的compose文件即可，不用重复的编出不同环境用的docker image</p>
</blockquote>
<hr>
<blockquote>
<p>tips2: 上面的wordpress示例里，启动多个应用容器之上，并没有用nginx做代理，因为阿里云容器服务提供了routing，省去了这部分， 如果是在企业内部，当启动三个应用，还是需要在compose里再声明一个nginx 或者 haproxy 在前面做应用代理和负载均衡的。</p>
</blockquote>
<hr>
<h3 id="完整拼接"><a href="#完整拼接" class="headerlink" title="完整拼接"></a>完整拼接</h3><p><img src="/assets/blogImg/continuous_archeitecture.png" alt="continuous_archeitecture"></p>
<p>对一个公司/企业来说，将自身应用docker化，编译服务，测试集群docker化之后， 要跑通整个的过程，达到BBC News 这样的效果， 整个流程就如图中所示：</p>
<ul>
<li>代码，测试脚本，配置，Dockerfile/Compose 等从开发本地push到代码仓库中</li>
<li>代码仓库能够hook 这个信息，通过事件trigger build 服务，通过容器进行app build，运行test， 通过后对应用进行docker image 的build</li>
<li>build 好的docker image push到远程docker registry 用于存储和传递</li>
<li>当build test 都pass之后， 通过deploy service 告诉应用集群进行更新，从docker registry 上pull 下来新的image进行应用更新，或更新集群配置</li>
</ul>
<h3 id="用docker-为开发-运维人员带来的好处"><a href="#用docker-为开发-运维人员带来的好处" class="headerlink" title="用docker 为开发/运维人员带来的好处"></a>用docker 为开发/运维人员带来的好处</h3><p>Docker技术是 DevOps 的最好诠释， DevOps不是开发去做运维的事情， 而是:</p>
<ul>
<li>将编程的思想应用到运维领域</li>
</ul>
<p>举例来说： Immutable，Copy on Write 这些思想在研发领域是耳熟能详的，好处大家秒懂。而在运维领域的Immutable，传统是怎么做的？ 靠组织架构，权限管理。各种人为订制的机制，规范。 而docker 是用技术来解决了这个问题， 官方文档的介绍docker是 An open platform for distributed applications for developers and sysadmins， 很明显看到了DevOps有木有？</p>
<ul>
<li>由于应用的软件依赖栈完全由应用自己在Dockerfile中定义和维护 ，因此开发人员能够更清楚，更灵活的掌控自己的软件运行环境。 运维人员也不用为应用软件依赖栈的变更碎片化自己的时间。</li>
<li>最最重要的一点，Dockerfile的存在，非常清晰地将研发和PE的责任和界限划分清楚了。 开发人员可以FROM 运维人员提供的基础镜像，配置自己应用的依赖栈； 运维人员可以FROM 更底层的系统工程师的基础镜像， 配置环境依赖栈； 系统工程师则定义了一个公司的基础Linux系统所需的版本和配置。</li>
</ul>
<p>另外，从资源的角度上讲， docker化能够大大减少开发/测试环境的成本，测试或者调试的场景是当发起测试的时候才需要， 其他时候测试环境并不承担业务， 如果用虚拟机则白白的在那里空跑。 Docker 化之后可以在需要的时候随时拉起来整个环境，很快，并且不会出错， 因此阿里云持续交付平台CRP在会在将来提供集成测试环境，作为一项基础服务， 如果没有容器化，那提供整个服务的成本和可行性都是无法想象的</p>
<ul>
<li>片尾：希望大家都能运用docker技术做到被说了很久但无法落地的：DevOps</li>
</ul>
<p><img src="/assets/blogImg/continuous_devops.png" alt="continuous_devops"></p>
<hr>
<h3 id="花絮"><a href="#花絮" class="headerlink" title="花絮"></a>花絮</h3><ul>
<li>不是都片尾了怎么还有花絮？ 因为我感觉刚才通篇说的好像把docker神话了， 为了防止大家出现过度崇拜和追捧的情况，还是要回过头来考虑一下， 到底什么样的应用适合Docker化，换句话说到底什么样的应用适合容器化？</li>
</ul>
<p><img src="/assets/blogImg/continuous_dockerize.png" alt="continuous_dockerize"></p>
<ul>
<li>如上图： 我们认为Web应用，微服务，这种即无状态（是指好比一个web应用，通过10台服务器提供服务，当挂掉1台的时候流量自动被其他九台分摊，不会影响到用户，这样就叫无状态应用），又生命周期很短的业务适合docker化， 反过来，每个应用都是有状态，有存储的这种情况，不太容易docker化，或者说docker化的好处不明显 。</li>
<li>但我们认为的也不一定是对的，今天docker技术，容器技术发展的速度太快， 所以花絮里这个问题 Let’s Think out together ……</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/04/continuous/" class="archive-article-date">
  	<time datetime="2019-03-03T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-04</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Continuous/">Continuous</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_hosts" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/08/31/k8s_hosts/">修改 Kubernetes 集群中容器的 hosts</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近工作中遇到一个问题，就是某个系统需要访问一个第三发系统，在没有私有DNS的情况下，只能通过修改hosts文件解析域名。</p>
<p>假设需要添加的解析为 <code>1.2.3.4 thirdparty.com</code></p>
<p>而我们知道Docker的hosts文件是容器启动后动态加载的，所以无法在Dockerfile中设置。</p>
<p>而如果是使用<code>docker run</code>命令启动容器，可以使用｀–add-host thirdparty.com:1.2.3.4｀参数修改hosts；</p>
<p>如果是<code>docker compose</code>，则可以使用<code>extra_hosts: &quot;thirdparty.com:1.2.3.4&quot;</code>。</p>
<p>但Kubernetes却没有提供类似的方法修改hosts…</p>
<p>刚开始尝试使用 Services，并手动创建EndPoints指向外部服务的地址。但这种方式实际提供的是一种服务，对于只是域名解析来说不合适。</p>
<p>还有一种最脏的办法是每次容器创建成功后通过脚本之行<code>kubectl exec</code>添加hosts，太脏了…</p>
<p>最后一种办法是我发现的最适合也相对干净的一种办法了，需要用到Kubernetes <code>ConfigMap</code>。</p>
<h3 id="创建一个ConfigMap"><a href="#创建一个ConfigMap" class="headerlink" title="创建一个ConfigMap"></a>创建一个ConfigMap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: thirdparty-hosts</div><div class="line">data:</div><div class="line">  hosts: |</div><div class="line">    thirdparty.com  1.2.3.4</div><div class="line">    baidu.com  2.3.4.5</div><div class="line">    google.com 3.4.5.6</div></pre></td></tr></table></figure>
<h3 id="集成start-sh脚本到镜像里"><a href="#集成start-sh脚本到镜像里" class="headerlink" title="集成start.sh脚本到镜像里"></a>集成start.sh脚本到镜像里</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#!/bin/sh</div><div class="line"></div><div class="line">cat /mnt/hosts.append/hosts &gt;&gt; /etc/hosts</div><div class="line">exec your-app args</div></pre></td></tr></table></figure>
<h3 id="将ConfigMap以Volume的方式挂载并执行启动脚本"><a href="#将ConfigMap以Volume的方式挂载并执行启动脚本" class="headerlink" title="将ConfigMap以Volume的方式挂载并执行启动脚本"></a>将ConfigMap以Volume的方式挂载并执行启动脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">spec:</div><div class="line">  template: </div><div class="line">    spec:</div><div class="line">      volumes:</div><div class="line">      - name: hosts-volume</div><div class="line">        configMap:</div><div class="line">          name: thirdparty-hosts</div><div class="line">      containers:</div><div class="line">        command:</div><div class="line">        - ./start.sh</div><div class="line">        volumeMounts:</div><div class="line">        - name: hosts-volume</div><div class="line">          mountPath: /mnt/hosts.append</div></pre></td></tr></table></figure>
<p>这种方式虽然看起来也有点麻烦，但通过维护一个ConfigMap集中管理所有hosts还不错。</p>
<p>另外据说 V1.4 版本中添加了一个新特性能更好的解决这个问题，<code>&quot;external name&quot; (CNAME) services</code>，还没升级暂时不评价。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/08/31/k8s_hosts/" class="archive-article-date">
  	<time datetime="2016-08-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-08-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_context" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/01/k8s_context/">Kubernetes 不同工作组共享集群案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在一个组织内部，不同的工作组可以在同一个 Kubernetes 集群中工作，Kubernetes 通过命名空间和 Context 的设置来实现对不同工作组进行区分，使得它们既可以共享同一个 Kubernetes 集群的服务，也能够互不干扰。</p>
<p>假设在我们的组织中有两个工作组：开发组和生产运维组。开发组在 Kubernetes 集群中需要不断创建、修改、删除各种 Pod、RC、Service 等资源对象，以便实现敏捷开发的过程。而生产运维组则需要使用严格的权限设置来确保生产系统中的 Pod、RC、Service 处于正常运行状态且不会被误操作。</p>
<h2 id="创建-namespace"><a href="#创建-namespace" class="headerlink" title="创建 namespace"></a>创建 namespace</h2><p>为了在 Kubernetes 集群中实现这两个分组，首先需要创建两个命名空间。</p>
<p><strong>namespace-development.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: development</div></pre></td></tr></table></figure>
<p><strong>namespace-production.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: production</div></pre></td></tr></table></figure>
<p>使用 kubectl create 命令完成命名空间的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f namespace-development.yaml</div><div class="line">$ kubectl create -f namespace-production.yaml</div></pre></td></tr></table></figure>
<p>查看系统中的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get namespaces</div></pre></td></tr></table></figure>
<h2 id="定义-Context（运行环境）"><a href="#定义-Context（运行环境）" class="headerlink" title="定义 Context（运行环境）"></a>定义 Context（运行环境）</h2><p>接下来，需要为这两个工作组分别定义一个 Context，即运行环境。这个运行环境将属于某个特定的命名空间。</p>
<p>通过 kubectl config set-context 命令定义 Context，并将 Context 置于之前创建的命名空间中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl config set-cluster kubernetes-cluster --server=https://192.168.1.128:8080</div><div class="line">$ kubectl config set-context ctx-dev --namespace=development --cluster=kubernetes-cluster --user=dev</div><div class="line">$ kubectl config set-context ctx-prod --namespace=production --cluster=kubernetes-cluster --user=prod</div></pre></td></tr></table></figure>
<p>使用 kubectl config view 命令查看已定义的 Context：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">$ kubectl config view</div><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    server: http://192.168.1.128:8080</div><div class="line">  name: kubernetes-cluster</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: kubernetes-cluster</div><div class="line">    namespace: development</div><div class="line">  name: ctx-dev</div><div class="line">- context:</div><div class="line">    cluster: kubernetes-cluster</div><div class="line">    namespace: production</div><div class="line">  name: ctx-prod</div><div class="line">current-context: ctx-dev</div><div class="line">preferences: &#123;&#125;</div><div class="line">users: []</div></pre></td></tr></table></figure>
<p>注意，通过 kubectl config 命令在 ${HOME}/.kube 目录下生成了一个名为 config 的文件，文件内容即 kubectl config view 命令看到的内容。所以，也可以通过手工编辑该文件的方式来设置 Context。</p>
<h2 id="设置工作组在特定-Context-环境中工作"><a href="#设置工作组在特定-Context-环境中工作" class="headerlink" title="设置工作组在特定 Context 环境中工作"></a>设置工作组在特定 Context 环境中工作</h2><p>使用 kubectl config use-context <context_name> 命令来设置当前的运行环境。</context_name></p>
<p>下面的命令把当前运行环境设置为 ”ctx-dev“：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl config use-context ctx-dev</div></pre></td></tr></table></figure>
<p>通过这个命令，当前的运行环境即被设置为开发组所需的环境。之后的所有操作都将在名为 “development” 的命名空间中完成。</p>
<p>各工作组之间的工作将不会相互干扰，并且它们都能够在同一个 Kubernetes 集群中同时工作。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/04/01/k8s_context/" class="archive-article-date">
  	<time datetime="2016-03-31T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-04-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_security_demo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/01/k8s_security_demo/">Kubernetes 集群安全配置案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kubernetes 系统提供了三种认证方式：CA 认证、Token 认证 和 Base 认证。安全功能是一把双刃剑，它保护系统不被攻击，但是也带来额外的性能损耗。集群内的各组件访问 API Server 时，由于它们与 API Server 同时处于同一局域网内，所以建议用非安全的方式访问 API Server 效率更高。</p>
<p>接下来对集群的双向认证配置和简单认证配置过程进行详细说明。</p>
<h2 id="双向认证配置"><a href="#双向认证配置" class="headerlink" title="双向认证配置"></a>双向认证配置</h2><p>双向认证方式是最为严格和安全的集群安全配置方式，主要配置流程如下：</p>
<ol>
<li>生成根证书、API Server 服务端证书、服务端私钥、各个组件所用的客户端证书和客户端私钥。</li>
<li>修改 Kubernetes 各个服务进程的启动参数，启用双向认证模式。</li>
</ol>
<p>详细的配置操作流程如下：</p>
<h4 id="生成根证书"><a href="#生成根证书" class="headerlink" title="生成根证书"></a>生成根证书</h4><p>用 openssl 工具生成 CA 证书，请注意将其中 subject 等参数改为用户所需的数据，CN 的值通常是域名、主机名或 IP 地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cd /var/run/kubernetes</div><div class="line">$ openssl genrsa -out dd_ca.key 2048</div><div class="line">$ openssl req -x509 -new -nodes -key dd_ca.key -subj &quot;/CN=YOUDOMAIN.COM&quot; -days 5000 -out dd_ca.crt</div></pre></td></tr></table></figure>
<h4 id="生成-API-Server-服务端证书和私钥"><a href="#生成-API-Server-服务端证书和私钥" class="headerlink" title="生成 API Server 服务端证书和私钥"></a>生成 API Server 服务端证书和私钥</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_server.key 2048</div><div class="line">$ HN=`hostname`</div><div class="line">$ openssl req -new -key dd_server.key -subj &quot;/CN=$HN&quot; -out dd_server.csr</div><div class="line">$ openssl x509 -req -in dd_server.csr -CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial-out dd_server.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="生成-Controller-Manager-与-Scheduler-进程共用的证书和私钥"><a href="#生成-Controller-Manager-与-Scheduler-进程共用的证书和私钥" class="headerlink" title="生成 Controller Manager 与 Scheduler 进程共用的证书和私钥"></a>生成 Controller Manager 与 Scheduler 进程共用的证书和私钥</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_cs_client.key 2048</div><div class="line">$ openssl req -new -key dd_cs_client.key -subj &quot;/CN=$HN&quot; -out dd_cs_client.csr</div><div class="line">$ openssl x509 -req -in dd_cs_client.csr －CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial -out dd_cs_client.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="生成-Kubelet-所用的客户端证书和私钥"><a href="#生成-Kubelet-所用的客户端证书和私钥" class="headerlink" title="生成 Kubelet 所用的客户端证书和私钥"></a>生成 Kubelet 所用的客户端证书和私钥</h4><p>注意，这里假设 Kubelet 所在机器的 IP 地址为 192.168.1.129。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_kubelet_client.key 2048</div><div class="line">$ openssl req -new -key dd_kubelet_client.key -subj &quot;/CN=192.168.1.129&quot; -out dd_kubelet_client.csr</div><div class="line">$ openssl x509 -req -in dd_kubelet_client.csr -CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial -out dd_kubelet_client.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的启动参数"><a href="#修改-API-Server-的启动参数" class="headerlink" title="修改 API Server 的启动参数"></a>修改 API Server 的启动参数</h4><p>增加 CA 根证书、Server 自身证书等参数并设置安全端口为 443.</p>
<p>修改/etc/kubernetes/apiserver 配置文件的 KUBE_API_ARGS 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_API_ARGS=&quot;--log-dir=/var/log/kubernetes --secure-port=443 --client_ca_file=/var/run/kubernetes/dd_ca.crt --tls-private-key-file=/var/run/kubernetes/dd_server.key --tls-cert-file=/var/run/kubernetes/dd_server.crt&quot;</div></pre></td></tr></table></figure>
<p>重启 kube-apiserver 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="验证-API-Server-的-HTTPS-服务。"><a href="#验证-API-Server-的-HTTPS-服务。" class="headerlink" title="验证 API Server 的 HTTPS 服务。"></a>验证 API Server 的 HTTPS 服务。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubernetes-master:443/api/v1/nodes --cert /var/run/kubernetes/dd_cs_client.crt --key /var/run/kubernetes/dd_cs_client.key --cacert /var/run/kubernetes/dd_ca.crt</div></pre></td></tr></table></figure>
<h4 id="修改-Controller-Manager-的启动参数"><a href="#修改-Controller-Manager-的启动参数" class="headerlink" title="修改 Controller Manager 的启动参数"></a>修改 Controller Manager 的启动参数</h4><p>修改/etc/kubernetes/controller-manager 配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--log-dir=/var/log/kubernetes --service_account_private_key_file=/var/run/kubernetes/server.key --root-ca-file=/var/run/kubernetes/ca.crt --master=https://kubernetes-master:443 --kubeconfig=/etc/kubernetes/cmkubeconfig&quot;</div></pre></td></tr></table></figure>
<p>创建/etc/kubernetes/cmkubeconfig 文件，配置证书等相关参数，具体内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users</div><div class="line">- name: controllermanager</div><div class="line">  user:</div><div class="line">    client-certificate: /var/run/kubernetes/dd_cs_client.crt</div><div class="line">    client-key: /var/run/kubernetes/dd_cs_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /var/run/kubernetes/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: controllermanager</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>重启 kube-controller-manager 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-controller-manager</div></pre></td></tr></table></figure>
<h4 id="配置各个节点上的-Kubelet-进程"><a href="#配置各个节点上的-Kubelet-进程" class="headerlink" title="配置各个节点上的 Kubelet 进程"></a>配置各个节点上的 Kubelet 进程</h4><p>复制 Kubelet 的证书、私钥 与 CA 根证书到所有 Node 上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scp /var/run/kubernetes/dd_kubelet* root@kubernetes-minion1:/home</div><div class="line">$ scp /var/run/kubernetes/dd_ca.* root@kubernetes-minion:/home</div></pre></td></tr></table></figure>
<p>在每个 Node 上创建/var/lib/kubelet/kubeconfig 文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users:</div><div class="line">- name: kubelet</div><div class="line">  user:</div><div class="line">    client-certificats: /home/dd_kubelet_client.crt</div><div class="line">    client-key: /home/dd_kubelet_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /home/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: kubelet</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>修改 Kubelet 的启动参数，以修改/etc/kubernetes/kubelet 配置文件为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">KUBELET_API_SERVER=&quot;--api_servers=https://kubernetes-master:443&quot;</div><div class="line">KUBELET_ARGS=&quot;--pod_infro_container_image=192.168.1.128:1180/google_containers/pause:latest --cluster_dns=10.2.0.100 --cluster_domain=cluster.local --kubeconfig=/var/lib/kubelet/kubeconfig&quot;</div></pre></td></tr></table></figure>
<p>重启 kubelet 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kubelet</div></pre></td></tr></table></figure>
<h4 id="配置-kube-proxy"><a href="#配置-kube-proxy" class="headerlink" title="配置 kube-proxy"></a>配置 kube-proxy</h4><p>首先，创建/var/lib/kubeproxy/proxykubeconfig 文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users:</div><div class="line">- name: kubeproxy</div><div class="line">  user:</div><div class="line">    client-certificate: /home/dd_kubelet_client.crt</div><div class="line">    client-key: /home/dd_kubelet_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /home/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: kubeproxy</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>然后，修改 kube-proxy 的启动参数，引用上述文件并指明 API Server 在安全模式下的访问地址，以修改配置文件/etc/kubenetes/proxy 为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_PROXY_ARGS=&quot;--kubeconfig=/var/lib/kubeproxy/proxykubeconfig --master=https://kubenetes-master:443&quot;</div></pre></td></tr></table></figure>
<p>重启 kube-proxy 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-proxy</div></pre></td></tr></table></figure>
<p>至此，一个双向认证的 Kubernetes 集群环境就搭建完成了。</p>
<h2 id="简单认证配置"><a href="#简单认证配置" class="headerlink" title="简单认证配置"></a>简单认证配置</h2><p>除了双向认证方式，Kubernets 也提供了基于 Token 和 HTTP Base 的简单认证方式。通信方式仍然采用 HTTPS，但不使用数字证书。</p>
<p>采用基于 Token 和 HTTP Base 的简单认证方式时，API Server 对外暴露 HTTPS 端口，客户端提供 Token 或用户名、密码来完成认证过程。这里需要说明的一点是 Kubelet 比较特殊，它同时支持双向认证与简单认证两种模式，其他组件智能配置为双向认证或非安全模式。</p>
<p><strong>API Server 基于 Token 认证的配置过程如下</strong></p>
<h4 id="建立包括用户名、密码和-UID-的文件-token-auth-file："><a href="#建立包括用户名、密码和-UID-的文件-token-auth-file：" class="headerlink" title="建立包括用户名、密码和 UID 的文件 token_auth_file："></a>建立包括用户名、密码和 UID 的文件 token_auth_file：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat /root/token_auth_file</div><div class="line">dingmingk,dingmingk,1</div><div class="line">admin,admin,2</div><div class="line">system,system,3</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的配置，采用上述文件进行安全认证"><a href="#修改-API-Server-的配置，采用上述文件进行安全认证" class="headerlink" title="修改 API Server 的配置，采用上述文件进行安全认证"></a>修改 API Server 的配置，采用上述文件进行安全认证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/kubernetes/apiserver</div><div class="line">KUBE_API_ARGS=&quot;--secure-port=443 --token_auth_file=/root/token_auth_file&quot;</div></pre></td></tr></table></figure>
<h4 id="重启-API-Server-服务"><a href="#重启-API-Server-服务" class="headerlink" title="重启 API Server 服务"></a>重启 API Server 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="用-curl-验证连接-API-Server"><a href="#用-curl-验证连接-API-Server" class="headerlink" title="用 curl 验证连接 API Server"></a>用 curl 验证连接 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubenetes-master:443/version --header &quot;Authorization: Bearer dingmingk&quot; -k</div><div class="line">&#123;</div><div class="line">  &quot;major&quot;: &quot;1&quot;,</div><div class="line">  &quot;minor&quot;: &quot;0&quot;,</div><div class="line">  &quot;gitVersion&quot;: &quot;v1.0.0&quot;,</div><div class="line">  &quot;gitCommit&quot;: &quot;xxxHASHCODE&quot;,</div><div class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>API Server 基于 HTTP Base 认证的配置过程如下</strong></p>
<h4 id="创建包括用户名、密码和-UID-的文件-basic-auth-file："><a href="#创建包括用户名、密码和-UID-的文件-basic-auth-file：" class="headerlink" title="创建包括用户名、密码和 UID 的文件 basic_auth_file："></a>创建包括用户名、密码和 UID 的文件 basic_auth_file：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat /root/basic_auth_file</div><div class="line">dingmingk,dingmingk,1</div><div class="line">admin,admin,2</div><div class="line">system,system,3</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的配置，采用上述文件进行安全认证-1"><a href="#修改-API-Server-的配置，采用上述文件进行安全认证-1" class="headerlink" title="修改 API Server 的配置，采用上述文件进行安全认证"></a>修改 API Server 的配置，采用上述文件进行安全认证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/kubernetes/apiserver</div><div class="line">KUBE_API_ARGS=&quot;--secure-port=443 --basic_auth_file=/root/basic_auth_file&quot;</div></pre></td></tr></table></figure>
<h4 id="重启-API-Server-服务-1"><a href="#重启-API-Server-服务-1" class="headerlink" title="重启 API Server 服务"></a>重启 API Server 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="用-curl-验证连接-API-Server-1"><a href="#用-curl-验证连接-API-Server-1" class="headerlink" title="用 curl 验证连接 API Server"></a>用 curl 验证连接 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubernetes-master:443/version --basic -u dingmingk:dingmingk -k</div><div class="line">&#123;</div><div class="line">  &quot;major&quot;: &quot;1&quot;,</div><div class="line">  &quot;minor&quot;: &quot;0&quot;,</div><div class="line">  &quot;gitVersion&quot;: &quot;v1.0.0&quot;,</div><div class="line">  &quot;gitCommit&quot;: &quot;xxxHASHCODE&quot;,</div><div class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="使用-Kubelet-时则需要指定用户名和密码来访问-API-Server"><a href="#使用-Kubelet-时则需要指定用户名和密码来访问-API-Server" class="headerlink" title="使用 Kubelet 时则需要指定用户名和密码来访问 API Server"></a>使用 Kubelet 时则需要指定用户名和密码来访问 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get nodes --server=&quot;https://kubernetes-master:443&quot; --api-version=&quot;v1&quot; --username=&quot;dingmingk&quot; --password=&quot;dingmingk&quot; --insecure-skip-tls-verify=true</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/04/01/k8s_security_demo/" class="archive-article-date">
  	<time datetime="2016-03-31T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-04-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2019 dingmingk
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cgroup/" style="font-size: 12.5px;">Cgroup</a> <a href="/tags/Continuous/" style="font-size: 10px;">Continuous</a> <a href="/tags/Docker/" style="font-size: 15px;">Docker</a> <a href="/tags/Fleet/" style="font-size: 12.5px;">Fleet</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Jenkins/" style="font-size: 12.5px;">Jenkins</a> <a href="/tags/Kubernetes/" style="font-size: 20px;">Kubernetes</a> <a href="/tags/Life/" style="font-size: 12.5px;">Life</a> <a href="/tags/Linux/" style="font-size: 12.5px;">Linux</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Nginx/" style="font-size: 17.5px;">Nginx</a> <a href="/tags/Pipeline/" style="font-size: 10px;">Pipeline</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/SaltStack/" style="font-size: 10px;">SaltStack</a> <a href="/tags/Systemd/" style="font-size: 15px;">Systemd</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>