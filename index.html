<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <title>他的国</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="他的国">
<meta property="og:url" content="https://dingmingk.github.io/index.html">
<meta property="og:site_name" content="他的国">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="他的国">
  
    <link rel="alternative" href="/atom.xml" title="他的国" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <script src="/style.js"></script>
  

</head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">dingmingk</a></h1>
		</hgroup>

		
		<p class="header-subtitle">做一个安静的美男子</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">友链</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="3" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
		        
					<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">dingmingk</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/assets/blogImg/dingmingk.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">dingmingk</h1>
			</hgroup>
			
			<p class="header-subtitle">做一个安静的美男子</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/dingmingk" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="http://weibo.com/2704974481" title="weibo">weibo</a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/jin-ding-ming" title="zhihu">zhihu</a>
			        
						<a class="mail" target="_blank" href="/dingmingk@gmail.com" title="mail">mail</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-continuous" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/03/04/continuous/">持续交付实践</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ol>
<li>传统交付过程中遇到的问题</li>
<li>变革软件交付方式的技术：Docker</li>
<li>应用 Docker 化交付的过程实践</li>
</ol>
<h3 id="研发过程的困境"><a href="#研发过程的困境" class="headerlink" title="研发过程的困境"></a>研发过程的困境</h3><ul>
<li>任何一家互联网或者软件公司，随着产品规模的扩大，市场需求的变化，都会逐步的发现产品版本管理混乱，运维人员总是在兜底， 不知道开发/测试/集成/预发布/生产等等环境到底经历过几代运维人员之手，所以环境压根没人敢动。</li>
<li>因为市场永远在变化，需求一定在变化，人员也在变化，导致了研发过程中遇到的这样那样的问题。因此，大多数企业都用CI/CD这个解决方案来应对，如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_cicd.png" alt="continuous_cicd"></p>
<ul>
<li>另外，我认为的持续交付概念如下：</li>
</ul>
<blockquote>
<p><strong>在一起</strong>就是集成，每次集成都应该有<strong>反馈</strong>。<br>只有不停的集成才是持续集成。越少持续，每次反馈<strong>代价越大</strong>。<br><strong>多次集成</strong>产生一次交付。</p>
</blockquote>
<hr>
<ul>
<li>CI/CD 是无法提升你的代码质量的，是无法解决你代码中的Bug的，但能够提升效率和质量的原因是: 他能把问题发现在前面，让小问题提前暴露出来。</li>
<li>我们说做持续集成最重要的是<strong>有效反馈</strong>和<strong>持续</strong>，因为CI就像体检服务一样，好比有个胖子要减肥，体检服务不能让他吃的更少动的更多，但他如果每天都称一下体重，就能随时知道自己身体的状态，随时知道每天该干什么， 这就是持续的重要性。</li>
<li>如果他不做这个事儿，很可能等到我年度体检的时候才发现，TMD脂肪肝又加重了。。 同理如果每次代码提交都能自动和其他代码集成，和测试环境集成，就不会出现最终发布的时候出现各种各样的问题，也就是刚才说的运维总在兜底的问题。</li>
<li>CI过程的<strong>有效反馈</strong>也很重要，每次集成都应该给出准确的问题定位和建议，谁的代码merge出现冲突，谁提交的commit导致UT失败，谁应该立刻去解决什么样的问题，这都是有效的反馈。就好比胖子中午没吃饭，去称一下体重，体重秤告诉他：还凑合。那这个反馈让他晚饭是吃。。还是不吃呢？。。这就是无效反馈。</li>
</ul>
<p>简单来说，持续交付的pipeline就像下面的管道图一样：</p>
<p><img src="/assets/blogImg/continuous_pipeline.png" alt="continuous_pipeline"></p>
<blockquote>
<p>当然这个图里的每个节点（stage）的定义并不适用于所有应用，每个stage 是不同角色，运行需要耗费不同的成本，那么只要保证每个 Stage 是一个独立有效的反馈就是正确的持续交付pipeline。</p>
</blockquote>
<p>那么，构建出能够运行这样pipeline的一个环境，都需要什么东西：</p>
<p><img src="/assets/blogImg/continuous_process.png" alt="continuous_process"></p>
<ul>
<li>如上图， 你需要有代码托管服务（存储），运行CI中的单元测试，编译打包服务（环境）， 如果你的应用已经托管在公共云上，还要涉及到网络问题。也就是你核心要解决的除了需要服务本身，关键是解决“存储，环境和网络”这三个问题。</li>
</ul>
<hr>
<p>现在，当你辛辛苦苦做好了这些过程之后，仍然会遇到一些问题：</p>
<ul>
<li>每次build，是需要不同的build环境的</li>
</ul>
<p><strong>编译环境维护困难</strong></p>
<ul>
<li>每次集成 Test，是需要依赖其他环境，被依赖的环境不受提交者的控制</li>
</ul>
<p><strong>依赖环境维护困难</strong></p>
<ul>
<li>每个package， 在不同的环境，run的结果是不一样的</li>
</ul>
<p><strong>切换环境调试困难</strong></p>
<ul>
<li>每个package，是无法回溯的</li>
</ul>
<p><strong>运行包的版本维护困难</strong></p>
<ul>
<li>每个环境，是不同的维护者（开发环境，测试环境，生产/产品环境）</li>
</ul>
<p><strong>统一环境标准困难</strong></p>
<ul>
<li>每个环境，除了维护者，是无法清楚知道环境的搭建过程的</li>
</ul>
<p><strong>环境回溯，更是难上加难</strong></p>
<hr>
<p>Why ？ 为什么会遇到这样那样的问题？ 为什么开发人员经常抱怨： “明明我的程序在测试环境已经调试好了，为什么一上生产环境就运行不了？”</p>
<blockquote>
<p>归根结底的原因是：</p>
<blockquote>
<p><strong>开发人员交付的只是软件代码本身， 而运维人员需要维护的是一整套运行环境，以及运行环境之间的依赖关系。</strong></p>
</blockquote>
</blockquote>
<p><img src="/assets/blogImg/continuous_confusion.png" alt="continuous_confusion"></p>
<h3 id="变革软件交付方式的技术：Docker"><a href="#变革软件交付方式的技术：Docker" class="headerlink" title="变革软件交付方式的技术：Docker"></a>变革软件交付方式的技术：Docker</h3><ul>
<li>有人说：“交付方式的变革，改变了全球的经济格局”</li>
</ul>
<p><img src="/assets/blogImg/continuous_box.png" alt="continuous_box"></p>
<ul>
<li>那么，在软件开发领域，Docker ( An open platform for distributed applications for developers and sysadmins) , 就是变革软件交付方式的技术。 </li>
</ul>
<p><img src="/assets/blogImg/continuous_ship.gif" alt="continuous_ship"></p>
<hr>
<p>回到最初的问题， 我们找到了开发和运维之间问题的关键，找到了写代码和维护生产环境之间的核心差别， 那么我们试想一下。</p>
<blockquote>
<p>如果我们能像描述代码依赖关系一样，描述代码运行所需的环境依赖呢？ 如果又能像描述应用之间的依赖关系一样，描述环境之间的依赖呢？</p>
</blockquote>
<ul>
<li>假定，我们的代码中有一个文件，定义了运行需要的环境依赖栈（就像pom.xml文件中定义了java应用的jar包依赖一样）</li>
<li>构建时，我们能根据整个文件，将所有软件依赖栈安装到一个镜像中，镜像是只读的。任何变更都会新产生一个新的镜像而不会更改原先的镜像。</li>
<li>并且只要这个镜像不变，镜像起来的容器之内的环境也不变。</li>
<li>那我们是不是可以像把代码，依赖，测试脚本，环境依赖，环境描述等等这些东西装到集装箱中一样， 集装箱作为一个整体来传递， 作为一个整体在不同的平台上运行， 集装箱不变，任何平台上运行的结果都不变。 YY思路如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_mind.png" alt="continuous_mind"></p>
<hr>
<p><strong>如果我们能轻松的交付整个软件依赖栈，是不是刚才说到的在不同环境调试的问题就能大大减少或者不复存在了?</strong></p>
<p>这个YY过程正好被Docker技术所覆盖， 我们看一下Docker提供什么样的能力，能满足刚才的YY：</p>
<ol>
<li><p>描述环境的能力</p>
<p> 提供了描述运行栈，并且自定义Build 过程的能力。Code中的描述文件就 Dockerfile。     </p>
</li>
<li>分层文件系统<br> Image可以像Git一样进行管理，并且每一层都是只读的，对环境的每个操作都会被记录，并且可回溯。</li>
<li>Docker Registry<br> 提供了管理Image存储系统，可以存储，传递，并且对Image进行版本管理。</li>
<li>屏蔽Host OS 差异<br> 解决了环境差异，保证在任何环境下的运行都是一致的（只要满足运行docker的linux 内核）。</li>
</ol>
<p>这几种能力天然的帮助我们解决环境描述和传递的问题，因此docker能够做到<strong>Build Once， Run EveryWhere ！</strong></p>
<ul>
<li>因此，软件的交付方式，变成了最简单的 Build – Ship – Run， 如下图：</li>
</ul>
<p><img src="/assets/blogImg/continuous_BSR.png" alt="continuous_BSR"></p>
<h3 id="应用Docker化交付的过程实践"><a href="#应用Docker化交付的过程实践" class="headerlink" title="应用Docker化交付的过程实践"></a>应用Docker化交付的过程实践</h3><p>首先先看个例子，用docker做持续交付能带来的好处。我用docker官方网站上的案例： BBC News。</p>
<ul>
<li>简单来说，一个全球新闻中心，内容的变化是最快的， BBC 公司内部的第一个问题是涉及10几种CI环境，26000 Jobs，500Dev人员。</li>
<li>第二个核心问题是，CI任务需要等待，无法并行。</li>
</ul>
<p>经过Docker化改造之后：</p>
<p><img src="/assets/blogImg/continuous_BBC.png" alt="continuous_BBC"></p>
<blockquote>
<p>最明显的改变，开发可以自己定义自己的开发语言，自己所需的build，集成测试环境，以及应用运行所需的依赖环境。</p>
</blockquote>
<hr>
<p><strong>既然效果这么明显， 该怎么做呢？</strong></p>
<p>基本思路如下：</p>
<ul>
<li>安装好Docker环境</li>
<li>Docker 化你的应用运行环境</li>
<li>Docker 化你的应用编译，UT环境</li>
<li>Docker 化你的应用运行的依赖环境</li>
</ul>
<hr>
<h4 id="第一步，如何安装运行一个Docker环境"><a href="#第一步，如何安装运行一个Docker环境" class="headerlink" title="第一步，如何安装运行一个Docker环境"></a>第一步，如何安装运行一个Docker环境</h4><p>官方提供了详细的文档：</p>
<p><a href="https://docs.docker.com/install/" target="_blank" rel="external">docker_install</a></p>
<hr>
<h4 id="第二步，如何将自己的应用运行在Docker容器中"><a href="#第二步，如何将自己的应用运行在Docker容器中" class="headerlink" title="第二步，如何将自己的应用运行在Docker容器中"></a>第二步，如何将自己的应用运行在Docker容器中</h4><p>这句话可以翻译为： 如何将我的应用环境通过Dockerfile描述出来？</p>
<p>假如我的应用是一个Java Web 应用，需要Java运行环境和Tomcat 容器 ，那么大概我的环境所需下面这些东西：</p>
<ul>
<li>某Linux发行版操作系统</li>
<li>基础软件（起码有个能解压缩包的吧）</li>
<li>openjdk 7 &amp;&amp; 配置 Java Home 等环境变量</li>
<li>Tomcat 7 &amp;&amp; 配置 环境变量</li>
<li>应用包 target.war</li>
<li>应用包 启动参数 JVM</li>
<li>Web Server 指定端口 8080</li>
<li>启动tomcat</li>
</ul>
<p>转化为成Dockerfile 的语言大致如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">FROM buildpack-deps:jessie-curl</div><div class="line">RUN apt-get update &amp;&amp; apt-get install -y unzip  openjdk-7-jre-headless=“$JAVA_DEBIAN_VERSION”  </div><div class="line"></div><div class="line">ENV LANG C.UTF-8</div><div class="line"></div><div class="line">ENV JAVA_VERSION 7u91</div><div class="line">ENV JAVA_DEBIAN_VERSION 7u91-2.6.3-1~deb8u1</div><div class="line"></div><div class="line">ENV CATALINA_HOME /usr/local/tomcat</div><div class="line">ENV PATH $CATALINA_HOME/bin:$PATH</div><div class="line">RUN mkdir -p &quot;$CATALINA_HOME&quot;</div><div class="line">WORKDIR $CATALINA_HOMEENV TOMCAT_VERSION 7.0.68</div><div class="line">ENV TOMCAT_TGZ_URL  https://xxxx/apache-tomcat-$TOMCAT_VERSION.tar.gz</div><div class="line"></div><div class="line">RUN set -x \</div><div class="line">    &amp;&amp; curl -fSL &quot;$TOMCAT_TGZ_URL&quot; -o tomcat.tar.gz \</div><div class="line">    &amp;&amp; curl -fSL &quot;$TOMCAT_TGZ_URL.asc&quot; -o tomcat.tar.gz.asc \</div><div class="line">    &amp;&amp; gpg --batch --verify tomcat.tar.gz.asc tomcat.tar.gz \</div><div class="line">    &amp;&amp; tar -xvf tomcat.tar.gz --strip-components=1 \</div><div class="line">    &amp;&amp; rm bin/*.bat \</div><div class="line">    &amp;&amp; rm tomcat.tar.gz*</div><div class="line"></div><div class="line">EXPOSE 8080</div><div class="line">CMD [&quot;catalina.sh&quot;, &quot;run&quot;]</div></pre></td></tr></table></figure>
<ul>
<li>可以看出 ，Dockerfile 第一步永远是From 某个镜像， 开始安装了一些基础包（这里是Jre7）， 又设置了java的环境变量， 之后安装tomat（这里是7.0），再声明启动8080端口，最后运行tomcat的启动脚本结束，在最后结束之前将我的Web 应用.war包COPY或者ADD进去即可。</li>
</ul>
<hr>
<p>我们再看一个Nodejs的环境:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:14.04</div><div class="line"></div><div class="line">COPY sources.list /etc/apt/sources.list</div><div class="line">COPY .npmrc /root/.npmrc</div><div class="line"></div><div class="line">RUN apt-get update &amp;&amp; apt-get -y install curl automake tar libtool make wget xz-utils supervisor</div><div class="line"></div><div class="line">ENV NODE_VERSION 0.12.5</div><div class="line">ENV NPM_VERSION 2.11.3</div><div class="line"></div><div class="line">RUN curl -SLO &quot;https://npm.taobao.org/dist/v$NODE_VERSION/node-v$NODE_VERSION-linux-x64.tar.gz&quot; \</div><div class="line">    &amp;&amp; tar -xzf &quot;node-v$NODE_VERSION-linux-x64.tar.gz&quot; -C /usr/local --strip-components=1 \</div><div class="line">    &amp;&amp; npm install -g npm@&quot;$NPM_VERSION&quot; \</div><div class="line">    &amp;&amp; npm cache clear</div><div class="line"></div><div class="line">RUN rm -rf ~/.node-gyp \</div><div class="line">    &amp;&amp; mkdir ~/.node-gyp \</div><div class="line">    &amp;&amp; tar zxf node-v$NODE_VERSION-linux-x64.tar.gz -C ~/.node-gyp \</div><div class="line">    &amp;&amp; rm &quot;node-v$NODE_VERSION-linux-x64.tar.gz&quot; \</div><div class="line">    &amp;&amp; mv ~/.node-gyp/node-v$NODE_VERSION-linux-x64 ~/.node-gyp/$NODE_VERSION \</div><div class="line">    &amp;&amp; printf &quot;9\n&quot;&gt;~/.node-gyp/$NODE_VERSION/installVersion</div><div class="line"></div><div class="line">CMD [&quot;node&quot;]</div></pre></td></tr></table></figure>
<ul>
<li>关于这个环境，COPY了本地的sources.list和.npmrc 到容器中，是更换了安装源为mirrors.aliyun.com 和 NPM源为npm.taobao.org ， 国内源更快。 其他就是安装了基本的Nodejs 运行环境</li>
</ul>
<p>那么通过这两个例子，我们发现Dockerfile 还是写起来很麻烦的（其实也不麻烦，就是刚刚说的装要装的东西，配置，运行这三步）。 那么，刚刚说到每一个Dockerfile的第一行都是FROM另一个镜像， 那么思考一下：</p>
<ul>
<li>如果有一个安装好Java的环境 ？</li>
<li>如果有一个安装好Java和Tomcat的环境 ？</li>
<li>如果是微服务，对环境只依赖Java/Node基础环境，是不是所有应用都可以共用1个环境?</li>
</ul>
<p>通过这些思考，得到如下寻找docker镜像的过程：</p>
<ul>
<li>寻找java镜像 ，选择镜像版本， 检查 Dockerfile</li>
<li>寻找tomcat镜像，选择 Tomcat &amp; Java 版本， 检查 Dockerfile</li>
<li>测试运行 ： docker run -ti —rm -v /home/app.war:/canhin/webapp/ tomcat:7-jre7</li>
</ul>
<blockquote>
<p>说句题外话，这个思路同样适用于公司内部，因为Dockerfile 明确划分出了开发和运维的边界， 如果公司有统一的运维标准，比如某个操作系统的某个版本， 某种确定的Web Server， 这样开发只需要From 运维提供的镜像来描述自己的应用环境特殊的部分就好了。 如果大家的环境都一样，调试和测试的过程中，只需要把应用代码通过-v 的参数挂载进去运行就好了， 这样世界就变的很简单和清楚了。</p>
</blockquote>
<p>那么当我需要一个Java 7， Tomcat 7的环境的时候， 直接选择一个官方的tomcat 7 - jre7 镜像即可 ， 比如 <a href="https://hub.docker.com/_/tomcat?tab=description" target="_blank" rel="external">https://hub.docker.com/_/tomcat?tab=description</a> 这个。</p>
<hr>
<h4 id="第三步，用Docker描述我的编译环境"><a href="#第三步，用Docker描述我的编译环境" class="headerlink" title="第三步，用Docker描述我的编译环境"></a>第三步，用Docker描述我的编译环境</h4><p>编译/CI环境往往在公司规模越来越大的时候， 变得越来越麻烦， 因为不同语言，不同类型的应用对编译环境的要求都不一样。 就像刚才说到的BBC News的例子，一个大公司几十种编译环境的存在是很正常的。</p>
<blockquote>
<p>那么，编译环境Docker化最大的好处是： 自定义，可扩展，可复制。</p>
</blockquote>
<ul>
<li>试想一下， 假如你的应用编译只需要依赖标准的Jdk 1.7 和 Maven 2， 或者你是python应用编译过程其实只是需要安装依赖， 那么你可以跟很多人共用编译镜像。</li>
<li>但假如你的应用是Nodejs ，编译依赖特定的C库， 或者是C++之类的编译环境一定要和运行环境一致等等，那就需要定制自己的编译环境了。</li>
</ul>
<p>这里我做一个最简单的用于编译java的镜像示例：</p>
<ul>
<li>编译镜像的Dockerfile 示例：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">FROM registry.aliyuncs.com/acs-sample/centos:7</div><div class="line">RUN yum update  yum install -y open-jdk-1.7.0_65-49 </div><div class="line">COPY build.sh /build.sh</div><div class="line">COPY settings.xml /home/apache-maven-2.2.1/conf/</div><div class="line">ENTRYPOINT [“./build.sh&quot;]</div></pre></td></tr></table></figure>
<ul>
<li><p>上述Dockerfile的build.sh示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /ws ;  mvn -e -U clean package -Dmaven.test.skip=true $@</div><div class="line">cp target/*.war docker/ || exit 0</div></pre></td></tr></table></figure>
</li>
<li><p>运行方式示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone git@github.com:dingmingk/myproject.git  ~/myprj ; cd ~/myprj</div><div class="line">docker run --rm -v `pwd`:/ws -v ~/.m2/repo:/buf build_maven:1.0</div></pre></td></tr></table></figure>
</li>
<li><p>解释一下这个过程:</p>
</li>
</ul>
<p>我的编译环境需要CentOS7系统， 安装JDK1.7 ， 然后把maven的setting（这里主要配置指向其他私有nexus和编译脚本拷贝进去。<br>编译脚本也很简单，就是maven编译打包命令，并且把最终生成的war拷贝到一个定义好的docker目录下，这个目录随便定义。<br>最后是运行方式，即把源代码挂载到容器里进行编译，同时可以选择把本地的.m2缓存到镜像内加快编译速度</p>
<hr>
<p>这里提两个小提示，都是经验之谈：</p>
<blockquote>
<p>建议： build app 和 build docker image 建议分开进行， 即先进行应用本身的编译，再将输出物拷贝到镜像内（但脚本语言可以例外） 因为：</p>
</blockquote>
<ul>
<li>镜像分层概念导致源码可能泄露：因为DockerImage 每一层都会保存一个版本， 即便是ADD代码进去，编译后再rm掉，也可以通过获取ADD这一层镜像拿到源码，因为镜像是运行在各个环境中，是不应该包含源代码信息的。</li>
<li>镜像最小化原则：编译环境可能需要和运行环境不一样的东西，比如Maven的配置，Nodejs的一些C库的依赖， 都不需要在运行环境中体现，所以本着镜像应该最小化原则，不需要的东西最好都不要放进去，也应该分开进行这个步骤。</li>
<li>所以，整个过程还是分为build app和build docker image 两个过程，类似下面这个简单流程 </li>
</ul>
<p><img src="/assets/blogImg/continuous_simple.png" alt="continuous_simple"></p>
<hr>
<blockquote>
<p>建议： Dockerfile 不要放到代码根目录下</p>
</blockquote>
<ul>
<li>避免大量文件传给docker deamon ： docker build会先加载Dockerfile同级目录下所有文件进去，如果有不需要ADD/COPY到镜像里的文件不应该放到Dockerfile目录下， 可以试一下把Dockerfile放到系统/根目录下，这时build 十有八九就会让docker deamon挂掉。</li>
</ul>
<hr>
<h4 id="第四步，用Docker描述UT环境"><a href="#第四步，用Docker描述UT环境" class="headerlink" title="第四步，用Docker描述UT环境"></a>第四步，用Docker描述UT环境</h4><p>简单思路： 运行Docker 镜像环境，安装测试所需依赖 ，运行Docker容器，运行测试命令/脚本</p>
<p>用一个travis-ci官方的例子来说明容器测试这件事，先看下面一个ruby的镜像：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">FROM ubuntu:14.04</div><div class="line"></div><div class="line">MAINTAINER carlad &quot;https://github.com/carlad&quot;</div><div class="line"></div><div class="line"># Install packages for building ruby</div><div class="line">RUN apt-get update</div><div class="line">RUN apt-get install -y --force-yes build-essential wget git</div><div class="line">RUN apt-get install -y --force-yes zlib1g-dev libssl-dev libreadline-dev libyaml-dev libxml2-dev libxslt-dev</div><div class="line">RUN apt-get clean</div><div class="line"></div><div class="line">RUN wget -P /root/src http://cache.ruby-lang.org/pub/ruby/2.2/ruby-2.2.2.tar.gz</div><div class="line">RUN cd /root/src; tar xvf ruby-2.2.2.tar.gz</div><div class="line">RUN cd /root/src/ruby-2.2.2; ./configure; make install</div><div class="line"></div><div class="line">RUN gem update --system</div><div class="line">RUN gem install bundler</div><div class="line"></div><div class="line">RUN git clone https://github.com/travis-ci/docker-sinatra /root/sinatra</div><div class="line">RUN cd /root/sinatra; bundle install</div><div class="line"></div><div class="line">EXPOSE 4567</div></pre></td></tr></table></figure>
<ul>
<li>简单来说就是标准的一个ruby镜像，启动4567端口。那么通过这个镜像进行的测试过程如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">sudo: required</div><div class="line"></div><div class="line">language: ruby</div><div class="line"></div><div class="line">services:</div><div class="line">  - docker</div><div class="line"></div><div class="line">before_install:</div><div class="line">  - docker build -t carlad/sinatra .</div><div class="line">  - docker run -d -p 127.0.0.1:80:4567 carlad/sinatra /bin/sh -c &quot;cd /root/sinatra; bundle exec foreman start;&quot;</div><div class="line">  - docker ps -a</div><div class="line">  - docker run carlad/sinatra /bin/sh -c &quot;cd /root/sinatra; bundle exec rake test&quot;</div><div class="line"></div><div class="line">script:</div><div class="line">  - bundle exec rake test</div></pre></td></tr></table></figure>
<ul>
<li><p>这个其实就是大家可以在本地进行的一个过程，在before install部分内可以看到过程是：</p>
<p>  先build出运行环境的镜像</p>
<p>  运行这个镜像，看看服务能否正常启动</p>
<p>  查看容器是否存活（保证容器不是运行一下就挂了退出）</p>
<p>  运行测试</p>
</li>
</ul>
<p>再来看一个python的例子，也很好理解：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">language: python</div><div class="line">python:</div><div class="line">  - 2.7</div><div class="line"></div><div class="line">services:</div><div class="line">  - docker</div><div class="line"></div><div class="line">install:</div><div class="line">  - docker build -t blog .</div><div class="line">  - docker run -d -p 127.0.0.1:80:80 --name blog blog</div><div class="line"></div><div class="line">before_script:</div><div class="line">  - pip install -r requirements.txt</div><div class="line">  - pip install mock</div><div class="line">  - pip install requests</div><div class="line">  - pip install feedparser</div><div class="line"></div><div class="line">script:</div><div class="line">  - docker ps | grep -q blog</div><div class="line">  - python tests.py</div></pre></td></tr></table></figure>
<p>简单来说就是运行容器，安装依赖，运行测试脚本。或者直接通过下面一行命令进行</p>
<p><code>docker run -v mycode:/ws mytestimage:master /bin/sh -c &quot;python3 djanus/manage.py test djanus mobilerpc &quot;</code></p>
<blockquote>
<p>tips: 这里不是说推荐大家用travis-ci ，但travis-ci 制定了一种语法标准， 非常清楚的能够看到整个过程。</p>
</blockquote>
<hr>
<h3 id="用Docker-Compose描述依赖环境"><a href="#用Docker-Compose描述依赖环境" class="headerlink" title="用Docker-Compose描述依赖环境"></a>用Docker-Compose描述依赖环境</h3><p>刚刚说了单独一个容器运行测试的情况， 但实际情况可能是即便是运行测试，也需要依赖proxy，依赖db，依赖redis等。 简单来说一般web应用会需要下面的结构:</p>
<p><img src="/assets/blogImg/continuous_need.png" alt="continuous_need"></p>
<p>这个结构很简单也很常见， 那在传统思想里，要运行UT或者集成测试，需要依赖的组件，都是去搭建。 搭一个mysql，配置mysql ，运行mysql 这种思路。</p>
<ul>
<li><p>但是在docker的思想里，是声明的概念，就是说我需要一个mysql 去存一些数据进行测试， 这个mysql运行在哪里我根本不care 。 同样的思路告诉docker：</p>
<p>  I need 负载均衡（haproxy，Nginx）</p>
<p>  I need 数据库（mysql)</p>
<p>  I need 文件存储(通过-v , ossfs)</p>
<p>  I need 缓存服务(redis,kv-store)</p>
<p>  I need …</p>
</li>
<li><p>这时，用于编排多个Docker Image 的服务，docker-compose 就出现了，官方文档里用三张最简单的图表明了compose是怎么用的：</p>
</li>
</ul>
<p><img src="/assets/blogImg/continuous_compose.png" alt="continuous_conpose"></p>
<ul>
<li>就是说，我运行一次测试， 需要mysql， 那我就启动一个mysql容器就行，通过link 的方式将我的app链接上，配置一个密码即可，至于其他的信息，我根本不需要，或者说不关心。</li>
</ul>
<p>再举一个例子，假设一个php的Wordpress 应用， 除了应用本身还需要一个db ，他的编排文件（docker-compose.yml）如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">web:</div><div class="line">  image: registry.aliyuncs.com/acs-sample/wordpress:4.3</div><div class="line">  ports:</div><div class="line">    - &apos;80&apos;</div><div class="line">  volumes:</div><div class="line">    - &apos;wp_upload:/var/www/html/wp-content/uploads&apos;</div><div class="line">  environment:</div><div class="line">    WORDPRESS_AUTH_KEY: changeme</div><div class="line">    WORDPRESS_SECURE_AUTH_KEY: changeme</div><div class="line">    WORDPRESS_LOGGED_IN_KEY: changeme</div><div class="line">    WORDPRESS_NONCE_KEY: changeme</div><div class="line">    WORDPRESS_AUTH_SALT: changeme</div><div class="line">    WORDPRESS_SECURE_AUTH_SALT: changeme</div><div class="line">    WORDPRESS_LOGGED_IN_SALT: changeme</div><div class="line">    WORDPRESS_NONCE_SALT: changeme</div><div class="line">    WORDPRESS_NONCE_AA: changeme</div><div class="line">  command: run test script </div><div class="line">  links:</div><div class="line">    - &apos;db:mysql&apos;</div><div class="line">  labels:</div><div class="line">    aliyun.logs: /var/log</div><div class="line">    aliyun.probe.url: http://container/license.txt</div><div class="line">    aliyun.probe.initial_delay_seconds: &apos;10&apos;</div><div class="line">    aliyun.routing.port_80: http://wordpress</div><div class="line">    aliyun.scale: &apos;3&apos;</div><div class="line">db:</div><div class="line">  image: registry.aliyuncs.com/acs-sample/mysql:5.7</div><div class="line">  environment:</div><div class="line">    MYSQL_ROOT_PASSWORD: password</div><div class="line">  restart: always</div><div class="line">  labels:</div><div class="line">    aliyun.logs: /var/log/mysql</div></pre></td></tr></table></figure>
<ul>
<li>除了自身的配置，文件挂载之外，声明的mysql 就是官方5.7的版本，只需要设置一个密码即可， 这样直接运行起来无论是提供服务， 还是运行测试， 都非常的方便。</li>
</ul>
<blockquote>
<p>tips1： compose的好处还在于将配置从Dockerfile中提取出来，比如在测试/生产环境所需要的配置差别， 就可以放到compose里，在不同环境运行的时候换不同的compose文件即可，不用重复的编出不同环境用的docker image</p>
</blockquote>
<hr>
<blockquote>
<p>tips2: 上面的wordpress示例里，启动多个应用容器之上，并没有用nginx做代理，因为阿里云容器服务提供了routing，省去了这部分， 如果是在企业内部，当启动三个应用，还是需要在compose里再声明一个nginx 或者 haproxy 在前面做应用代理和负载均衡的。</p>
</blockquote>
<hr>
<h3 id="完整拼接"><a href="#完整拼接" class="headerlink" title="完整拼接"></a>完整拼接</h3><p><img src="/assets/blogImg/continuous_archeitecture.png" alt="continuous_archeitecture"></p>
<p>对一个公司/企业来说，将自身应用docker化，编译服务，测试集群docker化之后， 要跑通整个的过程，达到BBC News 这样的效果， 整个流程就如图中所示：</p>
<ul>
<li>代码，测试脚本，配置，Dockerfile/Compose 等从开发本地push到代码仓库中</li>
<li>代码仓库能够hook 这个信息，通过事件trigger build 服务，通过容器进行app build，运行test， 通过后对应用进行docker image 的build</li>
<li>build 好的docker image push到远程docker registry 用于存储和传递</li>
<li>当build test 都pass之后， 通过deploy service 告诉应用集群进行更新，从docker registry 上pull 下来新的image进行应用更新，或更新集群配置</li>
</ul>
<h3 id="用docker-为开发-运维人员带来的好处"><a href="#用docker-为开发-运维人员带来的好处" class="headerlink" title="用docker 为开发/运维人员带来的好处"></a>用docker 为开发/运维人员带来的好处</h3><p>Docker技术是 DevOps 的最好诠释， DevOps不是开发去做运维的事情， 而是:</p>
<ul>
<li>将编程的思想应用到运维领域</li>
</ul>
<p>举例来说： Immutable，Copy on Write 这些思想在研发领域是耳熟能详的，好处大家秒懂。而在运维领域的Immutable，传统是怎么做的？ 靠组织架构，权限管理。各种人为订制的机制，规范。 而docker 是用技术来解决了这个问题， 官方文档的介绍docker是 An open platform for distributed applications for developers and sysadmins， 很明显看到了DevOps有木有？</p>
<ul>
<li>由于应用的软件依赖栈完全由应用自己在Dockerfile中定义和维护 ，因此开发人员能够更清楚，更灵活的掌控自己的软件运行环境。 运维人员也不用为应用软件依赖栈的变更碎片化自己的时间。</li>
<li>最最重要的一点，Dockerfile的存在，非常清晰地将研发和PE的责任和界限划分清楚了。 开发人员可以FROM 运维人员提供的基础镜像，配置自己应用的依赖栈； 运维人员可以FROM 更底层的系统工程师的基础镜像， 配置环境依赖栈； 系统工程师则定义了一个公司的基础Linux系统所需的版本和配置。</li>
</ul>
<p>另外，从资源的角度上讲， docker化能够大大减少开发/测试环境的成本，测试或者调试的场景是当发起测试的时候才需要， 其他时候测试环境并不承担业务， 如果用虚拟机则白白的在那里空跑。 Docker 化之后可以在需要的时候随时拉起来整个环境，很快，并且不会出错， 因此阿里云持续交付平台CRP在会在将来提供集成测试环境，作为一项基础服务， 如果没有容器化，那提供整个服务的成本和可行性都是无法想象的</p>
<ul>
<li>片尾：希望大家都能运用docker技术做到被说了很久但无法落地的：DevOps</li>
</ul>
<p><img src="/assets/blogImg/continuous_devops.png" alt="continuous_devops"></p>
<hr>
<h3 id="花絮"><a href="#花絮" class="headerlink" title="花絮"></a>花絮</h3><ul>
<li>不是都片尾了怎么还有花絮？ 因为我感觉刚才通篇说的好像把docker神话了， 为了防止大家出现过度崇拜和追捧的情况，还是要回过头来考虑一下， 到底什么样的应用适合Docker化，换句话说到底什么样的应用适合容器化？</li>
</ul>
<p><img src="/assets/blogImg/continuous_dockerize.png" alt="continuous_dockerize"></p>
<ul>
<li>如上图： 我们认为Web应用，微服务，这种即无状态（是指好比一个web应用，通过10台服务器提供服务，当挂掉1台的时候流量自动被其他九台分摊，不会影响到用户，这样就叫无状态应用），又生命周期很短的业务适合docker化， 反过来，每个应用都是有状态，有存储的这种情况，不太容易docker化，或者说docker化的好处不明显 。</li>
<li>但我们认为的也不一定是对的，今天docker技术，容器技术发展的速度太快， 所以花絮里这个问题 Let’s Think out together ……</li>
</ul>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/03/04/continuous/" class="archive-article-date">
  	<time datetime="2019-03-03T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-03-04</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Continuous/">Continuous</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Docker/">Docker</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_hosts" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/08/31/k8s_hosts/">修改 Kubernetes 集群中容器的 hosts</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>最近工作中遇到一个问题，就是某个系统需要访问一个第三发系统，在没有私有DNS的情况下，只能通过修改hosts文件解析域名。</p>
<p>假设需要添加的解析为 <code>1.2.3.4 thirdparty.com</code></p>
<p>而我们知道Docker的hosts文件是容器启动后动态加载的，所以无法在Dockerfile中设置。</p>
<p>而如果是使用<code>docker run</code>命令启动容器，可以使用｀–add-host thirdparty.com:1.2.3.4｀参数修改hosts；</p>
<p>如果是<code>docker compose</code>，则可以使用<code>extra_hosts: &quot;thirdparty.com:1.2.3.4&quot;</code>。</p>
<p>但Kubernetes却没有提供类似的方法修改hosts…</p>
<p>刚开始尝试使用 Services，并手动创建EndPoints指向外部服务的地址。但这种方式实际提供的是一种服务，对于只是域名解析来说不合适。</p>
<p>还有一种最脏的办法是每次容器创建成功后通过脚本之行<code>kubectl exec</code>添加hosts，太脏了…</p>
<p>最后一种办法是我发现的最适合也相对干净的一种办法了，需要用到Kubernetes <code>ConfigMap</code>。</p>
<h3 id="创建一个ConfigMap"><a href="#创建一个ConfigMap" class="headerlink" title="创建一个ConfigMap"></a>创建一个ConfigMap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">apiVersion: v1</div><div class="line">kind: ConfigMap</div><div class="line">metadata:</div><div class="line">  name: thirdparty-hosts</div><div class="line">data:</div><div class="line">  hosts: |</div><div class="line">    thirdparty.com  1.2.3.4</div><div class="line">    baidu.com  2.3.4.5</div><div class="line">    google.com 3.4.5.6</div></pre></td></tr></table></figure>
<h3 id="集成start-sh脚本到镜像里"><a href="#集成start-sh脚本到镜像里" class="headerlink" title="集成start.sh脚本到镜像里"></a>集成start.sh脚本到镜像里</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#!/bin/sh</div><div class="line"></div><div class="line">cat /mnt/hosts.append/hosts &gt;&gt; /etc/hosts</div><div class="line">exec your-app args</div></pre></td></tr></table></figure>
<h3 id="将ConfigMap以Volume的方式挂载并执行启动脚本"><a href="#将ConfigMap以Volume的方式挂载并执行启动脚本" class="headerlink" title="将ConfigMap以Volume的方式挂载并执行启动脚本"></a>将ConfigMap以Volume的方式挂载并执行启动脚本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">apiVersion: extensions/v1beta1</div><div class="line">kind: Deployment</div><div class="line">spec:</div><div class="line">  template: </div><div class="line">    spec:</div><div class="line">      volumes:</div><div class="line">      - name: hosts-volume</div><div class="line">        configMap:</div><div class="line">          name: thirdparty-hosts</div><div class="line">      containers:</div><div class="line">        command:</div><div class="line">        - ./start.sh</div><div class="line">        volumeMounts:</div><div class="line">        - name: hosts-volume</div><div class="line">          mountPath: /mnt/hosts.append</div></pre></td></tr></table></figure>
<p>这种方式虽然看起来也有点麻烦，但通过维护一个ConfigMap集中管理所有hosts还不错。</p>
<p>另外据说 V1.4 版本中添加了一个新特性能更好的解决这个问题，<code>&quot;external name&quot; (CNAME) services</code>，还没升级暂时不评价。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/08/31/k8s_hosts/" class="archive-article-date">
  	<time datetime="2016-08-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-08-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_security_demo" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/01/k8s_security_demo/">Kubernetes 集群安全配置案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kubernetes 系统提供了三种认证方式：CA 认证、Token 认证 和 Base 认证。安全功能是一把双刃剑，它保护系统不被攻击，但是也带来额外的性能损耗。集群内的各组件访问 API Server 时，由于它们与 API Server 同时处于同一局域网内，所以建议用非安全的方式访问 API Server 效率更高。</p>
<p>接下来对集群的双向认证配置和简单认证配置过程进行详细说明。</p>
<h2 id="双向认证配置"><a href="#双向认证配置" class="headerlink" title="双向认证配置"></a>双向认证配置</h2><p>双向认证方式是最为严格和安全的集群安全配置方式，主要配置流程如下：</p>
<ol>
<li>生成根证书、API Server 服务端证书、服务端私钥、各个组件所用的客户端证书和客户端私钥。</li>
<li>修改 Kubernetes 各个服务进程的启动参数，启用双向认证模式。</li>
</ol>
<p>详细的配置操作流程如下：</p>
<h4 id="生成根证书"><a href="#生成根证书" class="headerlink" title="生成根证书"></a>生成根证书</h4><p>用 openssl 工具生成 CA 证书，请注意将其中 subject 等参数改为用户所需的数据，CN 的值通常是域名、主机名或 IP 地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ cd /var/run/kubernetes</div><div class="line">$ openssl genrsa -out dd_ca.key 2048</div><div class="line">$ openssl req -x509 -new -nodes -key dd_ca.key -subj &quot;/CN=YOUDOMAIN.COM&quot; -days 5000 -out dd_ca.crt</div></pre></td></tr></table></figure>
<h4 id="生成-API-Server-服务端证书和私钥"><a href="#生成-API-Server-服务端证书和私钥" class="headerlink" title="生成 API Server 服务端证书和私钥"></a>生成 API Server 服务端证书和私钥</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_server.key 2048</div><div class="line">$ HN=`hostname`</div><div class="line">$ openssl req -new -key dd_server.key -subj &quot;/CN=$HN&quot; -out dd_server.csr</div><div class="line">$ openssl x509 -req -in dd_server.csr -CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial-out dd_server.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="生成-Controller-Manager-与-Scheduler-进程共用的证书和私钥"><a href="#生成-Controller-Manager-与-Scheduler-进程共用的证书和私钥" class="headerlink" title="生成 Controller Manager 与 Scheduler 进程共用的证书和私钥"></a>生成 Controller Manager 与 Scheduler 进程共用的证书和私钥</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_cs_client.key 2048</div><div class="line">$ openssl req -new -key dd_cs_client.key -subj &quot;/CN=$HN&quot; -out dd_cs_client.csr</div><div class="line">$ openssl x509 -req -in dd_cs_client.csr －CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial -out dd_cs_client.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="生成-Kubelet-所用的客户端证书和私钥"><a href="#生成-Kubelet-所用的客户端证书和私钥" class="headerlink" title="生成 Kubelet 所用的客户端证书和私钥"></a>生成 Kubelet 所用的客户端证书和私钥</h4><p>注意，这里假设 Kubelet 所在机器的 IP 地址为 192.168.1.129。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ openssl genrsa -out dd_kubelet_client.key 2048</div><div class="line">$ openssl req -new -key dd_kubelet_client.key -subj &quot;/CN=192.168.1.129&quot; -out dd_kubelet_client.csr</div><div class="line">$ openssl x509 -req -in dd_kubelet_client.csr -CA dd_ca.crt -CAkey dd_ca.key -CAcreateserial -out dd_kubelet_client.crt -days 5000</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的启动参数"><a href="#修改-API-Server-的启动参数" class="headerlink" title="修改 API Server 的启动参数"></a>修改 API Server 的启动参数</h4><p>增加 CA 根证书、Server 自身证书等参数并设置安全端口为 443.</p>
<p>修改/etc/kubernetes/apiserver 配置文件的 KUBE_API_ARGS 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_API_ARGS=&quot;--log-dir=/var/log/kubernetes --secure-port=443 --client_ca_file=/var/run/kubernetes/dd_ca.crt --tls-private-key-file=/var/run/kubernetes/dd_server.key --tls-cert-file=/var/run/kubernetes/dd_server.crt&quot;</div></pre></td></tr></table></figure>
<p>重启 kube-apiserver 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="验证-API-Server-的-HTTPS-服务。"><a href="#验证-API-Server-的-HTTPS-服务。" class="headerlink" title="验证 API Server 的 HTTPS 服务。"></a>验证 API Server 的 HTTPS 服务。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubernetes-master:443/api/v1/nodes --cert /var/run/kubernetes/dd_cs_client.crt --key /var/run/kubernetes/dd_cs_client.key --cacert /var/run/kubernetes/dd_ca.crt</div></pre></td></tr></table></figure>
<h4 id="修改-Controller-Manager-的启动参数"><a href="#修改-Controller-Manager-的启动参数" class="headerlink" title="修改 Controller Manager 的启动参数"></a>修改 Controller Manager 的启动参数</h4><p>修改/etc/kubernetes/controller-manager 配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_CONTROLLER_MANAGER_ARGS=&quot;--log-dir=/var/log/kubernetes --service_account_private_key_file=/var/run/kubernetes/server.key --root-ca-file=/var/run/kubernetes/ca.crt --master=https://kubernetes-master:443 --kubeconfig=/etc/kubernetes/cmkubeconfig&quot;</div></pre></td></tr></table></figure>
<p>创建/etc/kubernetes/cmkubeconfig 文件，配置证书等相关参数，具体内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users</div><div class="line">- name: controllermanager</div><div class="line">  user:</div><div class="line">    client-certificate: /var/run/kubernetes/dd_cs_client.crt</div><div class="line">    client-key: /var/run/kubernetes/dd_cs_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /var/run/kubernetes/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: controllermanager</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>重启 kube-controller-manager 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-controller-manager</div></pre></td></tr></table></figure>
<h4 id="配置各个节点上的-Kubelet-进程"><a href="#配置各个节点上的-Kubelet-进程" class="headerlink" title="配置各个节点上的 Kubelet 进程"></a>配置各个节点上的 Kubelet 进程</h4><p>复制 Kubelet 的证书、私钥 与 CA 根证书到所有 Node 上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ scp /var/run/kubernetes/dd_kubelet* root@kubernetes-minion1:/home</div><div class="line">$ scp /var/run/kubernetes/dd_ca.* root@kubernetes-minion:/home</div></pre></td></tr></table></figure>
<p>在每个 Node 上创建/var/lib/kubelet/kubeconfig 文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users:</div><div class="line">- name: kubelet</div><div class="line">  user:</div><div class="line">    client-certificats: /home/dd_kubelet_client.crt</div><div class="line">    client-key: /home/dd_kubelet_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /home/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: kubelet</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>修改 Kubelet 的启动参数，以修改/etc/kubernetes/kubelet 配置文件为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">KUBELET_API_SERVER=&quot;--api_servers=https://kubernetes-master:443&quot;</div><div class="line">KUBELET_ARGS=&quot;--pod_infro_container_image=192.168.1.128:1180/google_containers/pause:latest --cluster_dns=10.2.0.100 --cluster_domain=cluster.local --kubeconfig=/var/lib/kubelet/kubeconfig&quot;</div></pre></td></tr></table></figure>
<p>重启 kubelet 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kubelet</div></pre></td></tr></table></figure>
<h4 id="配置-kube-proxy"><a href="#配置-kube-proxy" class="headerlink" title="配置 kube-proxy"></a>配置 kube-proxy</h4><p>首先，创建/var/lib/kubeproxy/proxykubeconfig 文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">users:</div><div class="line">- name: kubeproxy</div><div class="line">  user:</div><div class="line">    client-certificate: /home/dd_kubelet_client.crt</div><div class="line">    client-key: /home/dd_kubelet_client.key</div><div class="line">clusters:</div><div class="line">- name: local</div><div class="line">  cluster:</div><div class="line">    certificate-authority: /home/dd_ca.crt</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: local</div><div class="line">    user: kubeproxy</div><div class="line">  name: my-context</div><div class="line">current-context: my-context</div></pre></td></tr></table></figure>
<p>然后，修改 kube-proxy 的启动参数，引用上述文件并指明 API Server 在安全模式下的访问地址，以修改配置文件/etc/kubenetes/proxy 为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KUBE_PROXY_ARGS=&quot;--kubeconfig=/var/lib/kubeproxy/proxykubeconfig --master=https://kubenetes-master:443&quot;</div></pre></td></tr></table></figure>
<p>重启 kube-proxy 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-proxy</div></pre></td></tr></table></figure>
<p>至此，一个双向认证的 Kubernetes 集群环境就搭建完成了。</p>
<h2 id="简单认证配置"><a href="#简单认证配置" class="headerlink" title="简单认证配置"></a>简单认证配置</h2><p>除了双向认证方式，Kubernets 也提供了基于 Token 和 HTTP Base 的简单认证方式。通信方式仍然采用 HTTPS，但不使用数字证书。</p>
<p>采用基于 Token 和 HTTP Base 的简单认证方式时，API Server 对外暴露 HTTPS 端口，客户端提供 Token 或用户名、密码来完成认证过程。这里需要说明的一点是 Kubelet 比较特殊，它同时支持双向认证与简单认证两种模式，其他组件智能配置为双向认证或非安全模式。</p>
<p><strong>API Server 基于 Token 认证的配置过程如下</strong></p>
<h4 id="建立包括用户名、密码和-UID-的文件-token-auth-file："><a href="#建立包括用户名、密码和-UID-的文件-token-auth-file：" class="headerlink" title="建立包括用户名、密码和 UID 的文件 token_auth_file："></a>建立包括用户名、密码和 UID 的文件 token_auth_file：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat /root/token_auth_file</div><div class="line">dingmingk,dingmingk,1</div><div class="line">admin,admin,2</div><div class="line">system,system,3</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的配置，采用上述文件进行安全认证"><a href="#修改-API-Server-的配置，采用上述文件进行安全认证" class="headerlink" title="修改 API Server 的配置，采用上述文件进行安全认证"></a>修改 API Server 的配置，采用上述文件进行安全认证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/kubernetes/apiserver</div><div class="line">KUBE_API_ARGS=&quot;--secure-port=443 --token_auth_file=/root/token_auth_file&quot;</div></pre></td></tr></table></figure>
<h4 id="重启-API-Server-服务"><a href="#重启-API-Server-服务" class="headerlink" title="重启 API Server 服务"></a>重启 API Server 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="用-curl-验证连接-API-Server"><a href="#用-curl-验证连接-API-Server" class="headerlink" title="用 curl 验证连接 API Server"></a>用 curl 验证连接 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubenetes-master:443/version --header &quot;Authorization: Bearer dingmingk&quot; -k</div><div class="line">&#123;</div><div class="line">  &quot;major&quot;: &quot;1&quot;,</div><div class="line">  &quot;minor&quot;: &quot;0&quot;,</div><div class="line">  &quot;gitVersion&quot;: &quot;v1.0.0&quot;,</div><div class="line">  &quot;gitCommit&quot;: &quot;xxxHASHCODE&quot;,</div><div class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>API Server 基于 HTTP Base 认证的配置过程如下</strong></p>
<h4 id="创建包括用户名、密码和-UID-的文件-basic-auth-file："><a href="#创建包括用户名、密码和-UID-的文件-basic-auth-file：" class="headerlink" title="创建包括用户名、密码和 UID 的文件 basic_auth_file："></a>创建包括用户名、密码和 UID 的文件 basic_auth_file：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">$ cat /root/basic_auth_file</div><div class="line">dingmingk,dingmingk,1</div><div class="line">admin,admin,2</div><div class="line">system,system,3</div></pre></td></tr></table></figure>
<h4 id="修改-API-Server-的配置，采用上述文件进行安全认证-1"><a href="#修改-API-Server-的配置，采用上述文件进行安全认证-1" class="headerlink" title="修改 API Server 的配置，采用上述文件进行安全认证"></a>修改 API Server 的配置，采用上述文件进行安全认证</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ vi /etc/kubernetes/apiserver</div><div class="line">KUBE_API_ARGS=&quot;--secure-port=443 --basic_auth_file=/root/basic_auth_file&quot;</div></pre></td></tr></table></figure>
<h4 id="重启-API-Server-服务-1"><a href="#重启-API-Server-服务-1" class="headerlink" title="重启 API Server 服务"></a>重启 API Server 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart kube-apiserver</div></pre></td></tr></table></figure>
<h4 id="用-curl-验证连接-API-Server-1"><a href="#用-curl-验证连接-API-Server-1" class="headerlink" title="用 curl 验证连接 API Server"></a>用 curl 验证连接 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">$ curl https://kubernetes-master:443/version --basic -u dingmingk:dingmingk -k</div><div class="line">&#123;</div><div class="line">  &quot;major&quot;: &quot;1&quot;,</div><div class="line">  &quot;minor&quot;: &quot;0&quot;,</div><div class="line">  &quot;gitVersion&quot;: &quot;v1.0.0&quot;,</div><div class="line">  &quot;gitCommit&quot;: &quot;xxxHASHCODE&quot;,</div><div class="line">  &quot;gitTreeState&quot;: &quot;clean&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="使用-Kubelet-时则需要指定用户名和密码来访问-API-Server"><a href="#使用-Kubelet-时则需要指定用户名和密码来访问-API-Server" class="headerlink" title="使用 Kubelet 时则需要指定用户名和密码来访问 API Server"></a>使用 Kubelet 时则需要指定用户名和密码来访问 API Server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get nodes --server=&quot;https://kubernetes-master:443&quot; --api-version=&quot;v1&quot; --username=&quot;dingmingk&quot; --password=&quot;dingmingk&quot; --insecure-skip-tls-verify=true</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/04/01/k8s_security_demo/" class="archive-article-date">
  	<time datetime="2016-03-31T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-04-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_context" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/04/01/k8s_context/">Kubernetes 不同工作组共享集群案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在一个组织内部，不同的工作组可以在同一个 Kubernetes 集群中工作，Kubernetes 通过命名空间和 Context 的设置来实现对不同工作组进行区分，使得它们既可以共享同一个 Kubernetes 集群的服务，也能够互不干扰。</p>
<p>假设在我们的组织中有两个工作组：开发组和生产运维组。开发组在 Kubernetes 集群中需要不断创建、修改、删除各种 Pod、RC、Service 等资源对象，以便实现敏捷开发的过程。而生产运维组则需要使用严格的权限设置来确保生产系统中的 Pod、RC、Service 处于正常运行状态且不会被误操作。</p>
<h2 id="创建-namespace"><a href="#创建-namespace" class="headerlink" title="创建 namespace"></a>创建 namespace</h2><p>为了在 Kubernetes 集群中实现这两个分组，首先需要创建两个命名空间。</p>
<p><strong>namespace-development.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: development</div></pre></td></tr></table></figure>
<p><strong>namespace-production.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: production</div></pre></td></tr></table></figure>
<p>使用 kubectl create 命令完成命名空间的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f namespace-development.yaml</div><div class="line">$ kubectl create -f namespace-production.yaml</div></pre></td></tr></table></figure>
<p>查看系统中的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get namespaces</div></pre></td></tr></table></figure>
<h2 id="定义-Context（运行环境）"><a href="#定义-Context（运行环境）" class="headerlink" title="定义 Context（运行环境）"></a>定义 Context（运行环境）</h2><p>接下来，需要为这两个工作组分别定义一个 Context，即运行环境。这个运行环境将属于某个特定的命名空间。</p>
<p>通过 kubectl config set-context 命令定义 Context，并将 Context 置于之前创建的命名空间中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl config set-cluster kubernetes-cluster --server=https://192.168.1.128:8080</div><div class="line">$ kubectl config set-context ctx-dev --namespace=development --cluster=kubernetes-cluster --user=dev</div><div class="line">$ kubectl config set-context ctx-prod --namespace=production --cluster=kubernetes-cluster --user=prod</div></pre></td></tr></table></figure>
<p>使用 kubectl config view 命令查看已定义的 Context：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">$ kubectl config view</div><div class="line">apiVersion: v1</div><div class="line">kind: Config</div><div class="line">clusters:</div><div class="line">- cluster:</div><div class="line">    server: http://192.168.1.128:8080</div><div class="line">  name: kubernetes-cluster</div><div class="line">contexts:</div><div class="line">- context:</div><div class="line">    cluster: kubernetes-cluster</div><div class="line">    namespace: development</div><div class="line">  name: ctx-dev</div><div class="line">- context:</div><div class="line">    cluster: kubernetes-cluster</div><div class="line">    namespace: production</div><div class="line">  name: ctx-prod</div><div class="line">current-context: ctx-dev</div><div class="line">preferences: &#123;&#125;</div><div class="line">users: []</div></pre></td></tr></table></figure>
<p>注意，通过 kubectl config 命令在 ${HOME}/.kube 目录下生成了一个名为 config 的文件，文件内容即 kubectl config view 命令看到的内容。所以，也可以通过手工编辑该文件的方式来设置 Context。</p>
<h2 id="设置工作组在特定-Context-环境中工作"><a href="#设置工作组在特定-Context-环境中工作" class="headerlink" title="设置工作组在特定 Context 环境中工作"></a>设置工作组在特定 Context 环境中工作</h2><p>使用 kubectl config use-context <context_name> 命令来设置当前的运行环境。</context_name></p>
<p>下面的命令把当前运行环境设置为 ”ctx-dev“：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl config use-context ctx-dev</div></pre></td></tr></table></figure>
<p>通过这个命令，当前的运行环境即被设置为开发组所需的环境。之后的所有操作都将在名为 “development” 的命名空间中完成。</p>
<p>各工作组之间的工作将不会相互干扰，并且它们都能够在同一个 Kubernetes 集群中同时工作。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/04/01/k8s_context/" class="archive-article-date">
  	<time datetime="2016-03-31T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-04-01</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_security" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/31/k8s_security/">Kubernetes 安全机制</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Kubernetes 通过一系列机制来实现集群的安全控制，其中包括 API Server 的认证授权、准入控制机制及保护敏感信息的 Secret 机制等。集群的安全性必须考虑如下几个目标：</p>
<ol>
<li>保证容器与其所在的宿主机的隔离；</li>
<li>限制容器给基础设施及其他容器带来消极影响的能力；</li>
<li>最小权限原则————合理限制所有组件的权限，确保组件只执行它被授权的行为，通过限制单个组件的能力来限制它所能到达的权限范围；</li>
<li>明确组件间边界的划分；</li>
<li>划分普通用户和管理员的角色；</li>
<li>在必要的时候允许将管理员权限赋给普通用户；</li>
<li>允许拥有“Secret”数据（Keys、Certs、Passwords）的应用在集群中运行。</li>
</ol>
<p>下面分别从 Authentication、Authorization、Admission Control、Secret 和 Service Account 六个方面来说明集群的安全机制。</p>
<h2 id="Authentication-认证"><a href="#Authentication-认证" class="headerlink" title="Authentication 认证"></a>Authentication 认证</h2><p>Kubernetes 对 API 调用使用 CA（Client Authentication）、Token 和 HTTP Base 方式实现用户认证。</p>
<p>使用 CA 认证的应用需包含一个 CA 认证机构给服务器端下发的根证书、服务端证书和私钥文件。因此 API Server 的三个参数“–client-ca-file”“–tls-cert-file”和“–tls-private-key-file”分别指向根证书文件、服务端证书文件和私钥文件。API Server 客户端应用的三个启动参数（例如 Kubectl 的三个参数 “certificate-authority”“client-certificate”和“client-key”），或客户端应用的 kubeconfig 配置文件中的配置项“certificate-authority”“client-certificate”和“client-key”分别指向根证书文件、客户端证书文件和私钥文件。</p>
<p>Kubernetes 的 CA 认证方式通过添加 API Server 的启动参数“–client-ca-file=SOMEFILE”实现，其中“SOMEFILE”为认证授权文件，该文件包含一个或多个证书颁发机构（CA Certificates Authorities）。</p>
<p>Token 认证方式通过添加 API Server 的启动参数“–token_auth_file=SOMEFILE”实现，其中“SOMEFILE”指的是 Token 文件。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">例如 Token 文件内容为：</div><div class="line">lkjqweroiuuou,Thomas,8x7d1kklzseertyywx</div><div class="line">用 CURL 去访问该 API Server：</div><div class="line">curl $APISERVER/api --header &quot;Authorization: Bearer lkjqweroiuuou&quot; --insecure</div><div class="line">&#123;</div><div class="line">  &quot;version&quot; : [</div><div class="line">    &quot;v1&quot;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>基本认证方式是通过添加 API Server 的启动参数“–basic_auth_file=SOMEFILE”实现的，其中“SOMEFILE”指的是用于存储用户和密码信息的基本认证文件。</p>
<p>当使用基本认证方式从 HTTP 客户端访问 API Server 时，HTTP 请求头中的 Authorization 域必须包含“Basic BASE64ENCODEDUSER:PASSWORD”的值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tmp=`base64 &quot;dingmingk:passwd&quot;`</div><div class="line">curl $APISERVER/api --header &quot;Authorization: Basic $tmp&quot; --insecure</div><div class="line">&#123;</div><div class="line">  &quot;version&quot;:[</div><div class="line">    &quot;v1&quot;</div><div class="line">  ]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="Authorization-授权"><a href="#Authorization-授权" class="headerlink" title="Authorization 授权"></a>Authorization 授权</h2><p>在 Kubernetes 中，授权（Authorization）是认证（Authenticaiton）后的一个独立步骤，作用于 API Server 主要端口的所有 HTTP 访问。授权流程不作用于只读端口，在计划中只读端口在不久之后将被删除。授权流程通过访问策略比较请求上下文的属性（例如用户名、资源和 Namespace）。在通过 API 访问资源之前，必须通过访问策略进行校验。访问策略通过 API Server 的启动参数 –authorization_mode 配置，该参数包含如下三个值：</p>
<ul>
<li>–authorization_mode=AlwaysDeny</li>
<li>–authorization_mode=AlwaysAllow</li>
<li>–authorization_mode=ABAC</li>
</ul>
<p>其中，“AlwaysDeny”表示拒绝所有的请求，该配置一般用于测试；“AlwaysAllow”表示接受所有请求，如果集群不需要授权流程，则可以采用该策略；“ABAC”表示使用用户配置的授权策略去管理访问 API Server 的请求，ABAC（Attribute-Based Access Control）为基于属性的访问控制。</p>
<p>在 Kubernetes 中，一个 HTTP 请求包含如下 4 个能被授权进程识别的属性：</p>
<ul>
<li>用户名（代表一个已经被认证的用户的字符型用户名）；</li>
<li>是否是只读请求（REST 的 GET 操作是只读的）；</li>
<li>被访问的是哪一类资源，例如访问 Pod 资源/api/v1/namespaces/defaults/pods；</li>
<li>被访问对象所属的 Namespace，如果这被访问的资源不支持 Namespace，则是空字符串。</li>
</ul>
<p>如果选用 ABAC 模式，那么需要通过设置 API Server 的“–authorization_policy_file=SOME_FILENAME”参数来指定授权策略文件，其中“SOME_FILENAME”为授权策略文件。授权策略文件的每一行都是一个 JSON 对象，该 JSON 对象是一个 Map，这个 Map 内不包含 List 和 Map。每行都是一个“策略对象”。策略对象包含下面 4 个属性：</p>
<ul>
<li>user（用户名），为字符串类型，该字符串类型的用户名来源于 Token 文件或基本认证文件中的用户名字段的值；</li>
<li>readonly（只读标识），为布尔类型，当它的值为 true 时，表明该策略允许 GET 请求通过；</li>
<li>resource（资源），为字符串类型，来自于 URL 的资源，例如“Pods”；</li>
<li>namespace（命名空间），为字符串类型，表明该策略允许访问某个 Namespace 的资源。</li>
</ul>
<p>没被设置的属性，将被等同于根据值的类型设置成零值（例如为字符串类型属性设置一个空字符串；为布尔值属性设置 false；为数值类型属性设置 0）。</p>
<p>授权策略文件中的策略对象的一个未设置属性，表示匹配 HTTP 请求中该属性的任何值。对请求的 4 个属性值和授权策略文件中的所有策略对象逐个匹配，如果至少有一个策略对象被匹配上，则该请求将被授权通过。</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">允许用户 alice 做任何事情：&#123;&quot;user&quot;:&quot;alice&quot;&#125;。</div><div class="line">用户 Kubelet 指定读取资源 Pods：&#123;&quot;user&quot;:&quot;kubelet&quot;,&quot;resource&quot;:&quot;pods&quot;,&quot;readonly&quot;:true&#125;。</div><div class="line">用户 Kubelet 能读和写资源 events：&#123;&quot;user&quot;:&quot;kubelet&quot;,&quot;resource&quot;:&quot;events&quot;&#125;</div><div class="line">用户 bob 只能读取 Namespace &quot;myNamespace&quot; 中的资源 Pods：&#123;&quot;user&quot;:&quot;bob&quot;,&quot;resource&quot;:&quot;pods&quot;,&quot;readonly&quot;:true,&quot;ns&quot;:&quot;myNamespace&quot;&#125;</div></pre></td></tr></table></figure>
<h2 id="Admission-Control-准入控制"><a href="#Admission-Control-准入控制" class="headerlink" title="Admission Control 准入控制"></a>Admission Control 准入控制</h2><p>Admission Control 是用于拦截所有经过认证和鉴权后的访问 API Server 请求的可插入代码（或插件）。这些可插入代码运行于 API Server 进程中，在被调用前必须被编译成二进制文件。在请求被 API Server 接收前，每个 Admission Control 插件按配置顺序执行。如果其中的任意一个插件拒绝该请求，就意味着这个请求被 API Server 拒绝，同时 API Server 反馈一个错误信息给请求发起方。</p>
<p>在某些情况下，Admission Control 插件会使用系统配置的默认值取改变进入集群对象的内容。此外，Admission Control 插件可能会改变请求处理所使用的配额，比如增加请求处理的资源配额。</p>
<p>通过配置 API Server 的启动参数“admission_control”，在该参数中加入需要的 Admission Control 插件列表，各插件的名称之间用逗号隔开。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--admission_control=NamespaceAutoProvision,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota</div></pre></td></tr></table></figure>
<p>Admission Control 的插件列表如下表所示：</p>
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:left">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AlwaysAdmit</td>
<td style="text-align:left">允许所有请求通过</td>
</tr>
<tr>
<td style="text-align:center">AlwaysDeny</td>
<td style="text-align:left">拒绝所有请求，一般用于测试</td>
</tr>
<tr>
<td style="text-align:center">DenyExecOnPrivileged</td>
<td style="text-align:left">拦截所有带有 SecurityContext 属性的 Pod 的请求，拒绝在一个特权容器中执行命令</td>
</tr>
<tr>
<td style="text-align:center">ServiceAccount</td>
<td style="text-align:left">配合 Service Account Controller 使用，为设定了 Service Account 的 Pod 自动管理 Secret，使得 Pod 能够使用相应的 Secret 下载 Image 和访问 API Server</td>
</tr>
<tr>
<td style="text-align:center">SecurityContextDeny</td>
<td style="text-align:left">不允许带有 SecurityContext 属性的 Pod 存在，SecurityContext 属性用于创建特权容器</td>
</tr>
<tr>
<td style="text-align:center">ResourceQuota</td>
<td style="text-align:left">在 Namespace 中做资源配额限制</td>
</tr>
<tr>
<td style="text-align:center">LimitRanger</td>
<td style="text-align:left">限制 Namespace 中的 Pod 和 Container 的 CPU 和 内存配额</td>
</tr>
<tr>
<td style="text-align:center">NamespaceExists</td>
<td style="text-align:left">读区请求中的 Namespace 属性，如果该 Namespace 不存在，则拒绝该请求</td>
</tr>
<tr>
<td style="text-align:center">NamespaceAutoProvision(deprecated)</td>
<td style="text-align:left">读取请求中的 Namespace 属性，如果该 Namespace 不存在，则尝试创建该 Namespace</td>
</tr>
<tr>
<td style="text-align:center">NamespaceLifecycle</td>
<td style="text-align:left">该插件限制访问处于中止状态的 Namespace，禁止在该 Namespace 中创建新的内容。当 NamespaceLifecycle 和 NamespaceExists 能够合并成一个插件后，NamespaceAutoProvision 就会变成 deprecated</td>
</tr>
</tbody>
</table>
<h2 id="Secret-私密凭据"><a href="#Secret-私密凭据" class="headerlink" title="Secret 私密凭据"></a>Secret 私密凭据</h2><p>Secret 的主要作用是保管私密数据，比如密码、OAuth Tokens、SSH Keys 等信息。将这些私密信息放在 Secret 对象中比直接放在 Pod 或 Docker Image 中更安全，也更便于使用。</p>
<p>Kubernetes 在 Pod 创建时，如果该 Pod 指定了 Service Account，那么为该 Pod 自动添加包含凭证信息的 Secrets，用于访问 API Server 和下载 Image。该功能可以通过 Admission Control 添加或失效，然而如果需要以安全的方式去访问 API Server，则建议开启该功能。</p>
<p>下面的例子用于创建一个 Secret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$ kubectl namespace myspace</div><div class="line">$ cat &lt;&lt;EOF &gt; secrets.yaml</div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysecret</div><div class="line">type: Opaque</div><div class="line">data:</div><div class="line">  password: dmFsdWUtMgOK</div><div class="line">  username: dmFsdWUtMQOK</div><div class="line">$ kubectl create -f secrets.yaml</div></pre></td></tr></table></figure>
<p>在上面的例子中，data域的各子域的值必须为 base64 编码值，其中 password 域和 username 域 base64 编码前的值分别为“value-1” 和“value-2”。</p>
<p>一旦 Secret 被创建，则可以通过下面的三种方式使用它：</p>
<ol>
<li>在创建 Pod 时，通过为 Pod 指定 Service Account 来自动使用该 Secret；</li>
<li>通过挂载该 Secret 到 Pod 来使用它；</li>
<li>在创建 Pod 时，指定 Pod 的 spc.ImagePullSecrets 来引用它。</li>
</ol>
<p>将一个 Secret 通过挂载的方式添加到 Pod 的 Volume 中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: myPod</div><div class="line">  namespace: myns</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    name: mycontainer</div><div class="line">    image: redis</div><div class="line">    volumeMounts:</div><div class="line">    - name: foo</div><div class="line">      mountPath: /etc/foo</div><div class="line">      readOnly: true</div><div class="line">  volumes:</div><div class="line">  - name: foo</div><div class="line">    secret:</div><div class="line">      secretNmae: mysecret</div></pre></td></tr></table></figure>
<p>手动使用 imagePullSecret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">$ docker login localhost:1180</div><div class="line"></div><div class="line">用 base64 编码 dockercfg 的内容</div><div class="line">$ cat ~/.dockercfg | base64</div><div class="line"></div><div class="line">将上一步命令的输出结果作为 Secret 的“data.dockercfg” 域的内容，由此来创建一个 Secret</div><div class="line">$ cat &gt; image-pull-secret.yaml &lt;&lt;EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: myregistrykey</div><div class="line">data:</div><div class="line">  .dockercfg: xxxxxxxxxxxxxxxxxxxxx</div><div class="line">type: kubernetes.io/dockercfg</div><div class="line">EOF</div><div class="line">$ kubectl create -f image-pull-secret.yaml</div><div class="line"></div><div class="line">在创建 Pod 时，引用该 Secret</div><div class="line">$ cat &gt; pods.yaml &lt;&lt; EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: mypod2</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: foo</div><div class="line">      image: janedoe/awesomeapp:v1</div><div class="line">  imagePullSecrets:</div><div class="line">    - name: myregistrykey</div><div class="line">EOF</div><div class="line"></div><div class="line">$ kubectl create -f pods.yaml</div></pre></td></tr></table></figure>
<p>Pod 创建时会验证所挂载的 Secret 是否真的指向一个 Secret 对象，因此 Secret 必须在任何引用它的 Pod 之前被创建。Secret 对象属于 Namespace，它们只能被同一个 NameSpace 中的 Pod 所引用。</p>
<p>Secret 包含三种类型：Opaque、ServiceAccount 和 Dockercfg。前面举例介绍了如何创建 Opaque 和 Dockercfg 类型的 Secret。下面的例子为创建一个 Service Account Secret：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Secret</div><div class="line">metadata:</div><div class="line">  name: mysecret</div><div class="line">  annotations:</div><div class="line">    kubernetes.io/service-account.name: myserviceaccount</div><div class="line">type: kubernetes.io/service-account-token</div></pre></td></tr></table></figure>
<h2 id="Service-Account"><a href="#Service-Account" class="headerlink" title="Service Account"></a>Service Account</h2><p>Service Account 是多个 Secret 的集合。它包含两类 Secret：一类为普通 Secret，用于访问 API Server，也被称为 Service Account Secret；另一类为 imagePullSecret，用于下载容器镜像。如果镜像库运行在 Insecure 模式下，则该 Service Account 可以不包含 imagePullSecret。在下面的例子中创建了一个名为 build-robot 的 Service Account，并查询该 Service Account 的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">$ cat &gt; serviceaccount.yaml &lt;&lt; EOF</div><div class="line">apiVersion: v1</div><div class="line">kind: ServiceAccount</div><div class="line">metadata:</div><div class="line">  name: myserviceaccount</div><div class="line">secrets:</div><div class="line">  - name: mysecret</div><div class="line">    kind: Secret</div><div class="line">    apiVersion: v1</div><div class="line">  - name: mysecret1</div><div class="line">    kind: Secret</div><div class="line">    apiVersion: v1</div><div class="line">imagePullSecrets:</div><div class="line">  - name: mysecret2</div><div class="line">EOF</div><div class="line">$ kubectl create -f serviceaccount.yaml</div><div class="line">$ kubectl get serviceaccounts build-robot -o yaml</div></pre></td></tr></table></figure>
<p>在 Pod 的创建过程中指定“spec.serviceAccountName”的值为相应的 Service Account 的名称：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Pod</div><div class="line">metadata:</div><div class="line">  name: mypod</div><div class="line">spec:</div><div class="line">  containers:</div><div class="line">    - name: mycontainer</div><div class="line">      image: nginx</div><div class="line">  serviceAccountName: myserviceaccount</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/31/k8s_security/" class="archive-article-date">
  	<time datetime="2016-03-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_monitor" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/31/k8s_monitor/">Kubernetes 集群性能监控</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在 Kubernetes 系统中，使用 cAdvisor 对 Node 所在主机资源和在该 Node 上运行的容器进行监控和性能数据采样。由于 cAdvisor 集成在 Kubelet 中，即运行在每个 Node 上，所以一个 cAdvisor 仅能对一台 Node 进行监控。在大规模容器集群中，我们需要对所有 Node 和全部容器进行性能监控，Kubernetes 使用一套工具来实现集群性能数据的采集、存储和展示：Heapster、InfluxDB 和 Grafana。</p>
<blockquote>
<p>Heapster：是对集群中各 Node、Pod 的资源使用数据进行采集的系统，通过访问每个 Node 上 Kubelet 的 API，再通过 Kubelet 调用 cAdvisor 的 API 来采集该节点上所有容器的性能数据。之后 Heapster 进行数据聚合，并将结果保存到后端存储系统中。Heapster 支持多种后端存储系统，包括 memory、InfluxDB、BigQuery、谷歌云平台提供的 Google Cloud Monitoring 和 Google Cloud Logging等。</p>
<p>InfluxDB：是分布式时序数据库（每条记录都带有时间戳属性），主要用于实时数据采集、事件跟踪记录、存储时间图标、原始数据等。InfluxDB 提供 REST API 用于数据的存储和查询。</p>
<p>Grafana：通过 Dashboard 将 InfluxDB 中的时序数据展现成图标或曲线等形式，便于运维人员查看集群的运行状态。</p>
</blockquote>
<h2 id="配置-Kubernetes-集群的-ServiceAccount-和-Secret"><a href="#配置-Kubernetes-集群的-ServiceAccount-和-Secret" class="headerlink" title="配置 Kubernetes 集群的 ServiceAccount 和 Secret"></a>配置 Kubernetes 集群的 ServiceAccount 和 Secret</h2><p>Heapster 当前版本需要使用 HTTPS 的安全方式与 Kubernetes Master 进行连接，所以需要先进行 ServiceAccount 和 Secret 的创建。如果不使用 Secret，则 Heapster 启动时将会报错，然后 Heapster 容器会被 ReplicationController 反复销毁、创建，无法正常工作。</p>
<p>关于 ServiceAccount 和 Secret 的原理详见<a href="http://blog.dingmingk.com/blog/kube_security.html" target="_blank" rel="external">http://blog.dingmingk.com/blog/kube_security.html</a>。</p>
<p>在进行一下操作时，我们假设在 Kubernetes 集群中没有创建过 Secret（如果之前创建过，则可以先删除 etcd 中与 Secret 相关的键值）。</p>
<p>首先，使用 OpenSSL 工具在 Master 服务器上创建一些证书和私钥相关的文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># openssl genrsa -out ca.key 2048</div><div class="line"># openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=yourcompany.com&quot; -days 5000 -out ca.crt</div><div class="line"># openssl genrsa -out server.key 2048</div><div class="line"># openssl req -new -key server.key -subj &quot;/CN=kubernetes-master&quot; -out server.csr</div><div class="line"># openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 5000</div></pre></td></tr></table></figure>
<p>注意，在生成 server.csr 时 -subj 参数中 /CN 指定的名字需为 Master 的主机名。另外，在生成 ca.crt 时 -subj 参数中 /CN 的名字最好与主机名不同，设为相同可能导致对普通 Master 的 HTTPS 访问认证失败。</p>
<p>执行完成后会生成 6 个文件：ca.crt、ca.key、ca.srl、server.crt、server.csr、server.key。将这些文件复制到 /var/run/kubernetes/ 目录中，然后设置kube-apiserver 的启动参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">--client_ca_file=/var/run/kubernetes/ca.crt</div><div class="line">--tls-private-key-file=/var/run/kubernetes/server.key</div><div class="line">--tls-cert-file=/var/run/kubernetes/server.crt</div></pre></td></tr></table></figure>
<p>之后重启 kube-apiserver 服务。</p>
<p>接下来，给 kube-controller-manager 服务添加以下启动参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">--service_account_private_key_file=/var/run/kubernetes/server.key</div><div class="line">--root-ca-file=/var/run/kubernetes/ca.crt</div></pre></td></tr></table></figure>
<p>然后重启 kube-controller-manager 服务。</p>
<p>在 kube-apiserver 服务成功启动后，系统会自动为每个命名空间创建一个 ServiceAccount 和一个 Secret（包含一个 ca.crt 和一个 token）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ kubectl get serviceaccounts --all-namespaces</div><div class="line">$ kubectl get secrets --all-namespaces</div><div class="line">$ kubectl describe secret xxx</div></pre></td></tr></table></figure>
<p>之后 ReplicationController 在创建 Pod 时，会生成类型为 Secret 的 Volume 存储卷，并将该 Volume 挂载到 Pod 内的如下目录中：/var/run/secrets/kubernetes.io/serviceaccount。然后，容器内的应用程序就可以使用该 Secret 与 Master 建立 HTTPS 连接了。Pod 的 Volumes 设置和挂载操作由 ReplicationController 和 Kubelet 自动完成，可以通过查看 Pod 的详细信息了解到。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl get pods kube-dns-v8-xxxxx --namespace=kube-system -o yaml</div></pre></td></tr></table></figure>
<p>进入容器，查看/var/run/secrets/kubernetes.io/serviceaccount 目录，可以看到两个文件 ca.crt 和 token，这两个文件就是与 Master 通信时所需的证书和秘钥信息。</p>
<h2 id="部署-Heapster、InfluxDB、Grafana"><a href="#部署-Heapster、InfluxDB、Grafana" class="headerlink" title="部署 Heapster、InfluxDB、Grafana"></a>部署 Heapster、InfluxDB、Grafana</h2><p>在 ServiceAccount 和 Secrets 创建完成后，我们就可以创建 Heapster、InfluxDB 和 Grafana 等 ReplicationController 和 Service 了。</p>
<p><strong>heapsster-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">    kubernetes.io/name: Heapster</div><div class="line">  name: heapster</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  ports:</div><div class="line">    - port: 80</div><div class="line">      targetPort: 8082</div><div class="line">  selector:</div><div class="line">    k8s-app: heapster</div></pre></td></tr></table></figure>
<p><strong>InfluxDB-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels: null</div><div class="line">  name: monitoring-InfluxDB</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">ports:</div><div class="line">  - name: http</div><div class="line">    port: 8083</div><div class="line">    targetPort: 8083</div><div class="line">    nodePort: 30083</div><div class="line">  - name: api</div><div class="line">    port: 8086</div><div class="line">    targetPort: 8086</div><div class="line">    nodePort: 30086</div><div class="line">selector:</div><div class="line">  name: influxGrafana</div></pre></td></tr></table></figure>
<p><strong>Grafana-service.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    kubernetes.io/name: monitoring-Grafana</div><div class="line">    kubernetes.io/cluster-service: &quot;true&quot;</div><div class="line">  name: monitoring-Grafana</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  type: NodePort</div><div class="line">  ports:</div><div class="line">    - port: 80</div><div class="line">      targetPort: 8080</div><div class="line">      nodePort: 30080</div><div class="line">  selector:</div><div class="line">    name: influxGrafana</div></pre></td></tr></table></figure>
<p><strong>heapster-controller.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    k8s-app: heapster</div><div class="line">    name: heapster</div><div class="line">    version: v6</div><div class="line">  name: heapster</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: heapster</div><div class="line">    k8s-app: heapster</div><div class="line">    version: v6</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: heapster</div><div class="line">        version: v6</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: heapster</div><div class="line">          image: gcr.io/google_containers/heapster:v0.17.0</div><div class="line">          command:</div><div class="line">            - /heapster</div><div class="line">            - --source=kubernetes:http://192.168.1.128:8080?inClusterConfig=false&amp;kubeletHttps=true&amp;useServiceAccount=true&amp;auth=</div><div class="line">            - --sink=InfluxDB:http://monitoring-InfluxDB:8086</div></pre></td></tr></table></figure>
<p>Heapster 需要设置的参数如下：</p>
<ul>
<li>–source：为配置监控来源。在本例中使用 kubernetes:表示从 Kubernetes Master 获取各 Node 的信息。在 URL 后面的参数部分，修改 kubeletHttps、inClusterConfig、useServiceAccount 的值，并设置 auth 的值为空。URL中可配置的参数如下：<ul>
<li>IP 地址和端口号：为 Kubernetes Master 的地址。</li>
<li>kubeletPort：默认为 10255（Kubelet 服务的只读端口号）。</li>
<li>kubeletHttps：是否通过 HTTPS 方式连接 Kubelet，默认为 false。</li>
<li>apiVersion： API 版本号，默认为 Kubernetes 系统的版本号，当前为 v1.</li>
<li>inClusterConfig：是否使用 Heapster 命名空间中的 ServiceAccount，默认为 true。</li>
<li>insecure：是否信任 Kubernetes 证书，默认为 false。</li>
<li>auth：客户端认证授权文件，当 ServiceAccount 不可用时对其进行设置。</li>
<li>useServiceAccount：是否使用 ServiceAccount，默认为 false。</li>
</ul>
</li>
<li>–sink：为配置后端的存储系统，在本例中使用 InfluxDB 系统。</li>
</ul>
<p><strong>InfluxDB-Grafana-controller.yaml</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  labels:</div><div class="line">    name: influxGrafana</div><div class="line">  name: infludb-Grafana</div><div class="line">  namespace: kube-system</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: influxGrafana</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: influxGrafana</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">        - name: InfluxDB</div><div class="line">          image: gcr.io/google_containers/heapster_InfluxDB:v0.3</div><div class="line">          ports:</div><div class="line">            - containerPort: 8083</div><div class="line">              hostPort: 8083</div><div class="line">            - containerPort: 8086</div><div class="line">              hostPort: 8086</div><div class="line">        - name: Grafana</div><div class="line">          image: gcr.io/google_containers/heapster_Grafana:v0.7</div><div class="line">          ports:</div><div class="line">            - containerPort: 8080</div><div class="line">              hostPort: 8080</div><div class="line">          env:</div><div class="line">            - name: INFLUXDB_HOST</div><div class="line">              value: monitoring-InfluxDB</div></pre></td></tr></table></figure>
<p>最后，创建所有 Service 和 RC：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ kubectl create -f heapster-service.yaml</div><div class="line">$ kubectl create -f InfluxDB-server.yaml</div><div class="line">$ kubectl create -f Grafana-service.yaml</div><div class="line">$ kubectl create -f InfluxDB-Grafana-controller.yaml</div><div class="line">$ kubectl create -f heapster-controller.yaml</div></pre></td></tr></table></figure>
<p>通过 kubectl get pods –namespace=kube-system 确认各 Pod 都成功启动了。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/31/k8s_monitor/" class="archive-article-date">
  	<time datetime="2016-03-30T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-31</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_dns" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/30/k8s_dns/">Kubernetes DNS 服务配置案例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在 Kubernetes 系统中，Pod 在访问其他 Pod 的 Service 时，可以通过两种服务发现方式完成，即环境变量和 DNS 方式。但是使用环境变量是有限制条件的，即 Service 必须在 Pod 之前被创建出来，然后系统才能在新建的 Pod 中自动设置与 Service 相关的环境变量。DNS 则没有这个限制，其通过全局的 DNS 服务器来完成服务的注册与发现。</p>
<p>Kubernetes 提供的 DNS 由以下三个组件组成：</p>
<ol>
<li>etcd: DNS 存储</li>
<li>kube2sky: 将 Kubernetes Master 中的 Service（服务）注册到 etcd。</li>
<li>skyDNS: 提供 DNS 域名解析服务。</li>
</ol>
<p>这三个组件以 Pod 的方式启动和运行，所以在一个 Kubernetes 集群中，它们都可能被调度到任意一个 Node 节点上去。为了能够使它们之间网络互通，需要将各 Pod 之间的网络打通。</p>
<h2 id="skydns-配置文件"><a href="#skydns-配置文件" class="headerlink" title="skydns 配置文件"></a>skydns 配置文件</h2><p>首先创建 DNS 服务的 ReplicationController 配置文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: kube-dns-v8</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    k8s-app: kube-dns</div><div class="line">    version: v8</div><div class="line">    kubernetes.io/cluster-service: &quot; true &quot;</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    k8s-app: kube-dns</div><div class="line">    version: v8</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        k8s-app: kube-dns</div><div class="line">        version: v8</div><div class="line">        kubernetes.io/cluster-service: &quot; true &quot;</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: etcd</div><div class="line">        image: gcr.io/google_containers/etcd:2.0.9</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        command:</div><div class="line">        - /usr/local/bin/etcd</div><div class="line">        - -data-dir</div><div class="line">        - /var/etcd/data</div><div class="line">        - -listen-client-urls</div><div class="line">        - http://127.0.0.1:2379,http://127.0.0.1:4001</div><div class="line">        - -advertise-client-urls</div><div class="line">        - http://127.0.0.1:2379,http://127.0.0.1:4001</div><div class="line">        - -initial-cluster-token</div><div class="line">        - skydns-etcd</div><div class="line">        volumeMounts:</div><div class="line">        - name: etcd-storage</div><div class="line">          mountPath: /var/etcd/data</div><div class="line">      - name: kube2sky</div><div class="line">        image: gcr.io/google_containers/kube2sky:1.11</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        args:</div><div class="line">        # command = &quot; /kube2sky &quot;</div><div class="line">        - --kube_master_url=http://192.168.1.128:8080</div><div class="line">        - -domain=cluster.local</div><div class="line">      - name: skydns</div><div class="line">        image: gcr.io/google_containers/skydns:2015-03-11-001</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 100m</div><div class="line">            memory: 50Mi</div><div class="line">        args:</div><div class="line">        # command = &quot; /skydns &quot;</div><div class="line">        - -machines=http://localhost:4001</div><div class="line">        - -addr=0.0.0.0:53</div><div class="line">        - -domain=cluster.local</div><div class="line">        ports:</div><div class="line">        - containerPort: 53</div><div class="line">          name: dns</div><div class="line">          protocol: UDP</div><div class="line">        - containerPort: 53</div><div class="line">          name: dns-tcp</div><div class="line">          protocol: TCP</div><div class="line">      volumes:</div><div class="line">        - name: etcd-storage</div><div class="line">          emptyDir: &#123;&#125;</div><div class="line">      dnsPolicy: Default</div></pre></td></tr></table></figure>
<p>需要修改的几个配置参数如下：</p>
<ul>
<li>kube2sky 容器需要访问 Kubernetes Master，需要配置 Master 所在物理主机的 IP 地址和端口号，本例中设置参数 –kube_master_url 的值为 <a href="http://192.168.1.128:8080。" target="_blank" rel="external">http://192.168.1.128:8080。</a></li>
<li>kube2sky 容器和 skydns 容器的启动参数 -domain,设置 Kubernetes 集群中 Service 所属的域名，本例中为 cluster.local。启动后，kube2sky 会监听 Kubernetes，当有新的 Service 创建时，就会生成相应的记录并保存到 etcd 中。kube2sky 为每个 Service 生成两条记录：<ul>
<li><service_name>.<namespace_name>.<domain>;</domain></namespace_name></service_name></li>
<li><service_name>.<namespace_name>.svc.<domain>。</domain></namespace_name></service_name></li>
</ul>
</li>
<li>skydns 的启动参数 -addr=0.0.0.0:53 表示使用本机 TCP 和 UDP 的 53 端口提供服务。</li>
</ul>
<p>创建 DNS 服务的 Service 配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Service</div><div class="line">metadata:</div><div class="line">  name: kube-dns</div><div class="line">  namespace: kube-system</div><div class="line">  labels:</div><div class="line">    k8s-app: kube-system</div><div class="line">    labels:</div><div class="line">      k8s-app: kube-dns</div><div class="line">      kubernetes.io/cluster-services: &quot; true &quot;</div><div class="line">      kubernetes.io/name: &quot; KubeDNS &quot;</div><div class="line">spec:</div><div class="line">  selector:</div><div class="line">    k8s-app: kube-dns</div><div class="line">  clusterIP: 20.1.0.100</div><div class="line">  ports:</div><div class="line">  - name: dns</div><div class="line">    port: 53</div><div class="line">    protocol: UDP</div><div class="line">  - name: dns-tcp</div><div class="line">    port: 53</div><div class="line">    protocol: TCP</div></pre></td></tr></table></figure>
<p>注意，skydns 服务使用的 clusterIP 需要我们指定一个固定的 IP 地址，每个 Node 的 Kubernetes 进程都将使用这个 IP 地址，不能通过 Kubernetes 自动分配。</p>
<p>另外，这个 IP 地址需要在 kube-apiserver 启动参数 –service-cluster-ip-range 指定的 IP 地址范围内。</p>
<h2 id="修改每个-Node-上的-Kubelet-启动参数"><a href="#修改每个-Node-上的-Kubelet-启动参数" class="headerlink" title="修改每个 Node 上的 Kubelet 启动参数"></a>修改每个 Node 上的 Kubelet 启动参数</h2><p>修改每台 Node 上 Kubelet 的启动参数：</p>
<ul>
<li>–cluster_dns=20.1.0.100，为 DNS 服务的 ClusterIP 地址；</li>
<li>–cluster_domain=cluster.local，为 DNS 服务中设置的域名。</li>
</ul>
<p>然后重启 Kubelet 服务。</p>
<h2 id="创建-skydns-Pod-和服务"><a href="#创建-skydns-Pod-和服务" class="headerlink" title="创建 skydns Pod 和服务"></a>创建 skydns Pod 和服务</h2><p>通过 kubectl create 完成 RC 和 Service 的创建：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># kubectl create -f skydns-rc.yaml</div><div class="line"># kubectl create -f skydns-svc.yaml</div></pre></td></tr></table></figure>
<p>创建完成后，可以查看系统创建的 RC、Pod、Service 是否创建成功：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># kubectl get rc|pods|services --namespace=kube-system</div></pre></td></tr></table></figure>
<h2 id="DNS-服务的工作原理"><a href="#DNS-服务的工作原理" class="headerlink" title="DNS 服务的工作原理"></a>DNS 服务的工作原理</h2><p>让我们看看 DNS 服务背后的工作原理。</p>
<p>kube2sky 容器应用通过调用 Kubernetes Master 的 API 获得集群中所有 Service 的信息，并持续监控新 Service 的生成，然后写入 etcd 中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">查看 etcd 中存储的 Service 信息</div><div class="line"># kubectl exec kube-dns-v8-xxxxx -c etcd --namespace=kube-system etcdctl ls /skydns/local/cluster</div></pre></td></tr></table></figure>
<p>根据 Kubelet 启动参数的设置（–cluster_dns），Kubelet 会在每个新创建的 Pod 中设置 DNS 域名解析配置文件 /etc/resolv.conf 文件，在其中增加了一条 nameserver 配置和一条 search 配置。</p>
<p>最后，应用程序就能够像访问网站域名一样，仅仅通过服务的名字就能访问到服务了。</p>
<p>通过 DNS 设置，对于其他 Service 的查询将可以不再依赖系统为每个 Pod 创建的环境变量，而是直接使用 Service 的名字就能对其进行访问，使得应用程序中的代码更加简洁。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/30/k8s_dns/" class="archive-article-date">
  	<time datetime="2016-03-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-30</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/30/k8s_network/">Kubernetes 网络配置方案</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>为了实现各 Node 上 Pod 之间的互通，需要一些方案来打通网络，这是 Kubernetes 能够正常工作的前提。本篇将对常用的直接路由、Flannel 和 Open vSwitch 三种配置进行详细说明。</p>
<h2 id="直接路由"><a href="#直接路由" class="headerlink" title="直接路由"></a>直接路由</h2><p>通过在每个 Node 上添加到其他 Node 上 docker0 的静态路由规则，就可以将不同物理服务器上 Docker Daemon 创建的 docker0 网桥互联互通。</p>
<p>例如 Pod1 所在 docker0 网桥的 IP 子网是 10.1.10.0，Node 地址为 192.168.1.128；而 Pod2 所在 docker0 网桥的 IP 子网是 10.1.20.0，Node 地址为 192.168.1.129.</p>
<p>在 Node1 上增加一条到 Node2 上 docker0 的静态路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">route add -net 10.1.20.0 netmask 255.255.255.0 gw 192.168.1.129</div></pre></td></tr></table></figure>
<p>同样，在 Node2 上增加一条到 Node1 上 docker0 的静态路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">route add -net 10.1.10.0 netmask 255.255.255.0 gw 192.168.1.128</div></pre></td></tr></table></figure>
<p>我们的集群中机器数量通常很多。假设有 100 台服务器，那么就需要在每台服务器上手工添加到另外 99 台服务器 docker0 的路由规则。为了减少手工操作，可以使用 Quagga 软件来实现路由规则的动态添加。</p>
<p>除了在每台服务器上安装 Quagga 软件并启动，还可以使用互联网上的一个 Quagga 容器来运行，在这里使用 index.alauda.cn/georce/router 镜像启动 Quagga。在每台 Node 上下载该 Docker 镜像：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker pull index.alauda.cn/georce/router</div></pre></td></tr></table></figure>
<p>在运行 Quagga 路由器之前，需要确保每个 Node 上 docker0 网桥的子网地址不能重叠，也不能与物理机所在的网络重叠，这需要网络管理员仔细规划。</p>
<p>下面以 3 个 Node 为例，使用 ifconfig 命令修改 docker0 网桥的地址和子网（假设 Node 所在的物理网络不是 10.1.X.X地址段）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Node 1: # ifconfig docker0 10.1.10.1/24</div><div class="line">Node 2: # ifconfig docker0 10.1.20.1/24</div><div class="line">Node 3: # ifconfig docker0 10.1.30.1/24</div></pre></td></tr></table></figure>
<p>然后在每个 Node 上启动 Quagga 容器。需要说明的是，Quagga 需要以 –privileged 特权模式运行，并且指定 –net=host，表示直接使用物理机的网络：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ docker run -itd --name=router --privileged --net=host index.alauda.cn/georce/router</div></pre></td></tr></table></figure>
<p>启动成功后，Quagga 会相互学习来完成到其他机器的 docker0 路由的添加。</p>
<h2 id="使用-flannel-叠加网络"><a href="#使用-flannel-叠加网络" class="headerlink" title="使用 flannel 叠加网络"></a>使用 flannel 叠加网络</h2><p>falnnel 采用叠加网络（Overlay Network）模型来完成网络的打通。</p>
<h4 id="安装-etcd"><a href="#安装-etcd" class="headerlink" title="安装 etcd"></a>安装 etcd</h4><p>由于 flannel 使用 etcd 作为数据库，所以需要预先安装好 etcd。</p>
<h4 id="安装-flannel"><a href="#安装-flannel" class="headerlink" title="安装 flannel"></a>安装 flannel</h4><p>需要在每台 Node 上都安装 flannel。flannel 软件的下载地址为 (<a href="https://github.com/coreos/flannel/releases)。将下载的压缩包解压，把二进制文件" target="_blank" rel="external">https://github.com/coreos/flannel/releases)。将下载的压缩包解压，把二进制文件</a> flanneld 和 mk-docker-opts.sh 复制到 /usr/bin（或其他 PATH 环境变量中的目录），即可完成对 flannel 的安装。</p>
<h4 id="配置-flannel"><a href="#配置-flannel" class="headerlink" title="配置 flannel"></a>配置 flannel</h4><p>此处以使用 systemd 系统为例对 flanneld 服务进行配置。编辑服务配置文件 /etc/systemd/system/flanneld.service：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[Unit]</div><div class="line">Description=Flanneld overlay address etcd agent</div><div class="line">After=network.target</div><div class="line">Before=docker.service</div><div class="line"></div><div class="line">[Service]</div><div class="line">Type=notify</div><div class="line">EnvironmentFile=/etc/sysconfig/flanneld</div><div class="line">EnvironmentFile=-/etc/sysconfig/docker-network</div><div class="line">ExecStart=/usr/bin/flanneld -etcd-endpoints=$&#123;FLANNEL_ETCD&#125; $FLANNEL_OPTIONS</div><div class="line"></div><div class="line">[Install]</div><div class="line">RequiredBy=docker.service</div><div class="line">WantedBy=multi-user.target</div></pre></td></tr></table></figure>
<p>编辑配置文件/etc/sysconfig/flannel，设置 etcd 的 URL 地址：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"># Flanneld configuration options</div><div class="line"># etcd url location. Point this to the server where etcd runs</div><div class="line">FLANNEL_ETCD=&quot;http://192.168.1.128:2379&quot;</div><div class="line"></div><div class="line"># etcd config key. THis is the configuration key that flannel queries</div><div class="line"># For address range assignment</div><div class="line">FLANNEL_ETCD_KEY=&quot;/coreos.com/network&quot;</div><div class="line"></div><div class="line"># Any additional options that you want to pass</div><div class="line">#FLANNEL_OPTIONS=&quot; &quot;</div></pre></td></tr></table></figure>
<p>在启动 flannel 之前，需要在 etcd 中添加一条网络配置记录，这个配置将用于 flannel 分配给每个 Docker 的虚拟 IP 地址段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># etcdctl set /coreos.com/network/config &apos;&#123; &quot;Network&quot; : &quot; 10.1.0.0/16 &quot; &#125;&apos;</div></pre></td></tr></table></figure>
<h4 id="由于-flannel-将覆盖-docker0-网桥，所以如果-Docker-服务网已启动，则停止-Docker-服务。"><a href="#由于-flannel-将覆盖-docker0-网桥，所以如果-Docker-服务网已启动，则停止-Docker-服务。" class="headerlink" title="由于 flannel 将覆盖 docker0 网桥，所以如果 Docker 服务网已启动，则停止 Docker 服务。"></a>由于 flannel 将覆盖 docker0 网桥，所以如果 Docker 服务网已启动，则停止 Docker 服务。</h4><h4 id="启动-flanneld-服务"><a href="#启动-flanneld-服务" class="headerlink" title="启动 flanneld 服务"></a>启动 flanneld 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># systemctl restart flanneld</div></pre></td></tr></table></figure>
<h4 id="在每个-Node-节点执行以下命令来完成对-docker0-网桥的配置："><a href="#在每个-Node-节点执行以下命令来完成对-docker0-网桥的配置：" class="headerlink" title="在每个 Node 节点执行以下命令来完成对 docker0 网桥的配置："></a>在每个 Node 节点执行以下命令来完成对 docker0 网桥的配置：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"># mk-docker-opts.sh -i</div><div class="line"># source /run/flannel/subnet.env</div><div class="line"># ifconfig docker0 $&#123;FLANNEL_SUBNET&#125;</div></pre></td></tr></table></figure>
<h4 id="重新启动-Docker-服务"><a href="#重新启动-Docker-服务" class="headerlink" title="重新启动 Docker 服务"></a>重新启动 Docker 服务</h4><h2 id="使用-Open-vSwitch"><a href="#使用-Open-vSwitch" class="headerlink" title="使用 Open vSwitch"></a>使用 Open vSwitch</h2><p>以两个 Node 为例，首先确保节点 192.168.18.128 的 Docker0 采用 172.17.43.0／24 网段，而 192.168.18.131 的 Docker0 采用 172.17.42.0/24 网段，对应参数为 DockerDaemon 进程里的 bip 参数。</p>
<h4 id="安装-ovs"><a href="#安装-ovs" class="headerlink" title="安装 ovs"></a>安装 ovs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># yum install openvswitch-x.x.x-x.x86_64.rpm</div></pre></td></tr></table></figure>
<p>禁止 SELINUX 功能，配置后重启机器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># vi /etc/selinux/config</div><div class="line">SELINUX=disabled</div></pre></td></tr></table></figure>
<p>查看 Open vSwitch 的服务状态，应该启动两个进程：ovsdb-server 与 ovs-vswitchd。</p>
<h4 id="创建网桥和-GRE-隧道"><a href="#创建网桥和-GRE-隧道" class="headerlink" title="创建网桥和 GRE 隧道"></a>创建网桥和 GRE 隧道</h4><p>接下来需要在每个 Node 上建立 ovs 的网桥 br0，然后在网桥上创建一个 GRE 隧道连接对端网桥，最后把 ovs 的网桥 br0 作为一个端口连接到 docker0 这个 Linux 网桥上。这样以来，两个节点上的 docker0 网段就能互通了。</p>
<p>创建网桥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ovs-vsctl add-br br0</div></pre></td></tr></table></figure>
<p>创建 GRE 隧道连接对端，remote_ip 为对端 eth0 的网卡地址</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ovs-vsctl add-port br0 grel -- set interface grel type=gre option:remote_ip=192.168.18.128</div></pre></td></tr></table></figure>
<p>添加 br0 到本地 docker0，使得容器流量通过 OVS 流经 tunnel</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># brctl addif docker0 br0</div></pre></td></tr></table></figure>
<p>启动 br0 与 docker0 网桥</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># ip link set dev br0 up</div><div class="line"># ip link set dev docker0 up</div></pre></td></tr></table></figure>
<p>添加路由规则。由于 192.168.18.128 与 192.168.18.131 的 docker0 网段分别为 172.17.43.0/24 与 172.17.42.0／24，这两个网段的路由都需要经过本机的 docker0 网桥路由，其中一个 24 网段是通过 OVS 的 GRE 隧道到达对端的，因此需要在每个 Node 上添加通过 docker0 网桥转发的 172.17.0.0/16 段的路由规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># ip route add 172.17.0.0/16 dev docker0</div></pre></td></tr></table></figure>
<p>清空 Docker 自带的 iptables 规则及 Linux 的规则 ，后者存在拒绝 icmp 报文通过防火墙的规则：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># iptables -t nat -F; iptables -F</div></pre></td></tr></table></figure>
<p>在 192.168.18.131 上完成上述步骤后，在 192.168.18.128 节点执行同样的操作。注意，GRE 隧道里的 IP 地址药改为对端节点（192.168.18.131）的 IP 地址。</p>
<h4 id="Node-上容器之间的互通测试"><a href="#Node-上容器之间的互通测试" class="headerlink" title="Node 上容器之间的互通测试"></a>Node 上容器之间的互通测试</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"># tshark -i br0 -R ip proto great</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/30/k8s_network/" class="archive-article-date">
  	<time datetime="2016-03-29T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-30</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_quota" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/29/k8s_quota/">Kubernetes 资源配额管理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>资源的配额管理功能主要用于对集群中可用的资源进行分配和限制。为了开启配额管理，需要设置 kube-apiserver 的 –admission_control 参数，使之家在这两个准入控制器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kube-apiserver ... --admission_control=LimitRanger,ResourceQuota...</div></pre></td></tr></table></figure>
<h2 id="指定容器配额"><a href="#指定容器配额" class="headerlink" title="指定容器配额"></a>指定容器配额</h2><p>对指定容器实施配额管理非常简单，只要在 Pod 或 ReplicationController 的定义文件中设定 resources 属性即可为某个容器指定配额。目前容器支持 CPU 和 Memory 两类资源的配额限制。</p>
<p>在下面这个 RC 定义文件中增加了 redis-master 的资源配额声明:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master</div><div class="line">  labels:</div><div class="line">    name: redis-master</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubenetes/redis-master</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div><div class="line">        resources:</div><div class="line">          limits:</div><div class="line">            cpu: 0.5</div><div class="line">            memory: 128Mi</div></pre></td></tr></table></figure>
<p>以上配置表示，系统将对名为 master 的容器限制 CPU 为 0.5（也可以写为500m），可用内存限制为 128MiB字节。</p>
<blockquote>
<p>1 KB(kilobyte) = 1000 bytes = 8000 bits</p>
<p>1 KiB(kibibyte) = 2^10 bytes = 1024 bytes = 8192 bits</p>
</blockquote>
<h2 id="全局默认配额"><a href="#全局默认配额" class="headerlink" title="全局默认配额"></a>全局默认配额</h2><p>除了可以直接在容器（或 RC）的定义文件中给指定的容器增加资源配额参数，我们还可以通过创建 LimitRange 对象来定义一个全局默认配额模版。这个默认配额模版会加载到集群中的每个 Pod 及容器上，这样就不用为每个 Pod 和容器重复设置了。</p>
<p>我们定义一个名为 limit-range-1 的 LimitRange：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: LimitRange</div><div class="line">metadata:</div><div class="line">  name: limit-range-1</div><div class="line">spec:</div><div class="line">  limits:</div><div class="line">    - type: &quot; Pod &quot; </div><div class="line">      max:</div><div class="line">        cpu: &quot; 2 &quot;</div><div class="line">        memory: 1Gi</div><div class="line">      min:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 32Mi</div><div class="line">    - type: &quot; Container &quot;</div><div class="line">      max:</div><div class="line">        cpu: &quot; 2 &quot;</div><div class="line">        memory: 1Gi</div><div class="line">      min:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 32Mi</div><div class="line">      default:</div><div class="line">        cpu: 250m</div><div class="line">        memory: 64Mi</div></pre></td></tr></table></figure>
<p>上述设置表明：</p>
<ul>
<li>任意 Pod 内所有容器的 CPU 使用限制在 0.25～2；</li>
<li>任意 Pod 内所有容器的内存使用限制在 32Mi～1Gi；</li>
<li>任意容器的 CPU 使用限制在 0.25～2，默认值为 0.25；</li>
<li>任意容器的内存使用限制在 32Mi～1Gi，默认值为 64Mi。</li>
</ul>
<h2 id="多租户配额管理"><a href="#多租户配额管理" class="headerlink" title="多租户配额管理"></a>多租户配额管理</h2><p>多租户在 Kubernetes 中以 Namespace 来体现，这里的多租户可以是多个租户、多个业务系统或者相互隔离的多种作业环境。一个集群中的资源总是有限的，当这个集群被多个租户的应用同时使用时，为了更好地使用这种有限的共有资源，我们需要将资源配额的管理单元提升到租户级别，只需要在不同租户对应的 Namespace 上加载对应的 ResourceQuota 配置即可达到目的。</p>
<p>假设我们集群拥有的总资源为：CPU 共有 128core；内存总量为 1024GiB；有两个租户，分别是开发组和测试组，开发组的资源配额为 32core CPU 及 256GiB 内存，测试组的资源配额为 96core CPU 及 768GiB 内存。</p>
<p>首先，创建开发组对应的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Namespace</div><div class="line">metadata:</div><div class="line">  name: development</div></pre></td></tr></table></figure>
<p>接着，创建用于限定开发组的 ResourceQuota 对象，注意 metadata.namespace 属性被设定为开发组的命名空间：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ResourceQuota</div><div class="line">metadata:</div><div class="line">  name: quota-development</div><div class="line">  namespace: development</div><div class="line">spec:</div><div class="line">  hard:</div><div class="line">    cpu: &quot; 32 &quot;</div><div class="line">    memory: 256Gi</div><div class="line">    persistentvolumeclaims: &quot; 10 &quot;</div><div class="line">    pods: &quot; 100 &quot;</div><div class="line">    replicationcontrollers: &quot; 50 &quot;</div><div class="line">    resourcequotas: &quot; 1 &quot;</div><div class="line">    secrets: &quot; 20 &quot;</div><div class="line">    services: &quot; 50 &quot;</div></pre></td></tr></table></figure>
<p>测试组相应的 namespace 与 ResourceQuota 同上，略。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/29/k8s_quota/" class="archive-article-date">
  	<time datetime="2016-03-28T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-29</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
    <article id="post-k8s_op_skills" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2016/03/25/k8s_op_skills/">Kubernetes 常用运维技巧</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>这篇文章对日常工作中常用的 Kubernetes 系统运维操作和技巧进行总结和详细说明。（不定期更新！）</p>
<h2 id="Node-的隔离和恢复"><a href="#Node-的隔离和恢复" class="headerlink" title="Node 的隔离和恢复"></a>Node 的隔离和恢复</h2><p>在硬件升级、维护等情况下，我们需要将某些 Node 进行隔离，脱离 Kubernetes 集群的调度范围。 Kubernetes 提供了一种机制，既可以将 Node 纳入调度范围，也可以将 Node 脱离调度范围。</p>
<p>创建配置文件 unschedule_node.yaml，在 spec 部分指定 unschedulable 为 true：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: Node</div><div class="line">metadata:</div><div class="line">  name: kubernetes-minion1</div><div class="line">  labels:</div><div class="line">    kubernetes.io/hostname: kubernetes-minion1</div><div class="line">spec:</div><div class="line">  unschedulable: true</div></pre></td></tr></table></figure>
<p>然后，通过<code>kubectl replace</code>命令完成对 Node 状态的修改：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl replace -f unschedule_node.yaml</div></pre></td></tr></table></figure>
<p>查看 Node 状态，可以观察到在 Node 的状态中增加了一项 SchedulingDisabled。</p>
<p>对于后续创建的 Pod，系统将不会再向该 Node 进行调度。</p>
<p>另一种方法时不使用配置文件，直接使用<code>kubectl patch</code>命令完成：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl patch node kubernetes-minion1 -p &apos;&#123;&quot;spec&quot;: &#123;&quot;unschedulable&quot;: true&#125;&#125;&apos;</div></pre></td></tr></table></figure>
<p>需要注意的是，将某个 Node 脱离调度范围时，在其上运行的 Pod 并不会自动停止，需要手动停止在该 Node 上运行的 Pod。。</p>
<p>同样，如果需要将某个 Node 重新纳入集群调度范围，则将 unschedulable 设置为 false，再次执行<code>kubectl replace</code>或<code>kubectl patch</code>命令就能恢复系统对该 Node 的调度。</p>
<h2 id="Node-的扩容"><a href="#Node-的扩容" class="headerlink" title="Node 的扩容"></a>Node 的扩容</h2><p>在实际生产系统中会经常遇到服务器容量不足的情况，这时就需要购买新的服务器，然后将应用系统进行水平扩展来完成对系统的扩容。</p>
<p>在 Kubernetes 集群中，对于一个新 Node 的加入是非常简单的。可以在 Node 节点上安装 <code>Docker、Kubelet 和 kube-proxy</code>服务，然后将 Kubelet 和 kube-proxy 的启动参数中的 Master URL 指定为当前 Kubernets 集群 Master 的地址，最后启动这些服务。基于 Kubelet 的自动注册机制，新的 Node 将会自动加入现有的 Kubetnetes 集群中。</p>
<p>Kubenetes Master 在接受了新 Node 的注册后，会自动将其纳入当前集群的调度范围内，在之后创建容器时，就可以向新的 Node 进行调度了。</p>
<h2 id="Pod-动态扩容和缩放"><a href="#Pod-动态扩容和缩放" class="headerlink" title="Pod 动态扩容和缩放"></a>Pod 动态扩容和缩放</h2><p>在实际生产系统中，我们经常会遇到某个服务需要扩容的场景，也可能会遇到由于资源紧张或者工作负责降低需要减少服务实例数的场景。此时我们可以利用命令<code>kubectl scale rc</code>来完成这些任务。比如通过执行以下命令将 redis-slave RC 控制的 Pod 副本数量更新为 3:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl scale rc redis-slave --replicas=3</div></pre></td></tr></table></figure>
<p>将 –replicas 设置为比当前 Pod 副本数量更小的数字，系统将会“杀掉”一些运行中的 Pod，即可实现应用集群缩容。</p>
<h2 id="更新资源对象的-Label"><a href="#更新资源对象的-Label" class="headerlink" title="更新资源对象的 Label"></a>更新资源对象的 Label</h2><p>Label（标签）作为用户可灵活定义的对象属性，在已创建的对象上，仍然可以随时通过<code>kubectl label</code>命令对其进行增加、修改、删除等操作。</p>
<p>例如，我们要给已创建的 Pod “redis-master-bobr0”添加一个标签 role=backend：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label pod redis-master-bobr0 role=backend</div></pre></td></tr></table></figure>
<p>删除一个 Label，只需在命令后最后指定 Label 的 key 名并与一个减号相连即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label pod redis-master-bobr0 role-</div></pre></td></tr></table></figure>
<p>修改一个 Label 的值，需要加上 –overwrite 参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl lable pod redis-master-bobr0 role=master --overwrite</div></pre></td></tr></table></figure>
<h2 id="将-Pod-调度到指定的-Node"><a href="#将-Pod-调度到指定的-Node" class="headerlink" title="将 Pod 调度到指定的 Node"></a>将 Pod 调度到指定的 Node</h2><p>Kubernetes 的 Scheduler 服务（kube-scheduler 进程）负责实现 Pod 的调度，整个调度过程通过一系列复杂的算法最终为每个 Pod 计算出一个最佳的目标节点，这一过程是自动完成的，我们无法知道 Pod 最终会被调度到哪个节点上。有时我们可能需要将 Pod 调度到一个指定的 Node 上，此时，我们可以通过 Node 的标签（Label）和 Pod 的 nodeSelector 属性相匹配，来达到上述目的。</p>
<p>首先，我们可以通过<code>kubectl label</code>命令给目标 Node 打上一个特定的标签，下面是此命令的完整用法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label nodes &lt;node-name&gt; &lt;label-key&gt;=&lt;label-value&gt;</div></pre></td></tr></table></figure>
<p>这里，我们为 kubenetes-minion1 节点打上一个 zone=north 的标签，表明它是“北方”的一个节点：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl label nodes kubernetes-minion1 zone=north</div></pre></td></tr></table></figure>
<p>上述命令行操作也可以通过修改资源定义文件的方式，并执行<code>kubectl replace -f xxx.yaml</code>命令来完成。</p>
<p>然后，在 Pod 的配置文件中加入 nodeSelector 定义，以 redis-master-controller.yaml 为例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master</div><div class="line">  labels:</div><div class="line">    name: redis-master</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubeguide/redis-master</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div><div class="line">      nodeSelector:</div><div class="line">        zone: north</div></pre></td></tr></table></figure>
<p>运行<code>kubectl create -f</code>命令创建 Pod，scheduler 就会将该 Pod 调度到拥有 zone=north 标签的 Node 上去。</p>
<p>使用<code>kubectl get pods -o wide</code>命令可以验证 Pod 所在的 Node。</p>
<p>如果我们给多个 Node 都定义了相同的标签，则 scheduler 将会根据调度算法从这组 Node 中挑选一个可用的 Node 进行 Pod 调度。</p>
<p>这种基于 Node 标签的调度方式灵活性很高，比如我们可以把一组 Node 分别贴上“开发环境”“测试环境”“正式环境”这三组标签中的一种，此时一个 Kubernetes 集群就承载了 3 个环境，这将大大提高开发效率。</p>
<p>需要注意的是，如果我们指定了 Pod 的 nodeSelector 条件，且集群中不存在包含相应标签的 Node 时，即使还有其他可供调度的 Node，这个 Pod 也最终会调度失败。</p>
<h2 id="应用的滚动升级"><a href="#应用的滚动升级" class="headerlink" title="应用的滚动升级"></a>应用的滚动升级</h2><p>当集群中的某个服务需要升级时，我们需要停止目前与该服务相关的所有 Pod，然后重新拉取镜像并启动。如果集群规模比较大，则这个工作就变成了一个挑战，而且先全部停止然后逐步升级的方式会导致较长时间的服务不可用。Kubernetes 提供了 rolling-update(滚动升级)功能来解决上述问题。</p>
<p>滚动升级通过执行<code>kubectl rolling-update</code>命令一键完成，该命令创建了一个新的 RC，然后自动控制旧的 RC 中的 Pod 副本数量逐渐减少到 0，同时新的 RC 中的 Pod 副本数量从 0 逐步增加到目标值，最终实现了 Pod 的升级。需要注意的是，系统要求新的 RC 需要与旧的 RC 在相同的命名空间（Namespace）内，即不能把别人的资产偷偷转移到自家名下。</p>
<p>以 redis-master 为例，假设当前运行的 redis-master Pod 是 1.0 版本，则现在需要升级到 2.0 版本。</p>
<p>创建 redis-master-controller-v2.yaml 的配置文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">apiVersion: v1</div><div class="line">kind: ReplicationController</div><div class="line">metadata:</div><div class="line">  name: redis-master-v2</div><div class="line">  version: v2</div><div class="line">spec:</div><div class="line">  replicas: 1</div><div class="line">  selector:</div><div class="line">    name: redis-master</div><div class="line">    version: v2</div><div class="line">  template:</div><div class="line">    metadata:</div><div class="line">      labels:</div><div class="line">        name: redis-master</div><div class="line">        version: v2</div><div class="line">    spec:</div><div class="line">      containers:</div><div class="line">      - name: master</div><div class="line">        image: kubenetes/redis-master:2.0</div><div class="line">        ports:</div><div class="line">        - containerPort: 6379</div></pre></td></tr></table></figure>
<p>在配置文件中有几处需要注意：</p>
<ol>
<li>RC 的名字（name）不能与旧的 RC 的名字相同；</li>
<li>在 selector 中应至少有一个 Label 与旧的 RC 的 Label 不同，以标识其为新的 RC。本例中新增了一个名为 version 的 Label，以与旧的 RC 进行区分。</li>
</ol>
<p>运行<code>kubectl rolling-update</code>命令完成 Pod 的滚动升级：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master -f redis-master-controller-v2.yaml</div></pre></td></tr></table></figure>
<p>等所有新的 Pod 启动完成后，旧的 Pod 也被全部销毁，这样就完成了容器集群的更新。</p>
<p>另一种方法是不使用配置文件，直接用<code>kubectl rolling-update</code>命令，加上<code>--image</code>参数指定新版镜像名称来完成 Pod 的滚动升级：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master --image=redis-master:2.0</div></pre></td></tr></table></figure>
<p>如果在更新过程中发现配置有误，则用户可以中断更新操作，并通过执行<code>kubectl rolling-update --rollback</code>完成 Pod 版本的回滚：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ kubectl rolling-update redis-master --image=redis-master:2.0 --rollback</div></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2016/03/25/k8s_op_skills/" class="archive-article-date">
  	<time datetime="2016-03-24T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2016-03-25</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li></ul>
	</div>

      

      <div class="clearfix"></div>
    </div>
  </div>
</article>










  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2019 dingmingk
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>

<script src="/./main.js"></script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
      <li data-hook="tools-section-friends"><span class="text">友链</span><i class="icon-link"></i></li>
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/Cgroup/" style="font-size: 12.5px;">Cgroup</a> <a href="/tags/Continuous/" style="font-size: 10px;">Continuous</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Fleet/" style="font-size: 12.5px;">Fleet</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Jenkins/" style="font-size: 10px;">Jenkins</a> <a href="/tags/Kubernetes/" style="font-size: 20px;">Kubernetes</a> <a href="/tags/Life/" style="font-size: 12.5px;">Life</a> <a href="/tags/Linux/" style="font-size: 12.5px;">Linux</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/Nginx/" style="font-size: 17.5px;">Nginx</a> <a href="/tags/RabbitMQ/" style="font-size: 10px;">RabbitMQ</a> <a href="/tags/SaltStack/" style="font-size: 10px;">SaltStack</a> <a href="/tags/Systemd/" style="font-size: 15px;">Systemd</a>
    			</div>
    	</section>
    

    
    	<section class="tools-section tools-section-friends">
  		
  			<div class="friends-wrap" id="js-friends">
  			
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接1</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接2</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接3</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接4</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接5</a>
  	        
  	          <a target="_blank" class="main-nav-link switch-friends-link" href="http://localhost:4000/">友情链接6</a>
  	        
  	        </div>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">很惭愧&lt;br&gt;&lt;br&gt;只做了一点微小的工作&lt;br&gt;谢谢大家</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>